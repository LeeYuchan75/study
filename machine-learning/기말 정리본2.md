## 9주차

### 불확실성의 원천

1. **Uncertain inputs (불확실한 입력 값)** -> 데이터 누락, 노이즈

2. **Uncertain knowledge (불확실한 지식)** -> 원인이 여러 결과를 초래, 또한 해당 분야에서 인과관계에 대한 지식이 불확실

3. **Uncertain outputs (불확실한 결과)** -> 귀납 및 유추만으로 불확실

<br/>

### Discrete Random Variables (이산 확률 변수)

A를 확률 변수라고 하면 다음을 만족함

- A는 **특정 값**을 가질 수 있는 사건을 나타낸다 (ex: "시험에 합격했다" vs "불합격했다"처럼, 확률 변수는 여러 가능한 결과 중 하나를 가짐)

- 각 값에는 관련된 **확률**이 있다 (ex: A가 0(불합격), 1(합격)이라면, P(A=1)=0.7, P(A=0)=0.3 같은 확률이 할당함)

<br>

### Axioms of Probability (확률의 공리)

**공리** : 증명 없이 자명하게 참으로 받아들이는 기본 명제

- 모든 확률은 0 이상 1 이하이다

- **항상 참인 명제는 확률이 1**이고, **절대로 참이 될 수 없는 명제는 확률이 0**이다 (P(true) = 1, P(false) = 0)

- P(A∨B) = P(A) + P(B) − P(A∧B)

<br/>

### Multi-valued Random Variables (다중값 확률 변수)

**Multi-valued Random Variables (다중값 확률 변수)** : 2개 이상의 값을 가질 수 있는 확률 변수 (ex: 시험 점수 A가 {A, B, C, D, F} 중 하나일 수 있음)

- P(A = vi​ ∧ A = vj) = 0 (if i != j) : A가 **동시에 vi와 vj가 되는 확률은 0**이다 (ex: 주사위는 동시에 2와 5가 될 수 없음)

- P(A = v1 ∨ A = v2 ∨ ... ∨ A = vk) = 1: A는 반드시 그 중 하나의 값을 가짐 -> **전체 확률의 합은 1**

<br/>

### marginalization (주변화)

marginalization : 구하고 싶은 사건 B가 일어날 확률을 확률 변수 A의 **가능한 값들에 대해 나누어서 계산하는 것**

![image](https://github.com/user-attachments/assets/27d76f17-d090-4e8f-a352-31c9f09d6063)

<br/>

### marginalization (주변화) 예시 

- A: 오늘 날씨 → {맑음, 흐림, 비}

- B: 야외 행사가 열린다

그럼 P(B)는 다음처럼 계산할 수 있다

p(B) = P(B ∧ 맑음) + P(B ∧ 흐림) + P(B ∧ 비)

<br/>

### marginalization(주변화)가 유용한 상황 

1. **어떤 사건의 전체 확률이 궁금할 때** (ex: 야외 행사가 열릴 확률은 얼마인가?" -> 그런데 이건 날씨(A)에 따라 다름 -> marginalization으로 개별적으로 총 확률 계산)

2. **모델 설계에서 ‘숨겨진 변수’를 제거할 때** (ex: 유전자 상태(A)는 모르지만 질병 발생(B)의 확률은 알고 싶다면? 아래와 같이 식 작성 가능

![image](https://github.com/user-attachments/assets/5f48e954-d49d-40bf-a126-5d5b3e823be0)

관측 불가능한 A를 수학적으로 없애줘서 원하는 값을 구할 수 있음 

예시 : p(B) = P(B ∧ 맑음) + P(B ∧ 흐림) + P(B ∧ 비) 에서 전체 합은 P(B)와 같음

<br/>

### Prior(사전 확률)

**Prior Probability (사전 확률)** : 다른 정보를 고려하지 않고, 단일 사건에 대한 믿음의 정도 -> **어떤 변수의 값을 관측 정보 없이, 단독으로 믿는 정도**

<br/>

### Joint Probability (결합 확률)

**Joint Probability (결합 확률)** : 여러 사건이 동시에 일어날 확률

<br/>

### 예시 : Alarm Domain

변수 : 알람(Alarm), 도둑(Burglary), 지진(Earthquake)

여기서 알람과 도둑의 관계를 보고싶다면 아래와 같이 표로 표현할 수 있고, **marginalization으로 표현**할 수 있음 

![IMG_3283](https://github.com/user-attachments/assets/3808cfcc-372e-4016-9141-2dd892da047f)

<br/>

### Joint Distribution（결합 확률 분포）

d개의 변수에 대한 결합 분포를 만드는 방법은 다음과 같다

1. 변수들의 모든 가능한 값을 나열한 진리표(truth table) 를 만든다 (만약 Boolean 변수 d개면, 테이블에는 2의 d제곱개의 행이 생긴다)

2. 각 조합에 대해, 그 조합이 일어날 확률을 지정

3. 확률의 공리를 따른다면, 그 모든 숫자의 합은 1이 되어야 한다

![image](https://github.com/user-attachments/assets/dacee06c-adee-454a-8223-04c734321b52)

<br/>

### 데이터로부터 결합 확률 분포(Joint Distribution)를 학습

결합 확률 분포를 데이터를 이용해 채우는 방법

![IMG_3273](https://github.com/user-attachments/assets/fea6deb0-8708-4246-a0f9-b70f310a27c6)

<br/>

### Joint Distribution (결합 분포) 얻는 방법

1. 전문가에게서 직접 값을 물어본다

2. 더 단순한 확률 정보를 결합해서 전체 분포를 계산한다

<br/>

### 결합 확률 분포(Joint Probability)로부터 사전 확률(Prior Probability)을 계산 방법 

![IMG_3284](https://github.com/user-attachments/assets/7d9fd4bb-05a0-40b0-add4-a1427b8c6c39)

<br/>

### Conditional Probability (조건부 확률)

**조건부 확률** : 어떤 조건이 주어진 상황에서 다른 사건이 일어날 확률

**P(A∣B)** : B가 참인 세계들 중에서, A도 참인 세계들의 비율

- B = “비가 온다”

- A = “우산을 썼다”

P(A∣B) = 비가 오는 날만 골라봤을 때, 그중에서 우산 쓴 날이 얼마나 되는지 보는 것

<br/>

### 조건부 확률 예시 

![image](https://github.com/user-attachments/assets/11180a54-3167-4885-9177-fc1487333932)

<br/>

### Priors(사전 확률)을 따로 계산하지 않고도 조건부 확률을 계산하는 방법

α는 전체 확률이 1이 되도록 만들어 주는 스케일 조정 값

![IMG_3286](https://github.com/user-attachments/assets/0cec536f-5884-43f7-8b11-d97d7b92442b)

<br/>

### 조건부 확률 역 성립 x 예제

![IMG_3287](https://github.com/user-attachments/assets/014c114a-10a8-4b16-a427-3f1dcd1f1043)

- 빨간 영역 Headache: 두통이 있는 경우

- 파란 영역 Flu: 독감인 경우

- 겹친 부분: 두통과 독감이 동시에 있는 경우

<br/>

- 두통이 있는 경우는 1/10로 드물게 발생

- 독감이 있을 때, 두통이 있을 확률은 50 : 50의 확률로 빈번하게 발생

**그럼 반대로 두통이 있을 때, 독감이 있을 확률도 50 : 50 일까?**

**답은 아니다** -> 베이즈 정리를 이용하면 역 추론 가능 

<br/>

![스크린샷 2025-05-23 183422](https://github.com/user-attachments/assets/e602f6f9-a04f-46da-93d3-b061cab0dfde)

<br/>

### Bayes' Rule (베이즈 정리)

![image](https://github.com/user-attachments/assets/35e432d2-2fb2-4ac0-a005-a1d08d78612b)

**Bayes' Rule (베이즈 정리)** 는 확률적 머신러닝에서 가장 중요한 공식이다 

<br/>

### 베이즈 정리 & 조건부확률 차이점

조건부확률은 B가 일어났다는 사실을 알고 있을 때, A가 일어날 확률을 계산한 것처럼 **원인이 발생했을 때 결과가 나온 확률을 구한 것**이고

베이즈 정리는 결과가 나왔을 때, **해당 결과가 도출된 원인의 확률을 구한 것**이다

- **조건부확률 = 원인 -> 결과**

- **베이즈 정리 = 결과 -> 원인**

<br/>

### 베이즈 정리의 목적

관측된 **결과**(증거)를 바탕으로 **원인**(가설)의 확률을 추론하기 위함

<br/>

### 베이즈 정리 사용 이유 

관측된 결과로부터 숨겨진 원인의 확률을 역방향으로 추론하기 위해 사용

<br/>

### 베이즈 정리 장점

- **역방향 추론이 가능**  

- **관측 불가능한 변수 추론에 유용**

<br/>

### 베이즈 정리 단점

- 고차원 문제에선 확률 분포 표현이 어려움

- 연속 변수/복잡한 모델에서 수식 해석 어려움

<br/>

## 10주차 

### Density Estimation (밀도 추정)

**Density Estimation (밀도 추정)** : **여러 입력 속성(attribute)** 들로부터 **확률을 예측하는 함수를 학습**하는 것

즉, 주어진 데이터 포인트에 대해 **얼마나 가능성이 높은지 (likelihood)** 를 추정하는 작업

<br/>

### 비교 예시 

- Classifier: 정답이 뭐야? (범주형)

- Regressor: 얼마야? (숫자형)

- **Density Estimator: 이 조합이 얼마나 가능해? (확률)**
  
<br/>

### Density Estimator 사용 목적 
입력 데이터가 어떤 확률 분포에 속하는지 모델링하여 새로운 데이터의 **가능성(likelihood)** 을 평가하고 이를 기반으로 **베이즈 분류기에 활용**하는 것 

<br/>

### Density Estimator 장점 

1. **사람이 직접 확률을 설정할 필요 없이**, 데이터를 통해 밀도 추정기를 학습할 수 있다

2. 각 레코드를 확률값 기준으로 정렬하면, **이상치(이상한 데이터) 를 찾아낼 수 있다**

3. **추론을 할 수 있다** (ex: 특정 값이 주어졌을 때, 다른 변수의 조건부 확률을 계산할 수 있음)

4. **베이즈 분류기의 핵심 재료이다** (조건부 확률을 계산하기 위해 Density Estimator (밀도 추정기)를 활용)

<br/>

### Density Estimator 단점

1. 전체 결합 확률을 무작정 통째로 학습하는 방식은 **단순하고, 무의미하며, 위험하다**

2. **데이터가 조금만 부족해도 희귀한 조합의 확률이 0**이 될 수 있음

3. 차원이 많아질수록 가능한 조합 수가 폭발적으로 증가 → **차원의 저주** -> **필요한 데이터 양도 증가** -> **정확한 확률 추정이 어려워짐**

<br/>

### Density Estimator 구하는 방법

우선 아래 공식을 보자 

![image](https://github.com/user-attachments/assets/9b2e4192-d95f-421a-8cfa-ba3ef389db15)

위 공식은 어떤 데이터의 행 (레코드) x가 주어졌을 때, Density Estimator(밀도 추정기) M은 **이 레코드가 얼마나 가능성 있는지(확률이 높은지) 알려준다**

![image](https://github.com/user-attachments/assets/d546fcaf-9383-4b43-b24a-23d9709dd8d0)

처음 구했던 공식을 확장한 위 공식이 바로 Density Estimator 구하는 방법이다

xi 가 모델 M로부터 나올 확률의 곱과 동일하다 -> **독립적이라고 가정**

<br/>

### Density Estimator 공식의 문제 (단점은 아님)

모든 확률을 모두 곱해버리면 값이 매우 작아진다 -> 로그 확률 사용 

<br/>

### Density Estimator & joint Density Estimator 

- **Density Estimator** : **입력 속성에 대한** 확률 분포를 추정하는 일반 개념

- **Joint Density Estimator** : **모든 속성 조합**에 대한 결합 확률분포를 직접 학습하는 특정한 형태의 Density Estimator

<br/>

### joint Density Estimator 단점

1. 테스트셋에 **처음 보는 조합이 나오면 모델은 아예 불가능하다고 판단**해버림 → 로그 가능도 -∞

2. 차원이 높아질수록 공간이 기하급수적으로 커져서 고차원에서 밀도 추정, 거리 계산, 분류 등은 **데이터 희소성 때문에** 성능이 떨어짐 (**차원의 저주**)

3. **다른 모델보다 가장 심하게 과적합이 된다**

4. **훈련 데이터에만 너무 맞춰서 일반화가 전혀 안 된다**

<br/>

### Naïve Bayes Classifier

Naïve Bayes Classifier : **베이즈 정리(Bayes' Theorem)** 를 기반으로 하되, 모든 속성(특징)이 클래스 Y에 대해 **조건부 독립이라는 가정**을 적용하여 클래스를 예측하는 확률 기반 분류기

<br/>

### Naïve Bayes Classifier 목적 

주어진 입력 속성에 대해 **가장 가능성 높은 클래스** 를 추론하기 위해 사용

<br/>

### Naïve Bayes Classifier 장점 

1. **빠르고 간단한 학습** : 한 번의 데이터 스캔으로 확률 추정 가능

2. **속성 수가 많아도 효율적** : 조건부 독립 가정으로 조합 수가 폭증하지 않음

3. **불필요한 속성에 덜 민감** : 관련 없는 속성의 영향이 제한적

4. **이산형·연속형, 스트리밍 데이터 모두 처리 가능** : 

<br/>

### Naïve Bayes Classifier 문제점

1. 데이터가 적으면 어떤 값은 **한 번도 등장하지 않아 확률이 0**이 될 수 있다

2. 어떤 경우든 확률이 0이 되어 전체 곱이 0이 되는 것은 모델이 지나치게 학습 데이터에만 맞춘 것일 수 있다 -> **overfitting 유발**

<br/>

### Naïve Bayes는 다음과 같은 상황에서 사용한다

- 이메일이 스팸인지 아닌지 분류 -> 이메일이 왔는데 (결과), 이게 스팸인가 (원인)

- 회의 공지 이메일인지 분류 

- 문서를 작성한 저자가 누구인지 분류 -> 작성된 문서가 있을 때 (결과), 누가 저자인가(원인)

- 뇌 상태를 분류 (ex: 𝑃(BrainActivity∣WordCategory) -> 단어 종류가 주어졌을 때 뇌 활동이 어떻게 되는지를 학습)
  
입력 특성들이 개별적이고 클래스별로 나타나는 특성 분포가 다르며 수천 개 특성 중 일부만 중요한 역할을 하는 경우

→ Naïve Bayes가 조건부 독립 가정을 통해 간단한 모델로도 높은 분류 성능을 낼 수 있는 이상적인 환경

<br/>

### Naïve Bayes Classifier 사용 이유 

**복잡한 joint probability를 추정하지 않고도** 클래스 조건부 확률을 효율적으로 계산할 수 있음

![IMG_3375](https://github.com/user-attachments/assets/11d76b09-78c4-4edc-8276-3acd4a6b0e8a)

- 파란칸 : 클래스(정답)의 사전 확률 (ex: 이메일이 100개 있는데 그중 30개가 스팸이면 P(spam) = 30/100 = 0.3) -> **계산이 쉬움**

- 초록색칸: 특성(feature)들이 전부 동시에 특정값이 될 확률 (ex: Y 값이 "play = yes"일 때, X 값은 (특징 값은) "sky = sunny ∧ temp = warm ∧ humid = high ∧ ...") -> **이런 경우의 수가 너무 많다는 것이 문제** 예를 들어 특성이 6개인데 각각 3가지 값이 있을 수 있으면 조합이 3⁶ = 729개가 나옴 -> **비현실적**

- 빨간칸 : Y값에 상관 없이 모든 특징의 확률을 계산 -> **어차피 모든 클래스에서 공통으로 들어가기 때문에 분자끼리만 비교하면 됨**

<br/>

### 주의할 점

Naïve Bayes Classifier는 **모든 특성이 실제로 독립이 아닐 수 있다**. 말 그대로 **가정만 한 것**이다

하지만 충분히 괜찮은 성능을 낼 수 있기 때문에 Naive Bayes는 실무에서 많이 사용한다

<br/>

### Naïve Bayes Classifier 예시

우리가 알고 싶은 것 : 이 날씨일 때 Play는 yes일까, no일까?

![image](https://github.com/user-attachments/assets/e420d40a-bde6-44bb-b9d5-ebc44e317590)

### step 1 : 사전확률 구하기

- play=yes: 3개

- play=no: 1개

<br/>

### step 2 : play가 yes일 때와 no일 때 조건부확률 구하기

### Play = yes 일 때 

- P(Sky = sunny | yes) = 3/3 = 1.0

- (Temp = warm | yes) = 3/3 = 1.0

- P(Humid = high | yes) = 2/3

- P(Wind = strong | yes) = 3/3 = 1.0

- P(Water = warm | yes) = 2/3

- P(Forecast = same | yes) = 2/3

<br/>

### Play = no 일 때 

- P(Sky = sunny | no) = 0/1 = 0.0

- P(Temp = warm | no) = 0/1 = 0.0

- P(Humid = high | no) = 1/1 = 1.0

- P(Wind = strong | no) = 1/1 = 1.0

- P(Water = warm | no) = 1/1 = 1.0

- P(Forecast = same | no) = 0/1 = 0.0

<br/>

### step 3 : 각각 확률을 모두 곱해서 비교 

ex : **P(play=yes)** ⋅P(sunny∣yes)⋅P(warm∣yes)⋅P(high∣yes)⋅P(strong∣yes)⋅P(warm∣yes)⋅P(same∣yes) vs ...

예측 값이 큰 값을 최종적으로 선택 

<br/>

### Laplace Smoothing

Naive Bayes에서 어떤 값이 한 번도 등장하지 않으면 확률이 0 되고, 전체 확률 구할 때 조차 예측 불가능해진다

따라서 Laplace smoothing을 이용한 것인데 핵심 아이디어가 **등장하지 않은 값도 아주 작은 확률을 주자는 것**

<br/>

### Laplace smoothing 공식 

![IMG_3376](https://github.com/user-attachments/assets/ce4b8797-e18f-4ac7-becd-621205144719)

<br/>

### 공식 이해 

- **분모 시그마** : 클래스 Y = yk 에서 **특성 Xj**의 모든 값들이 등장한 횟수의 총합

- 예를 들어 feature이 Sky이고, Sky가 가질 수 있는 값들은 sunny, rainy, overcast 라고 하자. 이때, Play = yes인 데이터에서: sunny = 3번, rainy = 0번, overcast = 1번 나왔다면, 시그마 공식에 의해 3 + 0 + 1 = 4의 값을 가진다

- **∣=values(Xj)∣** : 특성 Xj가 가질 수 있는 고유한 값의 개수

- 예를 들어 Sky는 가능한 값이 sunny, rainy, overcast로 3가지라면, **∣values(Sky)∣= 3**이다

<br/>

### Laplace smoothing 예시 

Naïve Bayes Classifier에서 설명한 예시와 동일하다

![image](https://github.com/user-attachments/assets/e420d40a-bde6-44bb-b9d5-ebc44e317590)

### step 1 : 사전확률 구하기 (사전 확률은 스무딩 기법 x)

- P(Play=yes) = 3/4

- P(Play=no) = 1/4

<br/>

### step 2 : 첫 번째 조건부 확률 계산

P(Sky=sunny | play=yes) 를 계산해보자

Naïve Bayes Classifier 기법을 이용하면 P(Sky=sunny | play=yes = 3/3 = 1.0이 나왔었다 

하지만 Laplace smoothing를 사용하면 

- play = yes인 행은 3개. 그 중 sunny는 3번 (rainy는 play = yes에 존재하지 x)

- Laplace Smoothing 적용: (3 + 1) / (3 + 2) = 4/5  (분자는 sunny가 3번 나온 것에 +1, 분모는 sunny = 3번, rainy = 0 번 이여서 나온 3과 총 특징의 개수 2개를 (sunny, rainy는 총 2개) 더함)

**따라서 이 경우 P(Sky=sunny | play=yes) = 4/5 가 나온다**

<br/>

### The Naïve Bayes Graphical Model


- 노드는 확률 변수 (Y, X₁, ..., X_d)

- Y → X_j로 향하는 화살표는 의존성을 뜻함 (각 특성 Xj의 값은 클래스 Y에 의존해서 달라진다는 뜻)

- 각 노드는 조건부 확률 테이블(CPT) 을 갖고, 그 값은 부모(Y)에 의해 결정됨 (x가 자식)

![IMG_3384](https://github.com/user-attachments/assets/871ca314-9438-47ac-ab55-9ab06fe7c4b2)

### 만약 

x = (rainy,warm,normal) 일 때 경기 여부를 구하려면 아래와 같이 푼다 

![image](https://github.com/user-attachments/assets/a16dbb2a-a4e6-4ba5-b9ce-67b3f43edf84)














































































































