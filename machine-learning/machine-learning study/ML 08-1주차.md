## 정리할 때 목적, 장단점, 대안책 등등 정리하기 



## 모델 예측이 틀리는 원인 

모델 예측이 틀리는 원인는 다음과 같다

- True non-determinism (진정한 비결정성) 

- Partial observability (부분 관찰 가능성)

- Representational bias (표현 편향)

- Algorithmic bias (알고리즘 편향)

- Bounded resources (제한된 자원)

위 내용을 하나씩 살펴보자 

<br/>

## 1. True non-determinism (진정한 비결정성)

만약 앞면이 나올 확률이 좀 더 높은 동전이 있다고 생각해보자 

동전의 앞면이 나올 확률이 0.7이라면, 모델은 항상 뒷면보다 확률이 더 높은 앞면만 예측을 할 것이다 

하지만 현실 세계에서는 앞면의 확률이 좀 더 높더라도 이것은 매우 확률적인 문제이다 

<br/>

**학습된 모델은 가장 좋은 예측만을 출력**하기 때문에 이러한 문제가 발생한다

<br/>

## 2. Partial observability (부분 관찰 가능성)

**Partial observability (부분 관찰 가능성)** : 어떤 값을 예측하기 위해 필요한 정보가 실제 관측값 x에서 빠져 있는 상태이다

N-bit parity problem를 생각해보자 

위 문제는 모든 N개의 비트를 알아야만 판단할 수 있지만, 만약 x가 N−1개의 비트를 포함하고 있는 경우, **필요한 모든 정보를 가지고 있지 않아** 예측이 어려워진다

**이것을 hard PO라고 한다**

만약, N개의 비트를 가지고 있지만, 학습된 모델이 capacity나 bias에 의해 **일부 비트를 무시하는 상황**이 발생할 수 있다 

**이것을 soft PO 라고 한다**

<br/>

또한 관찰된 x가 noise를 포함한 경우 **측정 과정에서 발생하는 오류**가 발생할 수 있고, **장비의 한계**로 인해 정확한 측정이 불가능한 상황이 생긴다

<br/>

## 3. Representational bias (표현 편향)

![image](https://github.com/user-attachments/assets/18444319-dca1-4625-b407-d7b2bca243ea)
  
모델이 **현실 세계의 데이터를 얼마나 잘 표현**할 수 있는지에 대한 문제이다

만약 모델이 실제 패턴을 잘 반영하지 못하면, **학습이 잘 되지 않고** 예측이 틀리게 된다

<br/>

**Algorithmic bias (알고리즘 편향)** 와 **Bounded resources (제한된 자원)** 은 객관식 대비로 가볍게 암기만 하자 

<br/>

## SVM (Support Vector Machines)

이번에 배울 분류 모델인 SVM에 대해 알아보자 

**SVM(서포트 벡터 머신)** : 분류와 회귀분석에 사용되는 지도학습 모델이다 

<br/>

## SVM 장점 

1. SVM은 이론적으로나 실제 사용에서 **모두 성능이 좋다**

2. **적은 양의 학습 데이터**로도 좋은 성능을 발휘

3. **전역 최적의 모델**을 찾아낸다

4. **효율적인 알고리즘**을 사용

5. **커널 트릭(Kernel Trick)** 을 적용하기에 유리


<br/>

## SVM 정보 

![image](https://github.com/user-attachments/assets/7a750f66-08fb-4f29-81a5-8ada75c78c31)

**학습데이터 x의 차원**은 d+1차원이다. 이때 +1은 상수항을 내적에 포함하기 위해 한 것이다 

**예시**

![image](https://github.com/user-attachments/assets/3fd0cb53-7e5c-4991-bcf9-698fcc461cd1)

<br/>

학습데이터의 차원이 d+1 이므로 **파라미터의 차원도 d+1**이다 

<br/>

또한 **SVM은 내적 결과가 0인 지점이 decision boundary**이다. 이것을 **Hyperplane**이라고 한다

Decision function은 부호에 따라 데이터가 어떤 클래스에 속하는지를 결정한다 

<br/>

## 정리 

- Training instances: 학습에 사용되는 데이터 (특징 + 레이블)

- Model parameters: 학습을 통해 구해진 가중치)

- Hyperplane: 두 클래스를 나누는 경계 (**내적 결과가 0인 지점**)

- Decision function: 어느 클래스에 속하는지 결정

<br/>

## 예시 

(다음 예시는 선형적인 경계의 경우이다. 비선형적 경계는 이후에 kernel trick에서 배우도록 하자)

아래 3가지 선은 모두 클래스를 잘 분리하는 경계이다 

![image](https://github.com/user-attachments/assets/2fbdcc40-9b85-47eb-a4c2-073c67fcaa64)

그렇다면, 이 중 **가장 분류를 잘하는 선**은 무엇이고, **그 기준**은 무엇일까 ?

그 기준이 바로 **각 데이터마다 noise가 있다고 가정**하여 분류를 하는 것이다

<br/>

![image](https://github.com/user-attachments/assets/20efe1f6-439b-41ea-b3cc-3b79ffd13064)

이렇게 각 데이터마다 noise가 있다고 가정했을 때, **noise에 걸치는 선은 가장 좋은 분류가 아니다**

남은 선들을 가지고 위 과정을 반복하여 최적의 분류 경계를 찾아야 한다 

![image](https://github.com/user-attachments/assets/9124f29c-2cff-4ce7-aa45-8e94c004ec4e)

위 과정을 모두 거치면 다음과 같은 최종결과가 산출된다 

![image](https://github.com/user-attachments/assets/4e2a8aa3-cd9c-49e1-bc97-519de0bb4a57)

여기서 중요한 것이 **margin**이다 이것에 대해 아래에서 자세히 살펴보자 

<br/>

## margin

**Margin(여백)** 은 SVM에서 두 클래스를 나누는 **결정 경계(Decision Boundary)** 와 가장 가까운 데이터 포인트(Support Vector) 사이의 거리를 의미한다 

이 거리가 **최대화**될수록, 모델은 새로운 데이터가 들어왔을 때 **더 잘 분류할 수 있다**

즉, 이 **margin을 최대화를 해야하는 것이 핵심**이다

<br/>

## SVM 목표

**SVM의 목표**는 각 데이터마다 **noise가 있다고 가정한 상황**에서 **최대한 일반화**가 잘 되는 **최적의 결정 경계** 를 찾는 것이다. 즉 **margin을 최대화하는 것이다**

<br/>

## margin 특징 

### margin이 커질수록 capacity(적합할 수 있는 능력)이 감소하여 overfitting을 예방

marin을 크게 하면 모델이 복잡하게 데이터를 분류하려고 하지 않고, 더 단순한 결정 경계를 찾는다 

단순해진 모델은 **과적합(Overfitting)이 줄어들고**, 새로운 데이터에 대한 **일반화 성능(Generalization)이 높아진다**

<br/>

### margin이 작으면 Linear decision 개수 증가, margin이 크면 Linear decision 개수 감소

마진이 작다는 것은 두 클래스 간의 **여유 공간이 좁다**는 뜻이다

즉, **결정 경계(Decision Boundary)** 가 서로 **빽빽하게 밀집**해 있는 상황이다 

이 경우 촘촘한 경계를 분류하기 위해 **비선형적인 경계**를 사용할 수 있고, **더 많은 선형 경계**를 이용하여 표현해야한다 

반대로 margin이 크다는 것은 단순하게 분류가 가능하다는 의미이다

<br/>

### underfitting은 될 가능성이 매우 낮음

SVM은 마진 최적화와 Support Vector 중심 학습을 통해, 복잡한 패턴을 충분히 학습하기 때문에 underfitting이 될 가능성이 매우 낮다 

<br/>

## 로지스틱 회귀 관점

다음은 SVM을 설명하기 위해 로지스틱 회귀의 관점에서 설명을 하겠다 

둘은 **결정 경계(Decision Boundary)** 를 학습하여, 데이터를 두 개의 클래스로 구분하기 때문 

그럼 아래 로지스틱 회귀 관점에서 살펴 보자 

![IMG_3482](https://github.com/user-attachments/assets/6813db63-540a-43c8-8604-7942161ccd32)

![image](https://github.com/user-attachments/assets/c73f8404-f5da-4d6a-8c67-d2b1349a6fec)

두 번째 슬라이드를 보면, y = 1 일 때, hθ(x)의 값이 1에 가까울수록 cost가 0에 가까워지는 것을 볼 수 있다 

<br/>

추가적으로 손실 함수와 비용 함수의 개념 설명을 하자면 손실 함수는 개별 오차를 의미하고, 비용 함수(cost function)은 전체 오차의 평균을 의미한다

둘은 유사하지만 다른 의미를 가지고 있다

<br/>

다시 본론으로 넘어가서 로지스틱 회귀를 설명한 이유는 **SVM이 위 공식과 매우 유사**하기 때문이다

아래는 SVM 공식이다 

![image](https://github.com/user-attachments/assets/e5d74df7-adda-40ff-8b74-86df824fe82b)

<br/>

## C의 역할

SVM의 C는 로지스틱 회귀의 **1/𝜆**에 해당한다 

C가 크면, **모델이 오차를 최소화**하는 데 더 집중하고,

C기 작으면 **규제(Regularization)** 에 더 집중한다

<br/>

C가 클 때, **오차를 최대한 줄이려는 방향**으로 학습하다 보니, **margin이 작아지는 경향**이 생기고, 따라서 **overfitting이 발생**할 수 있고, 

반대로 C가 작다면 오차를 허용하면서도 Margin을 최대화하는 경향이 존재한다. 따라서 **overfitting을 방지한다**

![image](https://github.com/user-attachments/assets/26303005-63b9-412f-94ba-6a82c8c67f99)

<br/>

## 정리 

**C가 클 때** -> 분류가 잘 됨 -> 오차 최소화 -> margin 감소 -> **overfitting 위험**

**C가 작을 때** -> 상대적으로 분류 능력 떨어짐 -> 일부 오차 허용 -> margin 증가 -> **overfitting 감소**

<br/>

## SVM 함수 표현 

![IMG_3480](https://github.com/user-attachments/assets/7078cdf4-6b2f-4fd2-8847-4ec31cfee331)

<br/>

![IMG_3481](https://github.com/user-attachments/assets/22029a5b-6d07-45d2-94c9-5788d7ab9311)

SVM은 아래에 존재하는 **Hinge(힌지) 함수를 손실 함수로 사용**한다

Hinge(힌지) 함수는 **일정 범위를 넘어가면 Loss를 0**으로 하는 함수이다 

위 공식을 자세히 보면 cost 아래 1과 0이 있는 것을 볼 수 있다 

이것은 다음과 같은 의미를 가진다 

![image](https://github.com/user-attachments/assets/359320d7-f38f-44c3-9214-8e4a96201898)

이것을 이용하여 **최대 마진 범위를 넘어가면 0**으로 처리하고 **margin 범위 내부에 존재하면 cost에 값이 생성**되는 원리이다 

![image](https://github.com/user-attachments/assets/97be0e36-c6e7-47a9-988d-3f2e9ddd10c2)

<br/>

## 예시

![image](https://github.com/user-attachments/assets/45e10ced-107c-432e-8268-b82c7c8b3c6f)

**θ**는 SVM(Support Vector Machine)에서 **결정 경계(Hyperplane)** 의 **방향 벡터(Weight Vector)** 를 나타낸다 (이후 자세히 설명)

이미지를 보면 θTx =1 과 θTx = -1에 걸친 선에 있는 클래스를 **support vector**라고 한다 

**support vector는** 결정 경계를 정의하는 **중요한 역할**

<br/>

또한, 위 그림과 같이 **margin과 θ는 반비례 관게이다**

다시 말하면, 

Support Vector와 결정 경계 사이의 거리가 ∥θ∥와 **반비례**한다

- ∥θ∥ 가 커지면 **margin이 작아지고**,

- ∥θ∥ 가 작아지면 **margin이 커진다**

<br/>

## 벡터 내적 복습 

![IMG_3181](https://github.com/user-attachments/assets/1b4ef1b4-ddcb-4a21-a181-7538c303500b)

<br/>

## Hyperplane(초평면)

![IMG_3177](https://github.com/user-attachments/assets/29d75984-1cb0-4af1-ba0f-c48ffa37a2d9)

x는 SVM이 학습하는 입력 데이터이고 θ는 **결정 경계(Hyperplane)의 방향**을 나타낸다 

θ는 Hyperplane과 내적 값이 0인 수직 방향으로 계산되어진다 

SVM의 목표는 클래스를 **최대한 멀리 분리**하는 것, 즉 **margin이 최대화**하는게 목표이기 때문에

**내적을 이용**하여 각 Support Vector가 결정 경계에서 어느 정도 떨어져 있는지 판단한다 

<br/>

## SVM 최적화 과정

**1. 초기화 단계**

초기 Hyperplane을 설정한다

<br/>

**2. 최적화 단계**

모든 데이터를 순차적으로 확인하여 각 데이터 포인트가 마진 조건을 만족하는지 검사한다

<br/>

**3. 업데이트 단계**

만약 마진을 침범하거나 오류가 발생하면, Hyperplane을 업데이트를 한다 

이때, 업데이트 후 마진 범위를 침범한 포인트는 새로운 Support Vector가 될 수 있다

<br/>

이것을 **최적화가 수렴할 때까지 반복**한다

즉, 학습 과정에서 Support Vector를 이용하여 **θ 값을 반복적으로 조정**하면서 최적의 Hyperplane를 찾는다 

![image](https://github.com/user-attachments/assets/4c4711e3-cc8c-461e-9f1e-f33b99be4cae)

위 이미지를 설명하자면

왼쪽 이미지에서 p가 잘을 때, 양성 클래스일 때는 1 이상이어야 하니까,**∥θ∥2를 키워야하고**, 

음수 클래도 마찬가지로 -1 이하여야 하니까 **∥θ∥2를 키워야한다** (cosθ 가 음수) 

세타를 키운다는 것은 시각적으로는 세타가 늘어나지만, **margin과 세타는 반비례이기 때문에 실제로는 margin이 줄어든다**

<br/>

만약 p가 크다면, 오른쪽 그림과 같이 줄여도 된다 **(강제가 아님)**

<br/>

## 정리 

![image](https://github.com/user-attachments/assets/3d9ba10c-9f71-4891-9270-9d4ace1d2808)








