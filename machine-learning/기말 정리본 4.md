## 12주차 

### 생성 모델 (Generative Models)

**생성 모델** : 어떤 확률 규칙을 통해 데이터를 만든다는 것

<br>

### 통계적 생성 모델 (Statistical Generative Models)

**통계적 생성 모델** : 확률 분포 Pθ(x) -> θ라는 파라미터를 이용해서 확률 분포 x를 추정할 수 있다는 의미

즉 데이터 생성 + 어디에 얼마나 분포되어 있는지 

<br/>

### 모델을 만들기 위해 우리가 알고 있거나 설정하는 것

- **어떤 분포**를 사용할지 (ex : 가우시안 분포)

- **어떤 기준**으로 모델을 학습시킬지 (ex: maximum likelihood)

- **어떤 알고리즘으로 최적화**할지 등을 포함

<br/>

### Discriminative (판별 모델) & Generative Models (생성 모델) 차이점

**Discriminative model (판별 모델)** : **정답(y)이 무엇인지 바로 맞히는 게** 목표 (ex: Regressions, SVMs)

수식 : P(y∣x) -> ex: 이 사진이 고양이일 확률은? → 바로 예측 가능

이 둘 사이를 어떻게 나눌까? → 선이나 경계만 배움

<br/>

**Generative Models (생성 모델)** : 각 정답(y)이 어떤 입력(x)을 만들어내는지를 **먼저 배우고**, 그걸 바탕으로 **정답(y)을 추론**

수식 : P(x∣y)을 먼저 구하고 → Bayes' rule로 P(y∣x) 계산

ex: "고양이들은 이런 특징을 가지고 있다!" → 그럼 이게 고양이일 확률은?

각 클래스가 어떤 모양으로 퍼져 있는지 배우자

![image](https://github.com/user-attachments/assets/cb3712d4-fb48-4a7d-865d-52c0f19a96e4)

<br/>

### 핵심 개념 정리 

- **latent variable (잠재 변수) z로 표현** : 우리가 관측한 데이터 x를 만들기 전에 랜덤하게 선택된 숨은 정보

- **random variable (랜덤 변수) x로 표현** : z는 그냥 정해지는 게 아니라 확률적으로 랜덤하게 뽑아서 정해짐. 그래서 x는 z에 따라 바뀜

- **prior distribution p(z∣ϕ)** : 잠재 변수 z의 사전 분포(prior distribution). Φ는 z가 어떻게 분포될지를 결정하는 역할 (ex: "웃는 얼굴이 70%, 찡그린 얼굴이 30%")

<br/>

![image](https://github.com/user-attachments/assets/59dc3ce4-7f9b-41fc-8b6e-7358ad617def)

- 각각의 관측 데이터 xn은 숨겨진 변수 zn과 연결되어 있다고 가정한다

  - zn​은 어떤 **사전 분포 p(z∣ϕ)** 를 따르는 무작위 변수라고 가정 (z는 처음부터 랜덤하게 뽑는 값, ϕ는 z를 뽑는 분포의 설정값 ex: '표정은 미소일 수도, 찡그림일 수도 있다'처럼 우연히 선택되는 요소)

- z가 주어졌을 때 x를 생성하는 **데이터 분포를 p(x∣z,θ)** 라고 가정 

- 즉, p(z∣ϕ)로 z를 설정하고 뽑힌 z를 이용해서 최종적인 x를 도출하는 것

<br/>

### 데이터가 만들어지는 과정(generative story) -> generative story(생성 이야기) 


1. 먼저, 무작위 잠재 변수 zn을 사전 분포 p(z∣ϕ)에서 뽑는다 (보이지 않는 설정값(ex: 나이, 표정, 각도 같은 숨은 정보)을 확률적으로 뽑는 것부터 시작)

2. zn이 주어졌을 때, 𝑥𝑛을 p(x∣z,θ)라는 분포를 이용해 생성

<br/>

### 생성 모델 사용 이유 

생성 모델은 **데이터를 어떻게 생성할 수 있는지에 대한 확률적 구조를 학습**함으로써, **새로운 데이터를 만들거나, 결측값을 처리하거나, 다양한 데이터 유형**을 다룰 수 있게 해준다

<br/>

### 생성 모델 사용 예시 

![image](https://github.com/user-attachments/assets/6eb04375-c817-4f22-8b1a-9d485b4d4f13)

이미 학습된 모델에서 흐릿한 얼굴 이미지를 z를 확률적으로 무작위로 뽑고, 그에 따라 x를 만들어내어 진짜 사람 얼굴처럼 정교하게 생성됨

<br/>

### 대표적 생성 모델 : Mixture model

**Mixture model** : **여러 개의 간단한 분포를 섞어서 복잡한 데이터의 구조를 설명**할 수 있게 하는 모델

**클러스터링이나 확률 밀도 추정에 사용**

![image](https://github.com/user-attachments/assets/82ecb1bf-05cf-4cd6-a5ca-8b1b2ec6f884)


- 데이터 x1, x2, ... xn이 K개의 분포를 섞어서 만들어졌다고 가정 (ex: 학생 키 데이터가 있으면 남자 키 분포, 여자 키 분포를 섞어서 전체 데이터가 만들어짐)

- 이 K개의 분포는 각각 θ1, ... θk 라는 파라미터를 가진 p(x∣θk)라고 하자 (ex: 남성 키는 평균 175cm, 표준편차 7cm인 정규분포, 여성 키는 평균 162cm, 표준편차 6cm인 정규분포, 여기서 평균과 표준편차가 파라미터)

- 각각의 𝑥n 이 어떤 분포에서 나왔는지는 모른다 (ex: x3이 남자에서 나왔는지 여자인지 우린 모름)

<br/>

아래의 생성 이야기를 가정해보자

1. 먼저, 그룹 하나를 랜덤하게 고른다 (zn ∈ {1,2,...,k}) 중 랜덤하게 하나 선택) ex: 남성 그룹(z=1)을 뽑음

2. 그 그룹에 맞는 분포로부터 𝑥𝑛을 생성 ex: 남성 키 평균 분포를 사용해서 키 데이터를 하나 생성

![스크린샷 2025-06-11 112639](https://github.com/user-attachments/assets/3547a75b-ca46-44a7-8e2f-8422d4cff927)

<br/>

### Gaussian Mixture Model (GMM)

**Gaussian Mixture Model (GMM)** : 데이터를 **여러 개의 가우시안 분포(Gaussian distributions)의 혼합(mixture)** 으로 모델링하는 확률적 생성 모델(generative model)

<br/>

### GMM: The Generative Story (GMM 생성 과정)

### 1. 먼저 π에 따라 zₙ을 고른다. 즉, 어떤 가우시안 분포에서 데이터를 만들지 정함

![image](https://github.com/user-attachments/assets/becc5b47-afa8-499f-9f39-20a8a875f015)

위 공식의 의미는 다음과 같다 

- zn : 군집(cluster) 번호 (예: 1번, 2번, 3번) **여기서 군집은 가우시안 분포의 집합을 의미**

- 𝜋 : 각 군집이 선택될 확률 (예: [0.5, 0.3, 0.2]) -> 1번 군집에서 나올 확률 50%, 2번 군집에서 나올 확률 30% ... **랜덤하게 결젇됨. 확률이 높은걸 고르는게 아님** (이 선택은 z에 대한 사전확률에서 이뤄진다, 즉 zₙ을 선택할 때 미리 정해진 확률 분포 π를 기준으로 무작위로 선택)

- 다항 분포로부터 군집 선택

<br/>

### 2. 만약 zₙ = k로 나왔다면, 이제 k번째 가우시안에서 xₙ을 만든다

![image](https://github.com/user-attachments/assets/5e0401c9-621e-4fd8-9cd1-16b0724bfbc0)

- μk : k번째 군집의 평균

- Σ𝑘 : k번째 군집의 분산 (퍼짐 정도)

- 𝑥𝑛 : 실제로 관측된 데이터 포인트

<br/>

만약 1번 과정에서 2번 군집이 선택되었다면, 2번 군집의  평균과 분포 모양에 맞춰서  실제 좌표 또는 점 하나를 랜덤하게 뽑아낸다

<br/>

### GMM(Gaussian Mixture Model)의 파라미터를 학습과정 

**Conditional p.d.f.** : z가 주어졌을 때 x의 분포. 즉, “이 군집에서 나왔으면 x는 이렇게 분포할 것이다" 를 의미

![image](https://github.com/user-attachments/assets/857814a9-d4c8-4340-ba46-e24caf5d5ff0)

위 공식은 만약 x가 k번 군집(정규분포)에서 왔다고 알고 있다면, 그때 𝑥가 어떤 값일 확률은 **정규분포 N(μk,Σk)** 을 따른다는 뜻이다 

<br/>

![image](https://github.com/user-attachments/assets/f3cea58f-c2ce-4d63-b820-935ddc013d2a)

**Marginal p.d.f.** : z를 모를 때 x의 전체 분포. 우리가 실제로 계산해야 할 확률

<br/>

### GMM을 학습할 때 어려운 이유 

아래 공식의 L 표현은 **로그 우도 함수 (log-likelihood)** 를 의미

![image](https://github.com/user-attachments/assets/e53e2b95-361a-4649-b9ce-289115c8b5ae)

파라미터들(π, μ, Σ)**이 합과 로그 안에 같이 섞여 있어서 미분해서 MLE 해를 구하기 어려워진다

<br>

### 이것을 해결하기 위해 EM 알고리즘을 사용한다 

<br/>

### EM 알고리즘을 사용하는 이유 

**숨겨진 변수(z)** 가 있을 때 **파라미터 추정에 딱 맞는 일반적 해법**이기 때문

<br/>

### EM 알고리즘의 핵심 아이디어 

**EM 알고리즘의 핵심 아이디어** : **숨겨진 변수(z)** 가 있을 때, **그것을 추측(예측)하면서** 파라미터를 반복적으로 학습해나가는 것

<br/>

- 만약 각 데이터 𝑥𝑛이 어느 군집(z)을 선택했는지 안다면, **MLE(최대우도추정)는 훨씬 쉬워진다**

- 우리가 GMM 학습이 어려운 주된 이유 **로그 안에 sum이 있어서 미분이 어려웠기 때문**

- 하지만 zn을 알면 기존의 내부 로그는 각각의 모든 가능성을 계산했지만 어디서 왔는지 알면 그 부분만 계산하면 되기 때문에 훨씬 쉬워짐 

<br>

## 과정 참고하기 

### EM 알고리즘 예시

### step 1 : 초기화 

- 군집의 평균, 분산, 비율 랜덤 설정

- 모든 점에 대해 각 군집 확률이 1/3로 균일하게 배분 (빨간/파란/초록 파이 차트로 표시)

![image](https://github.com/user-attachments/assets/e369e37c-d2cd-40d5-ad98-258338abcd96)

<br/>

### step 2 : 각 점의 𝛾𝑛𝑘 계산 후 업데이트 반복 

**1차 시도**

![image](https://github.com/user-attachments/assets/0cce94b5-36fa-4ab1-a00a-d96bc09f3390)

<br/>

**2차 시도**

![image](https://github.com/user-attachments/assets/d5fceb00-b73f-4e77-9a26-6e6aaf03942c)

<br/>

**3차 시도**

![image](https://github.com/user-attachments/assets/8eaf1d1a-f052-499b-a238-739165d15313)

<br/>

**20번째 까지 시도 -> 수렴 완료 판단**

![image](https://github.com/user-attachments/assets/3ac23f2e-809c-4d22-9d6c-6ca912ce2137)

<br/>

각 반복마다

- 군집의 평균(μ), 분산(Σ), 비율(π)이 점점 정제됨

- 타원(정규분포)들이 더 정확하게 데이터 클러스터를 감싸게 됨

- 각 점의 소속 확률(γ)이 점점 뚜렷해짐 (파이차트가 한 색으로 치우침)

<br/>

### 실제 데이터(Bio Assay)에 적용한 결과

### 초기 데이터 

![image](https://github.com/user-attachments/assets/b3dfc09c-0302-4158-849b-62bdc98e1e0d)

<br/>

### EM 알고리즘 적용 후 

![image](https://github.com/user-attachments/assets/06dc1931-ab55-4b32-86f3-fe79a2911a6c)

![image](https://github.com/user-attachments/assets/dacafc60-323a-4265-a681-fe4c5090c3c8)

<br/>

## GMM & K-means 차이점 

**GMM**의 경우 성분의 크기가 **비대칭인 경우도 모델링이 잘 되지만**, **K-means**의 경우 **전체 데이터를 절반으로 나누려는 경향**이 있다 

<br/>

결론은 **K-means는 분산이 작은 큰 클러스터를 과소평가**하고, **분산이 큰 작은 클러스터를 과대평가**할 수 있다 







































