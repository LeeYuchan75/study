## 학습 종류 

**1. 지도 학습**

-  input : 입력값과 정답을 제공

- output : 새로운 입력이 들어왔을 때, 그에 맞는 출력을 예측하는 모델을 학습

**2. 비지도 학습**

- input : 입력 데이터만 주어지고, 정답(출력)은 없음

- output : 데이터 내에 숨겨진 구조나 패턴(군집, 분포 등)을 찾아냄

**3. 강화 학습**

- input: 환경과 상호작용을 반복하며 얻는 입력 (ex: 게임에서 행동 → 보상/패널티 → 다음 상태)

- output : 주어진 목표를 잘 수행하기 위한 전략(policy)을 학습함

<br/>

## 비지도 학습 응용분야 

**1. Visualization (시각화)**

- 데이터셋이나 모델의 출력을 시각적으로 탐색 (ex: 100차원 데이터를 2D/3D로 줄여 시각화해 패턴 파악)

**2. Feature Learning (특징 학습)**

- 자동으로 낮은 차원의 유용한 특징을 학습함, 특히 라벨이 적고 데이터가 많을 때 유리 (ex: 이미지에서 가장 중요한 형태나 패턴 추출)

**3. Image Compression (이미지 압축)**

- JPEG 포맷은 비지도 학습 기법을 도입 중

<br/>

## 비지도 학습 유형 

### 1. Clustering (군집화)

**Clustering** : 데이터들을 유사한 특성을 기준으로 그룹으로 나누는 기법 

ex: 고객을 성향에 따라 그룹으로 나누는 것

![image](https://github.com/user-attachments/assets/cdb47834-14a1-4aaa-8ef0-aca404dacc75)

- x는 d차원의 데이터

- 이를 정수값 하나로 바꿈 (f(x)) = 어떤 그룹 번호

- 즉, **각 데이터가 몇 번 그룹에 속하는지 알려주는 함수**

- 각 그룹 **k는 중심점 역할을 하는 대표 데이터**를 가짐 (ex:  K-평균 (K-means): 가장 대표적인 군집화 알고리즘, 계층적 군집화: 군집들을 계속 병합 또는 분리하는 방식)

<br/>

### 2. Dimensionality Reduction (차원 축소)

**Dimensionality Reduction (차원 축소)** : 데이터의 **복잡한 구조를 간단하게 표현하는 방법**. 중요한 정보만 뽑고 나머지는 버리는 방식

<br/>

![image](https://github.com/user-attachments/assets/3207b3bc-4969-4f8a-8096-b8f567df0461)

- 원래 데이터는 고차원 (d) -> **이것을 중요한 정보를 유지하면서 저차원 (d'≪d)으로 줄임**

- 가장 많이 쓰이는 차원 축소 알고리즘 : Principal Components Analysis (PCA)

- 현대 딥러닝은 이 아이디어를 기반으로함 

<br/>

## Clustering

**Clustering** : 데이터들을 유사한 특성을 기준으로 그룹으로 나누는 기법

Clustering은 일반적으로 **비지도 학습 문제에 해당한다**

느슨하게 말하면, 정답 라벨이 없는 분류 문제이다 

<br/>

- input : 정답이 없는 데이터 N개

- output : k개의 그룹으로 분리 (ex: 고객 1,000명을 4개의 성향 그룹으로 분류)

<br/>

**Clustering 목표** : 이 데이터들을 동질적인(비슷한 성격의) K개의 그룹으로 나누는 것

![image](https://github.com/user-attachments/assets/0ce64ce2-57d7-42ef-a622-0bd220e158d0)

왼쪽과 같이 input 데이터가 주어질 때, 오른쪽과 같이 색으로 각 그룹을 분류함 

<br/>

## 좋은 Clustering 조건 

1. **High within-cluster similarity** : 같은 군집 안에 있는 데이터끼리는 매우 유사해야 한다

2. **Low inter-cluster similarity** : 다른 군집 간에는 서로 다른 특성을 가져야 함

<br/>

## Clustering 주의할 점 

Clustering은 오직 유사성에만 의존하며, 정답 라벨(label)은 없다

라벨이 없다면, **무엇이 비슷한 것인지 정의하기 어렵다**

![image](https://github.com/user-attachments/assets/fcd596d8-86b5-49b3-a65e-a6baaf95872d)

머리카락이 비슷한 사람과 개는 실제로 전혀 다른 존재지만 겉모양만 보면 유사해 보일 수 있다

유사성 기준이 적절하지 않으면 엉뚱하게 그룹화가 될 수 있다

<br/>

따라서 **적절한 거리 측정 방법 또는 유사도 기준을 사용하는 것이 매우 중요**하다

또한 **무엇을 기준으로 클러스터링할 것인가?** 를 명확히 정의하는 것이 중요하다(ex: 색깔을 기준으로 할지, 모양을 기준으로 할지 등등 -> 목적에 따라 유사성이 달라질 수 있음)

(**공부 내용 필기 : k-means 알고리즘은 언제가 수렴함**)

<br/>

**Clustering 예시**

![스크린샷 2025-05-28 160312](https://github.com/user-attachments/assets/0f62f5fb-983f-473d-b179-53083687c304)

<br/>

## Clustering 종류 

**1. Flat or Partitional clustering (평면 또는 분할 클러스터링)**

- 각 클러스터(**그룹**)는 **서로 독립적이다**

<br/>

**2. Hierarchical clustering (계층적 클러스터링)**

![IMG_3354](https://github.com/user-attachments/assets/c5ccc330-82ce-4349-8d88-8948e5d588c4)

- 클러스터 구조를 **트리 형태의 그림(덴드로그램)** 으로 시각화할 수 있음 -> 작은 클러스터들이 모여 큰 클러스터를 이루는 계층적 구조

- 클러스터에서 다양한 수준의 세분화/통합으로 볼 수 있음 → K 값을 조정하면 클러스터 수를 더 적게 또는 더 많이 볼 수 있음

- 즉, 같은 데이터를 가지고도 더 거칠게(3개로), 또는 더 정밀하게(6개로) 나눌 수 있음

<br/>

## Flat Clustering: K-means algorithm

평면 클러스터링에서 K-mean(평균) 알고리즘 데이터 군집화를 위한 가장 유명한 알고리즘 중 하나이다

- N개의 입력 데이터가 주어지고, 각 데이터는 D차원 공간에 있으며, K개의 군집으로 나눌 것

- 초기에 K개의 중심점(μ₁ ~ μ_K)을 무작위로 설정한다 (여기서 k는 군집의 개수다. **즉, 군집마다 중심점이 필요하다**)

- 보통 중심점을 정할 때 랜덤하게 시작하지만, 좋은 초기화에 따라 큰 영향이 끼친다

### K-means 예시 (ex: k = 2) 

![IMG_3386](https://github.com/user-attachments/assets/8499c46a-bd00-4805-8f9b-ce1dd802a0a9)

![IMG_3387](https://github.com/user-attachments/assets/1f99f6bf-ffe0-4006-bbca-be5cda59c8f4)

![IMG_3388](https://github.com/user-attachments/assets/aa3f685d-dd77-434f-90f7-2c1162c070e6)

![IMG_3389](https://github.com/user-attachments/assets/403ff671-8b36-40b0-9677-3d2c3b42d03a)

<br/>

예시 두번째 과정에서 각 데이터 xₙ을 가장 가까운 클러스터 중심(μₖ)에 배정할 때, 거리 계산은 **유클리드 거리(Euclidean distance)** 를 이용한다

![image](https://github.com/user-attachments/assets/54199c5c-62ba-4589-90e7-5e5176c2dd34)

여기서 Ck는 **클러스터 k가 속한 데이터들의 인덱스 집합**이다 

<br/>

## K-means 손실 함수 

- μ₁,…,μ_K는 K개의 클러스터 중심(평균)이라고 하자

- zₙₖ는 0 또는 1 값을 가지며, 만약 xₙ이 클러스터 k에 속하면 1, 아니면 0

- zₙ은 길이 K의 one-hot 벡터로, xₙ이 어느 클러스터에 속했는지를 나타냄 (ex zn = [zn1; zn2; zn3; ... ;znk]

- xₙ의 클러스터 할당에 대한 왜곡(손실)을 다음과 같이 정의함

**중심점과 하나의 데이터와의 손실**

![image](https://github.com/user-attachments/assets/eeaf878d-8438-4984-8698-262c5d04f98e)

<br/>

위 공식을 기반으로 **전체 손실함수는 아래와 같이 정의한다**

![image](https://github.com/user-attachments/assets/759e428d-3918-4bdd-bdb1-fcf7c40a7fc6)

<br/>

### K-means 손실 함수 예시

### step 1 : 상황 분석

- 데이터 : x1 = (1,2), x2 = (2,1), x3 = (5,4)

- 클러스터 중심 : μ1 = (1,1) , μ2 = (5,5)

![image](https://github.com/user-attachments/assets/d0788beb-0f4b-4753-89be-994b96281f12)

x1는 클러스터 1에 할당되었기 때문에 **zn1만 1이고 나머지는 모두 0이다** 

여기서 공식을 보면 시그마 내부에 znk * ||xn - μk||^2 로 되어 있는데 여기서 znk가 1인 부분만 계산이 되고 나머지는 할당이 되지 않았으니 0을 곱하여 계산을 안한다는 의미이다 

<br/>

### step 2 : 공식 적용 

![image](https://github.com/user-attachments/assets/de4cf614-ece1-475e-9fb9-bf09494956cc)

![image](https://github.com/user-attachments/assets/aa81c2d9-80a5-4690-b069-d86885718dd2)

<br/>

### step 3 : 최종 손실 파악 

L(μ,X,Z) = 1 + 1 + 1 = 3 이므로 손실 값은 3이다

즉, K-means 알고리즘은 **이 손실을 최소로 최소화 하는 것이 목적이다**
​
<br/>

### K-means 알고리즘 특징

K-means 알고리즘에서 사용한 손실 함수는 **비볼록 함수**다. 즉, 손실 함수에는 **여러 개의 최소점(해)** 가 존재할 수 있다

이 손실 함수를 정확하게 가장 잘 줄이는 방법을 찾는 건 **매우 어려운 계산 문제(NP-hard 문제)** 다

우리가 쓰는 K-means는 그런 어려운 문제를 **대신해서 풀어주는 쉬운 방법(=휴리스틱)** 이다

여기서 **heuristic (휴리스틱)** 이란 **완벽하진 않지만, 빠르게 괜찮은 답을 찾게 해주는 방법**을 말한다 

<br/>

K-means도 완벽한 해를 찾기는 어렵기 때문에 (NP-hard 문제) 우리가 위에서 했던 heuristic 전략을 쓴다

1. 중심을 무작위로 초기화하고

2. 가장 가까운 중심으로 할당 → 평균으로 중심 갱신

3. 이 과정을 반복해서 손실이 줄어들면 멈춤

<br/>

### K-means 알고리즘의 수렴 

K-means 알고리즘은 아래 두 가지 단계를 계속 번갈아 반복한다 

- Re-assign step → 각 데이터를 가장 가까운 클러스터 중심에 다시 할당

- Update step → 할당된 데이터들을 바탕으로 각 클러스터의 중심점을 평균으로 갱신 

<br/>

이것을 수식으로 표현하면 다음과 같다 

- L : 손실함수

- μ : 중심점

- Z : 클러스터 할당 집합

### Z 업데이트 

![image](https://github.com/user-attachments/assets/31586064-b465-4987-8cce-e0f19e4a52c2)

중심점에 가까운 데이터를 할당하는 과점에서 손실함수가 같거나 낮아진다 

<br/>

### μ를 업데이트

![image](https://github.com/user-attachments/assets/44b8a471-ae26-4425-b98c-25ab36ac89b3)

중심점이 그룹의 평균 위치로 중심 재설정되면서 손실함수가 같거나 낮아진다 

<br/>

즉, K-means 알고리즘은 Z와 μ를 업데이트 과정의 반복인데 **두 과정 모두 손실함수가 같거나 낮아지는 방향**이므로 **최종적으로 수렴하게 된다**

<br/>

## K-means 알고리즘 하이퍼 파라미터 k 선택 

K는 K-means 클러스터링에서 중요한 하이퍼파라미터이다

k를 선택하는 한 가지 방법은 K 값을 여러 개 시도하고, K-means 손실 함수 값과 K의 관계를 그래프로 그린 후 **엘보우 포인트(elbow point)** 를 찾는 것이다 

***엘보우 포인트(elbow point)** : 그래프에서 꺾이는 지점을 의미

![image](https://github.com/user-attachments/assets/3774ea0e-f7d9-46ab-a189-287b417f15ff)

위 예시에서는 K = 6이 엘보우 포인트이다 

즉, 클러스터를 6개로 나눴을 때까지는 손실이 많이 줄다가, 그 이후로는 별로 줄지 않기 때문에 6이 적절한 K라는 의미이다 

<br/>

## K-means 알고리즘 한계 

### 1. K-means는 각 점을 클러스터에 강제로 할당한다는 것이 한계이다 

예를 들어 어떤 데이터가 51% 확률로 A, 49% 확률로 B에 속한다고 해도, K-means는 무조건 하나에만 속하도록 강제로 나눠버린다 

각 클러스터에 속할 **확률(p₁, p₂, …)** 처럼 소프트 할당 개념이 없다

소프트 할당(soft assignment)이란 “이 점은 클러스터 1일 가능성이 70%, 클러스터 2일 확률이 20%...” 이런 식의 불확실성을 표현할 수 있는 방법인데 K-means는 이걸 지원하지 않는다

<br/>

### 2. 클러스터 크기(데이터 개수)가 대략 비슷할 때 잘 동작한다

예를 들어 어떤 그룹은 1000개, 다른 그룹은 5개 이런 식이면 K-means는 잘 분류하지 못 한다 → K-means는 크기가 비슷한 군집에 유리한 알고리즘

가우시안 혼합 모델(GMM) 같은 확률적 클러스터링 기법은 위 문제들(소프트 할당, 다른 크기) 모두를 다룰 수 있다

<br/>

### 3. K-means는 동그란 모양의 군집일 때만 잘 작동한다 

K-means는 동그란 모양의 군집일 때만 잘 작동하고, **복잡한 모양(비볼록, non-convex)** 일 때는 성능이 떨어진다

<br/>

이러한 문제점을 해결한 것이 **Kernel K-mean**이다 

<br/>

## Kernel K-means

- **Kernel K-means 기본 아이디어** : K-means에서 사용하는 유클리드 거리 계산을 **커널 기반 방식**으로 바꾸는 것

![IMG_3424](https://github.com/user-attachments/assets/ad8ea16f-573a-4f23-9916-6eb920bcbf01)

<br/>

기존 K-means를 생각해보면 클러스터 중심 𝜇𝑘와 데이터 𝑥𝑛사이의 유클리드 거리 ||𝑥𝑛 -𝜇𝑘|| ^ 2 으로 구하였다 

하지만 이렇게 거리를 계산하여 **복잡한 구조의 데이터(예: 동심원 형태)에서는 성능이 안 좋았다**

<br/>

**Kernel K-means**는 이를 극복하기 위해 데이터를 **고차원 임베딩 공간으로 매핑하는 ϕ**를 이용해 **커널 함수 기반 거리 계산**을 수행하여 **복잡한 구조도 선형적으로 (직선처럼) 나눌 수 있게 해준다**

<br/>

![image](https://github.com/user-attachments/assets/951cb8cc-de40-4bd3-aa9b-3078046a7a98)

<br/>

k(a, b)는 ϕ(a)ᵀϕ(b)와 같기 때문에 **임베딩을 직접 계산할 필요 없이** 커널만으로 거리 계산 가능 (임베딩 : 데이터를 고차원 벡터 공간으로 변환하는 것)

<br/>

여기서 **주의할 점**은 기존 K-means는 중심점과 데이터의 거리를 그 구하고 해당 데이터의 평균을 구하는 방식을 사용했지만, Kernel K-means **임베딩된 벡터ϕ(x)** 들의 평균을 중심으로 사용해야 한다 -> 문제점은 아님 

<br/>

## 계층적 클러스터링(Hierarchical Clustering)

![image](https://github.com/user-attachments/assets/d3a09bfa-de35-48c2-940f-388ac41352f7)

### Agglomerative (bottom-up) Clustering

- 각 데이터 포인트를 각각 하나의 클러스터로 시작함 → 처음엔 모두 따로따로 있음 (singleton = 1개짜리 클러스터)

- 매 단계마다 가장 비슷한 두 클러스터를 합침 → greedy하게 (= 항상 가장 가까운 것끼리) 병합

- 결국 하나의 클러스터만 남을 때까지 반복 → 그 전까지는 계속 2번 과정 반복

<br/>

### Divisive (top-down) Clustering

- 모든 데이터를 하나의 큰 클러스터로 시작함

- 매 단계마다 가장 덜 비슷한(outliers) 애들을 떼어냄 → "least cohesive" = 응집력이 가장 낮은 것들부터 나눔

- 모든 데이터가 각자 단일 클러스터가 될 때까지 반복

<br/>

이렇게 하여 전체적인 트리를 만든 후 트리의 각 부분을 적절하게 분리하여 여러 클러스터가 나오게 하는 방식

<br/>

## 클러스터끼리의 유사도 

위에서는 두 개의 데이터 포인트의 거리를 구해왔다. 이번 파트는 클러스터 집합의 유사도를 측정하는 3 가지 방식을 배울 것이다 

두 클러스터 R과 S의 유사도를 구하는 방식을 알아보자 

<br/>

### 방법 1 : Min-link or Single-link

R과 S 사이의 **가장 가까운** 두 점 간의 거리
  
![image](https://github.com/user-attachments/assets/dfd34387-1107-493e-ae76-4090eeba031d)

<br/>

### 방법 2 : Max-link or Complete-link

R과 S 사이의 **가장 먼** 두 점의 거리

![image](https://github.com/user-attachments/assets/cb3b8075-efd3-4a6b-8a8a-ab60c6b9d0c8)

<br/>

### 방법 3 : Average-link

R과 S 사이의 모든 점 쌍 간 거리의 **평균**

![image](https://github.com/user-attachments/assets/145aa7fa-3623-47f6-8f1f-b49287190f45)

<br/>

### 그림으로 표현

![image](https://github.com/user-attachments/assets/161ea29c-43af-4a94-88a8-5d6153dd799e)

<br/>

### Flat clustering & Hierarchical Clustering 차이점 

- 플랫 클러스터링은 한 번만 클러스터를 나눠서 **고정된 결과**를 만든다 (ex: K-means는 처음부터 K개로 나누고 끝)

- 계층적 클러스터링은 어디서 트리를 자르느냐에 따라 **다양한 클러스터링 결과**를 줄 수 있다

- 플랫 클러스터링은 보통 **속도가 빠르고 계산이 효율적**이다

- 계층적 클러스터링은 합치고 쪼개는 결정을 많이 하기 때문에 **느릴 수 있다**














































































