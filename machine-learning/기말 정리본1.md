## 8-1 주차 

### 모델 예측이 틀리는 원인

1. **True non-determinism (진정한 비결정성)** -> 동전의 앞면이 나올 확률이 0.7 이라면 항상 그것만 예측 -> 그러나 현실세계에서는 확률적인 경우

2. **Partial observability (부분 관찰 가능성)** -> N-bit parity problem에서 n개의 비트를 모두 모르는 경우

3. **Representational bias (표현 편향)** -> 현실 세계의 데이터를 얼마나 잘 표현

4. **Algorithmic bias (알고리즘 편향)**

5. **Bounded resources (제한된 자원)**

<br/>

### SVM (Support Vector Machines)

**SVM(서포트 벡터 머신)** : **최대한 Margin을 확보하는 결정 경계(Hyperplane)** 를 학습하는 모델 

- Training instances : 학습에 사용되는 데이터 (특징 + 레이블)

- Model parameters : 학습을 통해 구해진 가중치)

- **Hyperplane** : **두 클래스를 나누는 경계 (내적 결과가 0인 지점)**

- Decision function : 어느 클래스에 속하는지 결정

<br/>

### SVM 장점  

- **일반화 성능이 좋음** (이론상 & 실전 모두)

- **소규모 학습 데이터**에도 잘 작동

- **최적의 전역 해**(global optimum) 보장

- **효율적인 알고리즘** 사용 가능

- **커널 트릭(Kernel Trick)** 으로 **비선형 문제도 해결** 가능

<br/>

### SVM의 단점

- 대용량 데이터셋에서는 **훈련 속도가 느릴 수 있음**

- **적절한 커널 함수 및 하이퍼파라미터(C 등)를 선택**해야 함

- 이진 분류가 기본이므로 **다중 클래스 분류 시 one-vs-rest 등 전략이 필요**함

<br/>

### Margin 

**Margin(여백)** : SVM에서 두 클래스를 나누는 결정 경계(Decision Boundary) 와 가장 가까운 데이터 포인트(Support Vector) 사이의 거리

- **margin을 최대화**를 해야하는 것이 핵심 -> 최대화될수록 **더 잘 분류함**

- margin이 **커질수록** capacity(적합할 수 있는 능력)이 **감소하여** **overfitting을 예방** -> **더 단순해짐 + 일반화 성능 증가**

- margin이 **작을수록 Linear decision 개수 증가**, margin이 크면 Linear decision 개수 감소 ->  **margin이 작으면 두 클래스 간의 여유 공간**

![image](https://github.com/user-attachments/assets/45e10ced-107c-432e-8268-b82c7c8b3c6f)

![image](https://github.com/user-attachments/assets/436fe714-d041-470c-87f0-615db9749151)

<br/>

### SVM 목표 : 가장 Margin이 넓은 결정 경계(Hyperplane)를 찾는 것 (이걸로 암기)

두 클래스 사이를 최대한 넓게 벌려주는 분리 경계선을 학습하여 **일반화 오류(새로운 데이터에서의 오차)를 최소화**하는 것이 SVM의 핵심 목표

<br/>

### SVM이 underfitting(과소적합)이 될 가능성이 적은 이유

SVM은 마진을 최대화하면서도 **슬랙 변수와 커널 기법을 통해 모델 복잡도를 유연하게 조절하기 때문**

<br/>

### 복습

**로지스틱 회귀 (Logistic Regression)** : 입력값에 대해 0과 1 사이의 확률을 예측하는 선형 분류 모델

**선형 회귀 (Linear Regression)** : 연속적인 수치값 예측 (예: 집값)

**소프트맥스 회귀 (Softmax Regression)** : 3개 이상의 클래스 중 하나를 예측하는 다중 클래스 분류 모델로 확률의 총합은 항상 1

<br/>

### 로지스틱 회귀와 SVM 공통점 

**결정 경계(Decision Boundary) 를 학습하여, 데이터를 두 개의 클래스 구분함**


### 로지스틱 회귀 관점

![IMG_3482](https://github.com/user-attachments/assets/6813db63-540a-43c8-8604-7942161ccd32)

<br/>

![image](https://github.com/user-attachments/assets/c73f8404-f5da-4d6a-8c67-d2b1349a6fec)

<br/>

![image](https://github.com/user-attachments/assets/e5d74df7-adda-40ff-8b74-86df824fe82b)

<br/>

### SVM 공식에서 C의 역할 

**C가 클 때** -> **분류가 잘 됨** -> **오차 최소화하는데 집중**(margin이 줄어들더라도 오차가 없도록 하는 것) -> **margin 감소** -> **overfitting 위험**

**C가 작을 때** -> **상대적으로 분류 능력 떨어짐** -> **정규화에 집중** -> **일부 오차 허용**(오차가 있더라도 ∥θ∥를 최소화하는데 집중) -> **margin 증가**(정규화는 세타를 최소화하므로 이것은 margin을 극대화 시키는 것과 동일) -> **overfitting 감소**

<br/>

추가적으로 설명하면 **C가 작은 경우 hinge loss의 중요도가 낮아지고** 목적함수에서 **정규화 항이 상대적으로 더 크게 작용**하기 때문에 최적화는 그 항을 더 적극적으로 줄이게 된다

![image](https://github.com/user-attachments/assets/26303005-63b9-412f-94ba-6a82c8c67f99)

<br/>

### SVM 함수 표현

![IMG_3480](https://github.com/user-attachments/assets/7078cdf4-6b2f-4fd2-8847-4ec31cfee331)

<br/>

![IMG_3481](https://github.com/user-attachments/assets/22029a5b-6d07-45d2-94c9-5788d7ab9311)

- SVM은 아래에 존재하는 **Hinge(힌지) 함수를 손실 함수**로 사용한다

- Hinge(힌지) 함수는 **일정 범위를 넘어가면 Loss를 0으로** 하는 함수

<br/>

### support vector

**support vector** : 결정 경계(또는 초평면, hyperplane)에 가장 가까이 위치한 훈련 샘플

![image](https://github.com/user-attachments/assets/45e10ced-107c-432e-8268-b82c7c8b3c6f)

- **support vector는 결정 경계를 정의하는 중요한 역할**

- **θ**는 SVM(Support Vector Machine)에서 **결정 경계(Hyperplane) 의 방향 벡터(Weight Vector)**

<br/>

### margin과 θ는 반비례 관계

- ∥θ∥ 가 커지면 margin이 작아지고,

- ∥θ∥ 가 작아지면 margin이 커진다

<br>

### 벡터 내적 복습

내적을 이용하여 각 Support Vector가 결정 경계에서 어느 정도 떨어져 있는지 판단

![IMG_3181](https://github.com/user-attachments/assets/1b4ef1b4-ddcb-4a21-a181-7538c303500b)

<br/>

## Hyperplane(초평면)

![IMG_3177](https://github.com/user-attachments/assets/29d75984-1cb0-4af1-ba0f-c48ffa37a2d9)

<br/>

### SVM 최적화 과정

**1. 초기화 단계**

초기 Hyperplane을 설정한다

<br/>

**2. 최적화 단계**

모든 데이터를 순차적으로 확인하여 각 데이터 포인트가 마진 조건을 만족하는지 검사한다

<br/>

**3. 업데이트 단계**

만약 마진을 침범하거나 오류가 발생하면, Hyperplane을 업데이트를 한다

이때, 업데이트 후 마진 범위를 침범한 포인트는 새로운 Support Vector가 될 수 있다

<br/>

이것을 최적화가 수렴할 때까지 반복한다

즉, 학습 과정에서 Support Vector를 이용하여 **θ 값을 반복적으로 조정**하면서 최적의 Hyperplane를 찾는다 

![image](https://github.com/user-attachments/assets/4c4711e3-cc8c-461e-9f1e-f33b99be4cae)

위 이미지를 설명하자면

- 왼쪽 이미지에서 p가 잘을 때, 양성 클래스일 때는 1 이상이어야 하니까,**∥θ∥2를 키워야하고**, 

- 음수 클래도 마찬가지로 -1 이하여야 하니까 **∥θ∥2를 키워야한다** (cosθ 가 음수) 

- 세타를 키운다는 것은 시각적으로는 세타가 늘어나지만, **margin과 세타는 반비례이기 때문에 실제로는 margin이 줄어든다**

- 만약 p가 크다면, 오른쪽 그림과 같이 줄여도 된다 **(강제가 아님)**

<br/>

![image](https://github.com/user-attachments/assets/3d9ba10c-9f71-4891-9270-9d4ace1d2808)

<br/>

## 8-2 주차 

### SVM Dual Problem

SVM Dual Problem : 기존 SVM과 다르게 **패널티**를 주며 최적의 경계를 찾는 방식

더 정확하게 말하면 

SVM Dual Problem : **라그랑지 이중 형태(Dual Form)** 로 바꾸어 계산 효율을 높이고, 커널 함수 적용이 가능하도록 만든 형태

<br/>

### SVM Dual Problem 사용 이유

SVM Dual Problem을 사용하는 이유는 앞에서 우리가 한 **SVM 문제는 Primal Form으로 표현되는데, 이걸 직접 풀면 계산이 복잡**해지기 때문

<br/>

### SVM Dual Problem 장점 

- 서포트 벡터 중심 구조

- 고차원 적합

- **내적을 커널 함수로 대체 가능**

<br/>

### SVM Dual Problem 단점 

- 훈련 속도 느림, 

- n^2  크기의 행렬 사용

- 큰 데이터셋 비효율적

<br/>

### SVM Dual Problem 공식 

![IMG_3483](https://github.com/user-attachments/assets/8ffecf22-9d83-43a7-966f-02fc592dd3bf)

<br/>

### Lagrangian 역할

- 모델 복잡도를 최소화하기 위해 θ를 줄임

- 제약 조건을 만족시키기 위해 **𝛼i 라는 변수를 조절함**

- 만약 어떤 데이터가 **결정 경계에 아주 가깝다면, 그 데이터의 𝛼𝑖값이 커지고** 반대로 **멀리 떨어져 있으면 𝛼𝑖 값이 작아진다**

- 또한, ai의 범위 조건이 없어서 ai가 음수도 가능해진다면, 방향이 바뀌어버리는 문제가 발생

![IMG_3486](https://github.com/user-attachments/assets/19255dbb-d2dd-446b-9ad3-44df3a8310ac)

<br/>

### Primal 문제를 Dual Representation으로 변환하기

![IMG_3279](https://github.com/user-attachments/assets/3c06e47b-b736-4bd7-ac6d-933809b47098)

![IMG_3487](https://github.com/user-attachments/assets/1615c8fa-157e-4803-8daf-fa49fcef05e5)

![IMG_3484](https://github.com/user-attachments/assets/4fd98289-ba2f-4cb5-b0a0-4899d6d78501)

<br/>

### 최적화 이후

![IMG_3212](https://github.com/user-attachments/assets/a8521e56-cff1-4036-8458-5e0c95f0c567)

<br/>

## 비선형인 경우 

![IMG_3211](https://github.com/user-attachments/assets/ea059431-5daf-4116-91f2-f0d02e59d9c0)

<br/>

## SVM 예시 

![스크린샷 2025-05-23 130015](https://github.com/user-attachments/assets/809cade1-e3a6-4111-bf5e-b77c5204b11f)

![스크린샷 2025-05-23 130019](https://github.com/user-attachments/assets/e2f530c8-a7be-4de7-90a7-d8ae1887da19)

![스크린샷 2025-05-23 130024](https://github.com/user-attachments/assets/a16e18d6-58f9-496f-af5d-f27a2419b64f)

<br/>

### kernel method 

**Kernel Method** : 입력 데이터를 **고차원 특성 공간(Feature Space)** 으로 사상(매핑)한 뒤, 이 공간에서 선형 분리를 수행하는 방법

**핵심 아이디어** : 특성 공간에서의 내적을 계산할 때 명시적으로 고차원 벡터로 변환하지 않고, **커널 함수 K(xᵢ, xⱼ)** 를 이용해 간접적으로 계산 -> **복잡하게 계산하지 않아도 됨**

**SVM은 특징공간 ϕ(x)로 학습**

<br/>

### kernel method 목적 및 사용이유 

1. **비선형 데이터를 선형 분리가 가능하도록 하기 위함**

2. 차원을 고차원으로 변환히여 계산량이 급격히 증가할 때, Φ(xi)와 Φ(xj)와 같이 특징 공간에 직접 매핑하여 내적을 구하는 것 보다 **훨씬 효율적이기 때문**

<br/>

- 이러한 이유로 SVM 알고리즘에서 기존의 내적 <x1, xj>를 사용하지 않고 **커널 함수 K(xi,xj)를 사용**

- 실제로 커널 함수는 고차원으로 매핑된 결과를 계산하지 않고도, 마치 고차원에서 **내적을 계산한 것과 동일한 값을 반환**

<br/>

### kernel method 장점 

- **고차원 비선형 분류 문제 해결 가능**

- 명시적 매핑 없이 계산 가능 (효율성) -> **복잡하게 계산하기 않아도 됨**

- **다양한 커널 함수 선택 가능** → 도메인 특화 모델 가능

- **이론적으로도, 실험적으로도 성능이 좋다**

- 일반적인 경우 만약 차원이 너무 크다면 연산이 많아짐 -> 이것을 Kernel method를 이용하면 간단하게 구할 수 있음

<br/>

### kernel method 단점

- **매우 큰 데이터셋에서는 학습/예측이 느림**

- **커널 종류를 선택하고, 그에 맞는 파라미터도 조절**해야 한다

- 커널 선택과 하이퍼파라미터 튜닝이 성능에 매우 큰 영향을 줌

- 적절하지 않은 커널 선택 시 과적합 또는 과소적합 발생 가능

<br/>

### Kernel method 

**Kernel을 활용하게 된 배경**

![IMG_3214](https://github.com/user-attachments/assets/4df5b4f0-ff02-4479-8bab-54944e837989)

SVM은 특징공간 ϕ(x)로 학습 -> 만약 차원이 너무 크다면 연산이 많아짐 -> Kernel method로 해결


<br/>

### Kernel method 공식 

![image](https://github.com/user-attachments/assets/bfb38d3f-7035-4f84-b503-9662153b9a60)

K(xi,xj)는 고차원 공간에서 매핑된 벡터 Φ(xi)와 Φ(xj)의 **내적(Inner Product)** 을 의미

<br/>

### The Polynomial Kernel (다항 커널)

![image](https://github.com/user-attachments/assets/3e25b0f7-bf96-47e0-947d-bd014cec4d63)

**다항 커널(Polynomial Kernel)** : Polynomial Kernel은 두 벡터 𝑥𝑖와 xj의 내적(Dot Product)을 d 제곱한 것

<br/>

### 장점

- **시각적 패턴 인식에 유용** -> **다항 커널이 복잡한 패턴을 쉽게 학습하기 때문**

- 다양한 차수(d)를 통해 모델 복잡도 조절 가능

- 입력 변수 간의 다항식 관계를 자동으로 고려

<br/>

### 단점 

- 차수가 높아질수록 과적합(overfitting) 위험 증가

- 차수 d, 상수 c 선택이 성능에 민감

<br/>

### The Gaussian Kernel

![IMG_3216](https://github.com/user-attachments/assets/4a8b4cc0-cd1a-4732-978b-214675ad3aac)

- 두 벡터가 동일할 때 (𝑥𝑖=𝑥𝑗), 유사도가 **최대가 되어 1이 되어야 함** (지수가 0이 되기 때문)

- **σ가 작아질수록 더 날카로워지고**, 이것은 **극소수의 support vector에 의존**한다는 의미

- **σ가 커질수록 더 분포되어 있고**, 이것은 **support vector의 영향력을 분산**한다는 의미
  
<br/>

### Gaussian Kernel 사용 이유 

1. 입력 벡터 간 거리만으로 유사도를 계산할 수 있기 때문 

2. 비선형 경계를 학습할 수 있는 유연성이 매우 큼

<br/>

### Gaussian Kernel의 목적

선형 분리가 어려운 데이터를 무한 차원의 고차원 특징 공간으로 매핑하여 거기서 선형 초평면으로 분류하려는 목적

<br/>

### Gaussian Kernel 특징 

- **무한 차원의 특징 공간**에 해당하며, 이 공간에서 데이터의 **위치 차이를 중심으로 유사도를 측정**

- 두 포인트가 **가까울수록 내적값(유사도)이 1**에 가깝고, **멀수록 0**

- **지역적인(Local) 영향력**을 잘 포착할 수 있는 특성을 가짐

<br/>

### Gaussian Kernel 장점 

1. 고차원 (무한 차원) 특징 공간을 암묵적으로 사용할 수 있음

2. 두 벡터 간의 거리만으로 계산 가능 → 입력 차원수에 덜 민감

3. 강력한 비선형 분류 성능

<br>

### Gaussian Kernel 단점

1. 하이퍼파라미터 𝜎값 선택이 매우 중요

2. 데이터 수가 많을수록 계산량이 크게 증가

3. 직관적으로 해석이 어려움 (무한 차원 특징 공간)

<br/>

### Gaussian Kernel은 거리 기반**이기 때문에 반드시 정규화가 필요하다 

**예시**

![image](https://github.com/user-attachments/assets/fc9a4aed-cdfa-4b4f-b4df-559cfdd48426)

위 경우 나이도 중요한 특징인데도 모델은 키에만 집중하게 됨

이것을 0~1사이의 범위로 축소하여 계산 

<br/>

### 다항 커널 & 가우시안 커널 차이점 

**다항 커널**은 **내적을 다항식으로 확장해 전역적인 패턴**을 학습하고, **가우시안 커널**은 **거리 기반으로 국소적인 유사도**에 집중해 비선형 경계를 학습

<br/>

### 가우시안 커널 예시 

![IMG_3489](https://github.com/user-attachments/assets/45cd0784-8bcb-4fdc-88f7-534efd12a167)

<br/>

### SVM에서 선택해야 할 요소

- **매개변수 C를 선택** : C는 오차를 얼마나 허용할지 조절하는 하이퍼파라미터

- **어떤 커널 함수를 쓸지 선택** : 커널 함수 : 데이터를 고차원으로 옮겨서 선형 분리가 가능하게 만드는 함수

![image](https://github.com/user-attachments/assets/a00078da-c03c-4af0-b096-f71836f2783e)

<br/>

### 멀티클래스 SVM

![image](https://github.com/user-attachments/assets/79d06bbc-736e-4662-b3a6-383e799c531f)

- **여러 개의 SVM을 사용** -> 즉, 일대다(one-vs-rest) 방식

- 만약 분류할 클래스가 K개 라면, K개의 SVM을 학습하여 각 클래스마다 하나씩 분리하는 모델을 만들면, 결과적으로 각 클래스에 대한 가중치인 θ가 k개 만들어진다

- 입력 벡터 x에 대해 i번째 θ 와 x를 곱한 값이 가장 큰 클래스를 선택하는 것이다

<br/>

### 멀티클래스 SVM 예시 

분류할 클래스가 3개 있다고 해보자

- 클래스 1: 고양이

- 클래스 2: 강아지

- 클래스 3: 토끼

우리는 이미 각 클래스마다 SVM을 훈련해서

- 고양이용 : θ(1)

- 강아지용 : θ(2)

- 토끼용 : θ(3)
 
를 만들어 줬다고 하자 

만약 입력 데이터로 어떤 사진이 들어오면

이 데이터를 가중치와 계산하기 위해 숫자 벡터 x = [x1, x2, ... , xd] 로 표현하고 각 가중치와 계산한다 

<br/>

(예시로 계산, T는 전치 행렬 의미)

- 고양이 점수 = θ(1)T x X = 3.2 

- 강아지 점수 = θ(2)T x X = 1.8 
 
- 토끼 점수 = θ(3)T x X = -0.4

위 결과에서 가장 큰 값을 해당 클래스로 판단하는 것이다 

<br/>

### SVM & Logistic Regression 차이 

1. 특징 수가 데이터 수보다 많을 경우 -> 로지스틱 회귀나 선형 커널 SVM을 사용

2. 특징 수는 작고, 데이터 수는 중간 정도일 경우 -> 가우시안 커널 SVM을 사용

3. 특징 수는 작지만, 데이터 수가 아주 많은 경우 -> 특징을 더 만들고, 커널 없는 로지스틱 회귀나 SVM을 사용
































































































