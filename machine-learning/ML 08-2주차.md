## SVM Dual Problem

**SVM Dual Problem**은 기존 SVM과 다르게 **패널티를 주며 최적의 경계를 찾는 방식**이다

Dual : 변수가 2개라는 의미 (알파, 세타)

SVM Dual Problem을 사용하는 이유는 앞에서 우리가 한 SVM 문제는 Primal Form으로 표현되는데, **이걸 직접 풀면 계산이 복잡**해지기 때문이다

SVM Dual Problem에서는 **Lagrangian**를 사용한다 

![IMG_3486](https://github.com/user-attachments/assets/19255dbb-d2dd-446b-9ad3-44df3a8310ac)

Lagrangian은 두 가지 역할을 한다

- 모델 복잡도를 최소화하기 위해 θ를 줄임

- 제약 조건을 만족시키기 위해 𝛼i 라는 변수를 조절함

만약 어떤 데이터가 결정 경계에 **아주 가깝다면**, 그 데이터의 𝛼𝑖값이 **커지고**

**반대로 멀리 떨어져** 있으면 𝛼𝑖 값이 **작아진다**

또한, ai의 범위 조건이 없어서 ai가 음수도 가능해진다면, **방향이 바뀌어버리는 문제**가 발생한다
<br/>

## Primal 문제를 Dual Representation으로 변환하기
![IMG_3279](https://github.com/user-attachments/assets/3c06e47b-b736-4bd7-ac6d-933809b47098)

![IMG_3487](https://github.com/user-attachments/assets/1615c8fa-157e-4803-8daf-fa49fcef05e5)

![IMG_3484](https://github.com/user-attachments/assets/4fd98289-ba2f-4cb5-b0a0-4899d6d78501)

<br/>

## 최적화 이후 

![IMG_3212](https://github.com/user-attachments/assets/a8521e56-cff1-4036-8458-5e0c95f0c567)

<br/>

## 비선형인 경우 

![IMG_3211](https://github.com/user-attachments/assets/ea059431-5daf-4116-91f2-f0d02e59d9c0)

<br/>

## SVM 예시 

![스크린샷 2025-05-23 130015](https://github.com/user-attachments/assets/809cade1-e3a6-4111-bf5e-b77c5204b11f)

![스크린샷 2025-05-23 130019](https://github.com/user-attachments/assets/e2f530c8-a7be-4de7-90a7-d8ae1887da19)

![스크린샷 2025-05-23 130024](https://github.com/user-attachments/assets/a16e18d6-58f9-496f-af5d-f27a2419b64f)

<br/>

## Kernel method 

**Kernel을 활용하게 된 배경**

![IMG_3214](https://github.com/user-attachments/assets/4df5b4f0-ff02-4479-8bab-54944e837989)

<br/>

## Kernels

![image](https://github.com/user-attachments/assets/bfb38d3f-7035-4f84-b503-9662153b9a60)

K(xi,xj)는 고차원 공간에서 매핑된 벡터 Φ(xi)와 Φ(xj)의 **내적(Inner Product)** 을 의미한다

kernel를 사용하는 이유는 차원을 고차원으로 변환히여 계산량이 급격히 증가할 때, Φ(xi)와 Φ(xj)를 직접 매핑하여 내적을 구하는 것 보다 **훨씬 효율적이기 때문**이다

이러한 이유로 SVM 알고리즘에서 기존의 내적 <x1, xj>를 사용하지 않고 커널 함수 K(xi,xj)를 사용한다 

<br/>

실제로 커널 함수는 고차원으로 매핑된 결과를 계산하지 않고도, 마치 고차원에서 내적을 계산한 것과 동일한 값을 반환한다 

<br/>

## The Polynomial Kernel (다항 커널)

![image](https://github.com/user-attachments/assets/3e25b0f7-bf96-47e0-947d-bd014cec4d63)

**다항 커널(Polynomial Kernel)** : Polynomial Kernel은 두 벡터 𝑥𝑖와 xj의 내적(Dot Product)을 d 제곱한 것이다 

**다항 커널**을 사용하면 **시각적 패턴 인식-** 에 유용하다.

이미지나 시각적 데이터는 고차원에서 패턴을 잘 구분할 수 있어서 Polynomial Kernel을 사용하면, 이미지의 복잡한 패턴도 쉽게 학습할 수 있다 

<br/>

## SVM & Kernel 공식 

SVM은 내적을 **kernel function으로 대체하여 효율성을 증가시킬 수 있다는 장점**을 가지고 있다 

![IMG_3215](https://github.com/user-attachments/assets/df62a95f-2d98-4f4a-9be4-8b8b97fecdfd)

<br/>

## The Gaussian Kernel

![IMG_3216](https://github.com/user-attachments/assets/4a8b4cc0-cd1a-4732-978b-214675ad3aac)

**Gaussian Kernel**은 **Radial Basis Function (RBF) Kernel**이라고도 부른다

두 벡터 𝑥𝑖와 𝑥𝑗 사이의 유사도를 위와 같이 수식으로 나타낸다 

<br/>

**Gaussian Kernel의 성질**

- 두 벡터가 동일할 때 (𝑥𝑖=𝑥𝑗), 유사도가 **최대가 되어 1이 된다** (지수가 0이 되기 때문)

- 두 벡터 사이의 거리가 멀어질수록 유사도는 **지수적으로 감소**

- Gaussian Kernel은 거리 계산에 민감하기 때문에, **Feature Scaling (정규화)** 이 필요하다

**위 성질 중 가장 중요한 것은 정규화이다**

**SVM은 거리 기반**이기 때문에 **반드시 정규화를 하여야 정확한 값**을 얻을 수 있다

<br/>

또한 σ가 작아질수록 더 날카로워지고, 이것은 **극소수의 support vector에 의존**한다는 의미이다 

반대로 σ가 커질수록 더 분포되어 있고, 이것은 **support vector의 영향력을 분산**한다는 의미이다 

<br/>

## 가우시안 커널 예시 1

![IMG_3489](https://github.com/user-attachments/assets/45cd0784-8bcb-4fdc-88f7-534efd12a167)

## 가우시안 커널 예시 2

![image](https://github.com/user-attachments/assets/26f1d112-63c8-4ab3-9a4f-af148f693a67)

<br/>

## P.55 ~ P.56 참고만 하기 

<br/>

## SVM에서 선택해야 할 요소 

예측을 잘 하기 위해 SVM을 이용하여 최적의 파라미터를 정할 때 다음을 직접 정해줘야한다 

1. 매개변수 C를 선택 : C는 오차를 얼마나 허용할지 조절하는 하이퍼파라미터

2. 어떤 커널 함수를 쓸지 선택 : 커널 함수 : 데이터를 고차원으로 옮겨서 선형 분리가 가능하게 만드는 함수

![image](https://github.com/user-attachments/assets/a00078da-c03c-4af0-b096-f71836f2783e)

<br/>

## 멀티클래스 SVM

![image](https://github.com/user-attachments/assets/79d06bbc-736e-4662-b3a6-383e799c531f)

SVM은 기본적으로 2클래스 분류기이므로 다중 클래스 문제에서는 **여러 개의 SVM을 사용한다** 

**즉, 일대다(one-vs-rest) 방식을 사용한다**

만약 분류할 클래스가 K개 라면, K개의 SVM을 학습하여 각 클래스마다 하나씩 분리하는 모델을 만들면, 결과적으로 각 클래스에 대한 가중치인 θ가 k개 만들어진다 

입력 벡터 x에 대해 i번째 θ 와 x를 곱한 값이 가장 큰 클래스를 선택하는 것이다 

이것이 어떻게 활용되는지 다음 예시를 함께 살펴보자 

<br/>

**예시**

분류할 클래스가 3개 있다고 해보자

- 클래스 1: 고양이

- 클래스 2: 강아지

- 클래스 3: 토끼

우리는 이미 각 클래스마다 SVM을 훈련해서

- 고양이용 : θ(1)

- 강아지용 : θ(2)

- 토끼용 : θ(3)
 
를 만들어 줬다고 하자 

만약 입력 데이터로 어떤 사진이 들어오면

이 데이터를 가중치와 계산하기 위해 숫자 벡터 x = [x1, x2, ... , xd] 로 표현하고 각 가중치와 계산한다 

<br/>

(예시로 계산, T는 전치 행렬 의미)

- 고양이 점수 = θ(1)T x X = 3.2 

- 강아지 점수 = θ(2)T x X = 1.8 
 
- 토끼 점수 = θ(3)T x X = -0.4

위 결과에서 가장 큰 값을 해당 클래스로 판단하는 것이다 

<br/>

## SVM & Logistic Regression

N이 학습 데이터 개수이고 d가 특징(피처) 개수일 때,  

### 1. 특징 수가 데이터 수보다 많을 경우 -> 로지스틱 회귀나 선형 커널 SVM을 사용

특징이 많고 데이터는 적을 때는 복잡한 모델은 과적합 위험이 큼

그래서 단순한 선형 모델이 더 안정적임

로지스틱 회귀와 선형 SVM은 이럴 때 효과적

<br/>

### 2. 특징 수는 작고, 데이터 수는 중간 정도일 경우 -> 가우시안 커널 SVM을 사용

데이터가 적당히 많고, 특징 수가 작을 때는 비선형 패턴도 안전하게 학습 가능

이럴 때는 복잡한 경계를 학습할 수 있는 **가우시안 커널이 잘 맞음**

<br/>

### 특징 수는 작지만, 데이터 수가 아주 많은 경우 -> 특징을 더 만들고, 커널 없는 로지스틱 회귀나 SVM을 사용

데이터는 아주 많은데, 특징이 너무 적으면 **표현력이 부족하다**

그래서 새로운 특징 만들고, 단순한 모델(커널 없이)로 학습하는 게 효율적이다 

<br/>

## 장단점 & 결론 

**내용 정리**

1. **SVM**은 최적의 선형 분리 경계를 찾는다

2. **커널 트릭**을 사용하면, **SVM이 비선형 경계**도 학습할 수 있다

<br/>

**장점** 

1. 이론적으로도, 실험적으로도 **성능이 좋다**

2. 다양한 종류의 커널을 지원한다

<br/>

**단점**

1. 큰 데이터셋에 대해선 학습/예측 속도가 느릴 수 있다 (하지만 다른 알고리즘에 비해 상대적으로 빠른 편!)

2. 커널 종류를 선택하고, 그에 맞는 파라미터도 조절해야 한다




















