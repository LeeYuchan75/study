## GMM 시각적 예시

![image](https://github.com/user-attachments/assets/37213c83-9ce0-49ae-bca3-c09893609a17)

- Samples from p(x) (왼쪽 상단) : 혼합 분포 p(x) 로부터 샘플링된 데이터. 여러 분포에서 생성된 점들이 섞여 있음

- Component Distributions (오른쪽 상단) : GMM을 구성하는 개별 Gaussian 성분(즉, 분포들)의 형태를 등고선(Contour plot)으로 나타낸 것. 붉은색, 초록색, 파란색으로 표현된 세 개의 분포가 존재함

- Samples labeled using their true component (왼쪽 하단) : 각 점이 실제 어떤 성분(Component)에서 생성되었는지를 알고 있을 때의 라벨링 결과. 각 클러스터에 단일한 색이 배정되어 있음

- Soft clustering learned by a Gaussian mixture model (오른쪽 하단) : GMM으로 학습한 결과. **겹치는 부분에 있는 점들은 서로 다른 색**이 섞여 있음. 이는 **GMM이 각 점에 대해 어느 성분에 속할 확률**을 학습하기 때문임

<br/>

## GMM & K-means 차이점 

![image](https://github.com/user-attachments/assets/a192294c-df70-4352-aa7c-9918cac51794)

- K-means : 각 점을 가장 가까운 중심(centroid) 기준으로 나눈 결과

- EM Clustering (GMM) : GMM을 이용한 결과. 각 점에 대해 가장 확률이 높은 클러스터를 할당함

<br/>

GMM과 K-mean의 핵심적인 차이점은 K-means는 GMM과 달리 **동일한 크기의 클러스터를 만들려는 경향**이 있다는 것이다

<br/>

### 예시 

데이터가 다음과 같은 2개의 클러스터로 구성되어 있다고 가정하자

- Cluster A: 밀집된 Gaussian 분포 (평균 𝜇 = 0, 분산 𝜎^2 = 1, 1000개 샘플)

- Cluster B: 넓게 퍼진 Gaussian 분포 (평균 𝜇 = 10, 분산 𝜎^2 = 10, 100개 샘플)

<br/>

- **GMM의 경우** 각 점이 A일 확률, B일 확률을 모두 계산함

- GMM은 실제로 점의 수가 많은 클러스터에 더 많은 mixing weight(혼합 가중치) πₖ를 학습하기 때문에 Cluster A에는 더 많은 점, Cluster B에는 적은 점이 할당됨

- 즉, 성분 간 **크기가 비대칭인 경우도 잘 모델링 가능**

<br/>

- **K-means의 경우** 단지 중심과의 거리만을 기준으로 클러스터를 나누므로, 전체 데이터를 거의 500:500 정도로 비슷하게 두 클러스터에 나누는 경향이 있음

- K-means는 중심들이 비슷한 거리 간격에 있으면, 공간을 절반씩 나눠버려서 데이터 수와 상관없이 대충 500:500처럼 나뉘는 경우가 많음

- 이로 인해 실제보다 Cluster B가 과대평가되고 Cluster A가 **과소평가되는 문제가 발생할 수 있다**

<br/>

결론은 **K-means는 분산이 작은 큰 클러스터를 과소평가**하고, **분산이 큰 작은 클러스터를 과대평가**할 수 있다 

<br/>

## Mixture Models(혼합 모델)의 확장 

- 다른 종류의 성분 분포도 사용할 수 있다

- **HMM**과 같은 시퀀스(순차) 데이터 모델도 혼합 모델로 볼 수 있다

- 시간의 흐름이 있는 데이터(예: 음성, 글자, 센서값 등)를 다룰 때 사용하는 HMM도, 각 시점마다 다른 분포(Gaussian 등)를 섞어 쓰는 방식이므로 혼합 모델의 일종

- HMM과 같은 순차 데이터에서는 다음과 같이 클러스터가 결젇된다 

![image](https://github.com/user-attachments/assets/d9636525-63cb-4e9b-95b0-6a1487c2ab89)

- 성분 선택 확률이 이전 상태에 의존. 즉, **현재 클러스터가 이전 클러스터에 영향**을 받는다

<br/>

- 또한, 지도학습 문제에서도 사용된다 -> **mixture of experts**

- 데이터 𝑥𝑛에 대해 먼저 K명의 전문가 중 하나를 선택하고, 그 전문가의 예측 모델을 사용해 P(yn|xn)를 계산

- Experts와 각 데이터가 어떤 전문가에게 배정될지는 GMM처럼 반복적으로 학습할 수 있다 (EM 알고리즘처럼, “지금 이 데이터는 전문가 A일 확률이 얼마”라고 가정하고, 반복하면서 점점 전문가와 데이터 연결)

<br/>

- 생성 기반 분류에도 사용될 수 있다 -> **naïve Bayes**

![image](https://github.com/user-attachments/assets/82c01913-4d49-4654-8d6f-4e2586a9bdee)

- 클래스 확률은 사전확률과 조건부 확률의 곱에 비례한다

- 클래스별 사전확률과 조건부 분포는 학습 데이터를 통해 효율적으로 추정할 수 있다

<br/>

## p.40 부터는 EM 알고리즘 내용인데 전에 했던거라 수업을 자세히 안함 


















