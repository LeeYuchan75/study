## Machine Learning Design

머신 러닝의 구조는 다음과 같이 되어 있다 

**1. model family / hypothesis space**

모델이 파라미터 벡터 β ∈ ℝᴰ 의 차원에 따라 결정됨 -> β 의 개수에 따라 모델이 만들어지고 이것을 최적화하는 것이 목표 

<br/>

**2. objective function**

손실함수와 같은 의미로 목적 함수 L(β; Z)는 주어진 데이터셋 Z에 대해 좋은 β를 정의하는 기준

<br/>

**3. Optimization Approach**

주어진 목적 함수를 최소화(또는 최대화)하기 위해 파라미터 β를 찾는 과정

**머신러닝은 기본적으로 위의 3가지 구조로 되어 있지만 모든 머신러닝이 이러한 구조를 갖는 것은 아님 -> KNN**

<br/>

## KNN(K-Nearest Neighbor)

**KNN의 핵심은 별도의 학습 없이 거리를 계산하여 값을 구한다는 것이다**

<br/>

**KNN 과정**

1. 새로운 데이터 입력

2. KNN은 별도의 학습 없이, 기존 데이터를 그대로 저장

3. 새로운 입력이 들어오면, 기존 데이터 중에서 가장 가까운 K개를 찾아서 계산

<br/>

KNN은 가중치나 파라미터를 학습하지 않기 때문에, 일반적인 의미의 학습 과정이 없음. **즉, 데이터 자체가 예측의 기준이다**

**위와 같은 이유로 KNN은 non-parametric machine learning appreach라고 부름**

또한, KNN은 단순하면서 성능도 나쁘지 않기 때문에 base line으로 많이 사용함 

<br/>

## KNN 장점 

1. 구현하기 쉬움 -> 데이터를 저장만 하면 됨

2. 다양한 함수를 모델링하는 데 좋음 -> 데이터만 충분하면 선형, 비선형, 임의의 분포도 근사 가능

3. 해석이 쉬움 -> 예측할 때 어떤 이웃들이 영향을 미쳤는지 명확히 확인 가능. "왜 이런 예측이 나왔는가?"를 데이터 기반으로 직관적으로 설명 가능

<br/>

## KNN 단점 

1. 의미 없는 feature가 하나라도 포함되면, 전체 거리 계산 결과가 왜곡될 수 있음. **대부분 distance function은 모든 feature를 동등하게 생각함**

2. 차원이 많아질수록 모든 데이터 포인트가 서로 멀어지기만 함 즉, 가까운 이웃을 찾기가 의미 없어짐 ("차원의 저주")

3. 예측할 때마다 모든 학습 데이터를 대상으로 거리 계산을 해야 함. 따라서 샘플 수 feature 수나 data가 많아지면 예측 시간이 매우 느려짐

3번을 상세히 설명하면, KNN는 k-NN은 훈련 단계에서 그냥 **데이터를 저장만 하면 되기 때문에 훈련 자체는 빠르지만, 예측하는 과정이 느림**
<br/>

## 예시 

![image](https://github.com/user-attachments/assets/140f0c9d-688a-40a2-bf1e-6d59dbd944b1)

위 이미지에서 k=3 일 때 포함되는 값들의 수를 구해서 가운데 별이 어떤 색인지 예측함 

<br/>

![image](https://github.com/user-attachments/assets/9716fc5b-45ca-4441-9519-e02142b1c81b)

**하지만 이렇게 k의 값이 바뀌면, 예측이 달라질 수 있음**

<br/>

## Type of distance

![IMG_3076](https://github.com/user-attachments/assets/0ec76e59-b611-4fb7-a391-36c6e52190f6)

<br/>

## distance 예시 

아래 예시에서 시그마 j는 특성의 수임, 만약 특성이 height(x)와 weight(y)라면, 시그마는 2번만 반복함 -> 즉 x값 제곱 -> y값 제곱 -> 루트 -> 2차 방정식과 동일 

![image](https://github.com/user-attachments/assets/0caa7605-74e6-4a5a-9b71-1ff7dc4da58a)

<br/>

## KNN overfitting 

**KNN이 과적합이 되는 경우는 K =1  일 때이다**

새로운 데이터가 들어왔을 때, 가장 가까운 단 1개의 이웃만 보고 그 이웃의 정답 그대로 예측하기 때문이다 

-> noise 하나에도 민감하게 반응 

**KNN에서는 선형 회귀에서 같이 β와 같은 hyper-parameter가 없지만, 거리 함수나 k의 값을 hyper-parameter와 같이 사용한다**

이때, k가 너무 작으면 노이즈에 매우 민감하여 과적합 위험

k가 너무 크면 세부적인 경계를 놓칠 수 있음 → 과소적합

**이진 분류에서 동일한 경우를 피하기 위해 이웃을 홀수로 사용 (k는 반지름이 아니라 가장 가까운 요소의 개수)**

<br/>

## 예시 문제1

Q: k-NN이 표현할 수 없는 함수는 무엇인가?

A : k-NN 모델의 표현력은 학습 데이터에 의해 제한된다. **따라서 적절한 학습 데이터만 주어진다면, k-NN 모델은 어떤 함수든지 표현할 수 있다**

<br/>

## 예시 문제2

Q: k = 1, ℓ₂ 거리 기반 이진 kNN 분류기에서 학습 데이터 {𝑥𝑖}만 주어졌을 때, 모든 가능한 함수 공간은?

A: 각각의 x에 대한 y가 할당되지 않은 상태이고 이진 분류에서 y의 값은 0 또는 1 이기 때문에 **n개의 입력 데이터에 대해 2^n개의 공간을 갖는다**

## KNN normalization

![스크린샷 2025-04-06 122809](https://github.com/user-attachments/assets/6b222d0c-bcd6-4fee-99cf-8bf1c6f54c99)

왼쪽 그림은 𝑥2 축의 범위가 훨씬 크기 때문에, 거리 계산에서 𝑥2가 훨씬 더 큰 영향을 주고 있음

**이 때문에 이웃이 실제로 의미 있는 가까운 이웃이 아닐 수 있음**

<br/>

이것을 오른쪽과 같이 정규화를 시켜주면 **모든 feature가 비슷한 범위로 표준화 되었고**

이제 진정한 의미의 '가까운' 이웃을 제대로 찾을 수 있게 됨

**또한, KNN은 거리에 따라 neighbor의 차등이 존재하도록 하여 더 정교하게 만들 수 있음**

<br/>

## Decision Tree

![image](https://github.com/user-attachments/assets/cd8f140c-50ef-415f-ab59-05821e3eed4b)

왼쪽 텍스트 조건에 따라 데이터를 분할하는 구조이고 해당 규칙을 이용하여 오른쪽과 같이 표현

빨간색 코드 이외에 나머지 코드를 실행하면 여러 영역이 나옴 

Decision Tree가 표현할 수 있는 가설 공간은 어떤 것인가

축에 정렬된 axis-aligned hyperrectangles으로 나눈 공간이다 (axis-aligned hyperrectangles : 축으로 정렬한 직사각형)































