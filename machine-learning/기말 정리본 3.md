## 11주차 

### 비지도 학습 응용분야

1. **Visualization (시각화)** : 데이터셋이나 모델의 출력을 시각적으로 탐색 (ex: 100차원 데이터를 2D/3D로 줄여 시각화해 패턴 파악)

2. **Feature Learning (특징 학습)** : 자동으로 낮은 차원의 유용한 특징을 학습함, 특히 라벨이 적고 데이터가 많을 때 유리 (ex: 이미지에서 가장 중요한 형태나 패턴 추출)

3. **Image Compression (이미지 압축)** : JPEG 포맷은 비지도 학습 기법을 도입 중

<br/>

### 대표적 비지도 학습 유형 : Clustering (군집화), 차원축소

### Clustering (군집화)의 예시 : k-means

<br/>

### Clustering (군집화)

**Clustering** : 데이터들을 유사한 특성을 기준으로 그룹으로 나누는 기법

ex: 고객을 성향에 따라 그룹으로 나누는 것

<br/>

### Clustering 목표

이 데이터들을 **동질적인(비슷한 성격의) K개의 그룹으로 나누는 것**

![image](https://github.com/user-attachments/assets/0ce64ce2-57d7-42ef-a622-0bd220e158d0)

<br/>

### Clustering 종류

1. **Flat or Partitional clustering** (평면 또는 분할 클러스터링) : 각 클러스터(그룹)는 **서로 독립적이다** -> **대표적 예시 K-means**

2. **Hierarchical clustering** (계층적 클러스터링) 

<br>

### Flat clustering & Hierarchical Clustering 차이점

- 플랫 클러스터링은 한 번만 클러스터를 나눠서 고정된 결과를 만든다 (ex: K-means는 처음부터 K개로 나누고 끝)

- 계층적 클러스터링은 어디서 트리를 자르느냐에 따라 다양한 클러스터링 결과를 줄 수 있다

- 플랫 클러스터링은 보통 속도가 빠르고 계산이 효율적이다

- 계층적 클러스터링은 합치고 쪼개는 결정을 많이 하기 때문에 느릴 수 있다
  
<br/>

### 좋은 Clustering 조건

1. **High within-cluster similarity** : 같은 군집 안에 있는 데이터끼리는 매우 유사해야 한다

2. **Low inter-cluster similarity** : 다른 군집 간에는 서로 다른 특성을 가져야 함

<br/>

### Clustering 주의할 점

**라벨이 없어서, 무엇이 비슷한 것인지 정의하기 어렵다**

따라서 **적절한 거리 측정 방법 또는 유사도 기준을 사용하는 것이 매우 중요**하다

<br/>

### K 값을 조정하면 클러스터 수를 더 적게 또는 더 많이 볼 수 있음

즉, 같은 데이터를 가지고도 더 거칠게(3개로), 또는 더 정밀하게(6개로) 나눌 수 있음

<br/>

### Flat Clustering: K-means algorithm

![IMG_3386](https://github.com/user-attachments/assets/8499c46a-bd00-4805-8f9b-ce1dd802a0a9)

![IMG_3387](https://github.com/user-attachments/assets/1f99f6bf-ffe0-4006-bbca-be5cda59c8f4)

![IMG_3388](https://github.com/user-attachments/assets/aa3f685d-dd77-434f-90f7-2c1162c070e6)

![IMG_3389](https://github.com/user-attachments/assets/403ff671-8b36-40b0-9677-3d2c3b42d03a)

<br/>

### K-means 알고리즘 특징

K-means 알고리즘에서 사용한 손실 함수는 **비볼록 함수**다. 즉, 손실 함수에는 **여러 개의 최소점(해)** 가 존재할 수 있다

이 손실 함수를 정확하게 가장 잘 줄이는 방법을 찾는 건 **매우 어려운 계산 문제(NP-hard 문제)** 다

우리가 쓰는 K-means는 그런 어려운 문제를 **대신해서 풀어주는 쉬운 방법(=휴리스틱)** 이다

여기서 **heuristic (휴리스틱)** 이란 **완벽하진 않지만, 빠르게 괜찮은 답을 찾게 해주는 방법**을 말한다 

<br/>

### K-means 알고리즘 한계

1. K-means는 각 점을 클러스터에 **강제로 할당한다는 것** -> 소프트맥스와의 차이점, 소프트 맥스는 확률로 표시하지만, K-means는 강제적으로 집단에 넣어버림

2. 클러스터 크기(데이터 개수)가 **대략 비슷할 때 잘 동작**한다 (ex: 어떤 그룹은 1000개, 다른 그룹은 5개 이런 식이면 K-means는 잘 분류하지 못 한다)

3. K-means는 **동그란 모양의 군집일 때만 잘 작동**한다. 복잡한 모양(비볼록, non-convex) 일 때는 성능이 떨어진다

<br/>

### K-means의 한계를 극복한 것이 kernel K-means이다 

Kernel K-means 기본 아이디어 : K-means에서 사용하는 유클리드 거리 계산을 **커널 기반 방식**으로 바꾸는 것

Kernel K-means는 이를 극복하기 위해 데이터를 고차원 임베딩 공간으로 매핑하는 ϕ를 이용해 커널 함수 기반 거리 계산을 수행하여 **복잡한 구조도 선형적으로 (직선처럼) 나눌 수 있게 해준다**

![IMG_3424](https://github.com/user-attachments/assets/ad8ea16f-573a-4f23-9916-6eb920bcbf01)

<br/>

### 계층적 클러스터링(Hierarchical Clustering)

![image](https://github.com/user-attachments/assets/d3a09bfa-de35-48c2-940f-388ac41352f7)
## 클러스터끼리의 유사도 

위에서는 두 개의 데이터 포인트의 거리를 구해왔다. 이번 파트는 클러스터 집합의 유사도를 측정하는 3 가지 방식을 배울 것이다 

두 클러스터 R과 S의 유사도를 구하는 방식을 알아보자 

<br/>

### 클러스터끼리의 유사도 

### 방법 1 : Min-link or Single-link

R과 S 사이의 **가장 가까운** 두 점 간의 거리
  
![image](https://github.com/user-attachments/assets/dfd34387-1107-493e-ae76-4090eeba031d)

<br/>

### 방법 2 : Max-link or Complete-link

R과 S 사이의 **가장 먼** 두 점의 거리

![image](https://github.com/user-attachments/assets/cb3b8075-efd3-4a6b-8a8a-ab60c6b9d0c8)

<br/>

### 방법 3 : Average-link

R과 S 사이의 모든 점 쌍 간 거리의 **평균**

![image](https://github.com/user-attachments/assets/145aa7fa-3623-47f6-8f1f-b49287190f45)

<br/>

### 차원 축소(Dimensionality Reduction)

**정답(label)이 없는 데이터만으로 구조를 찾아내기 때문에** 보통 **비지도 학습(unsupervised learning) 방법**으로 간주

<br/>

### 차원 축소 장점

- 차원 축소는 특징 학습이나 표현 학습에도 매우 유용

- 차원을 줄이면 학습 속도가 빨라질 수 있다

- 데이터를 더 간단하게 압축하는 데에도 쓸 수 있다

<br/>

### 차원이 높아지면 생기는 문제점 : 차원의 저주(Curse of Dimensionality)

- 고차원 공간에서는 공간을 채우려면 **지수적으로 많은 데이터가 필요**

- **차원이 낮을수록 과적합 가능성↓** → **일반화 성능↑**

이 문제를 해결하려면 **차원 축소가 효과적인 방법**

<br/>

### 선형적인 방식으로 차원을 줄이는 방법 (Linear Dimensionality Reduction)

- **투영 방향** : 고차원 데이터를 낮은 차원으로 줄일 때, 어떤 **방향(축)** 으로 정보를 "눌러서(flatten)" 표현할지를 정하는 **벡터**

- **projection matrix U (투영 행렬)** : D차원 데이터를 **K차원(보통 𝐾<𝐷)** 으로 줄이기 위한 투영 행렬

- 각 투영 방향(벡터)은 원래 차원과 같은 크기를 가짐

![IMG_3425](https://github.com/user-attachments/assets/c5b62eaa-0a44-4e50-8c5e-4a7ae1c5d754)

<br/>

### PCA (Principal Component Analysis)

PCA : 고차원 데이터를 선형적으로 투영하여 **최대 분산을 보존하는 저차원 표현을 찾는 차원 축소 기법**

<br/>

### PCA 핵심 : u 방향으로 투영했을 때, 분산이 가장 높은 방향으로 축소하는 것

**분산이 크다는 것**은 해당 축(방향)으로 데이터가 **다양하게 분포하고 있다는 뜻**

**즉, 많은 정보를 유지하면서 축소하기 위함**

<br/>

### PCA는 여러 관점

- **데이터의 분산(정보)을 가장 많이 담는** 방향을 찾는 것

- **원래 데이터와의 차이가 가장 작도록**(오차가 최소가 되도록) 만드는 방향을 찾는 것

- **데이터를 표현하는 기준(좌표축, 즉 "기준 벡터")을 바꾸는 것** -> 새로 만들어진 특징들(feature)은 서로 상관관계가 없어지게 됨. 즉 , 서로 영향을 주지 않는 축들로 바뀜

<br/>

### 투영(Projection)을 통한 분산 계산 

2차원 데이터를 하나의 방향 u₁ (단위 벡터)에 투영해 보자

![image](https://github.com/user-attachments/assets/8dbbde25-7b87-43a1-848a-ceb5a02579e9)

- 투영된 값은 u₁ᵀxₙ 으로 계산된다. 쉽게 u₁ᵀxₙ는 말해 직선으로 눌렀을 때의 값을 의미 

<br/>

### step 1 : 평균 구하기 

- 모든 점을 눌러본 다음 **평균**을 구하면 → u₁ᵀμ 가 나온다 (μ는 평균을 의미)

- 여기서 μ는 기존 2차원 데이터의 평균이다. 즉, 모든 2차원 데이터를 눌러서 얻은 u₁ᵀxₙ 값의 평균은 기존 2차원 평균에서 u 벡터를 곱한 것과 **동일하다**

<br/>

### step 2 : 분산 구하기 

<br/>

이렇게 구한 평균을 이용하여 **분산**을 구한다 (복습 : 분산 = (데이터 - 평균) ^ 2 의 평균)

![image](https://github.com/user-attachments/assets/7f7701a0-8ff2-4c62-a1c4-854cef609dec)

<br/>

### 문제 :분산을 계속 키우기만 하면 u₁의 크기가 무한대가 됨 -> 길이를 1로 고정  -> 라그랑주 승수법 

## PCA cont'd 라그랑주 승수법 공식 및 핵심 개념 

![image](https://github.com/user-attachments/assets/a088a2ff-6882-4493-b62d-a642e336b066)

<br/>

어떤 정사각 행렬 𝐴에 대해, 만약 어떤 벡터 𝑢가 Au = λu를 만족한다면

- 𝑢는 고유벡터 (eigenvector)

- 𝜆는 그에 대응되는 고유값 (eigenvalue)

<br/>

**예시**

![IMG_3426](https://github.com/user-attachments/assets/d709f1a4-4f75-4334-9d96-b8e101822f44)

<br/>

라그랑주 승수법 공식을 미분하면 다음과 같이 결과가 나온다 

<br/>

![image](https://github.com/user-attachments/assets/182375e2-dc79-4643-918f-f685ff26b080)

- u₁은 공분산 행렬 S의 고유벡터

- 우리가 찾던 방향 u₁은 S의 고유벡터이며, 𝜆1은 그 고유벡터에 해당하는 고유값

- S에는 고유벡터가 여러 개(D개) 있고 그중 분산이 최대가 되는 고유벡터, 즉 **고유값이 가장 큰 벡터를 선택**

- **고유값이 클수록 분산을 크게 만듦**

- 이 벡터 𝑢1이 바로 **the first Principal Component (PC)** (**첫 번째 주성분 벡터**) 이다

- 두 번째, 세 번째 주성분도 똑같이 고유값 분해를 통해 구할 수 있다

































