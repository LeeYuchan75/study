## Recall the Joint Distribution (결합 분포)

이 표는 다음 3가지 변수들 간의 모든 가능한 조합에 대한 확률을 나타낸다

- burglary (도둑): 참/거짓

- earthquake (지진): 참/거짓

- alarm (알람): 참/거짓

![IMG_3271](https://github.com/user-attachments/assets/e415f9d8-e143-4790-8b7e-ca4950b88aa6)

## Joint Distribution (결합 분포) 얻는 방법 

**1. 전문가에게서 직접 값을 물어본다**

ex : 전문가가 “도둑이 들고 지진이 났을 때 알람이 울릴 확률은 0.01이야”라고 말해주는 것 

<br/>

**2. 더 단순한 확률 정보를 결합해서 전체 분포를 계산한다**

ex: 작은 확률들(예: P(a), P(b|a))을 조합해서 복잡한 확률(P(a ∧ b))을 만든다

우리가 만약 다음과 같은 정보를 알고 있다고 하자 

- P(a) = 0.7,

- P(b|a) = 0.2,

- P(b|¬a) = 0.1

<br/>

위 정보를 통해 우리는 P(a ∧ b) 를 도출할 수 있다 

<br/>

## 데이터로부터 결합 확률 분포(Joint Distribution)를 학습

결합 확률 분포를 데이터를 이용해 채우는 방법

![IMG_3273](https://github.com/user-attachments/assets/fea6deb0-8708-4246-a0f9-b70f310a27c6)

<br/>

**예시**

![IMG_3274](https://github.com/user-attachments/assets/bc46735b-ef27-4873-a479-fd494ef71207)

<br/>

## Density Estimation (밀도 추정)

우리가 위에서 배운 joint distribution는 **Density Estimation (밀도 추정)** 이라는 것의 한 예시이다

즉, Joint Distribution Table은 사실 Density Estimation이라는 더 큰 개념의 일종이다 

<br/>

**Density Estimation (밀도 추정)** : 여러 입력 속성(attribute)들로부터 확률을 예측하는 함수를 학습하는 것

여러 조건(A=1, B=0, C=1 같은)들을 보고, 그게 **실제로 나타날 확률이 몇인지 계산하는 모델**이다

아래는 Density Estimator가 어떤 모델과 어떻게 다른지를 비교하는 표이다

![IMG_3275](https://github.com/user-attachments/assets/cd210f4e-bc70-48da-a7fa-a8370e065d6f)

쉽게 요약하면 

- Classifier: 정답이 뭐야? (범주형)

- Regressor: 얼마야? (숫자형)

- **Density Estimator: 이 조합이 얼마나 가능해? (확률)**

<br/>

Classifier과 Regressor은 Test set accuracy를 통해 구할 수 있다. 그럼 Density Estimator는 어떻게 구할까 ? 

<br/>

## Density Estimator 구하는 방법

우선 아래 공식을 보자 

![image](https://github.com/user-attachments/assets/9b2e4192-d95f-421a-8cfa-ba3ef389db15)

위 공식은 어떤 데이터의 행 (레코드) x가 주어졌을 때, Density Estimator(밀도 추정기) M은 **이 레코드가 얼마나 가능성 있는지(확률이 높은지) 알려준다**

![image](https://github.com/user-attachments/assets/d546fcaf-9383-4b43-b24a-23d9709dd8d0)

처음 구했던 공식을 확장한 위 공식이 바로 Density Estimator 구하는 방법이다

전체 데이터 셋 (x1,x2, ... , xn) 이 모델 M으로부터 나올 확률은 각각의 레코드 xi 가 모델 M로부터 나올 **확률의 곱과 동일하다** (독립적이라고 가정)

<br/>

## Density Estimator 예시 

![IMG_3276](https://github.com/user-attachments/assets/966e4e14-f63a-4d74-9680-192f22b8c197)

제공한 자료의 모든 확률을 **모두 곱해버리면 값이 매우 작아진다**

![image](https://github.com/user-attachments/assets/caf1eaa2-88bb-426d-ae72-fb27bb330306)

이 값이 너무 작아지지 않게 하기 위해 **로그 확률**을 이용하여 계산한다 

**로그 확률 공식**

![image](https://github.com/user-attachments/assets/511a4ab9-d802-4d8c-a9db-9cbba601e480)

**로그 확률 공식 적용 후 결과**

![image](https://github.com/user-attachments/assets/ab2e8020-b51d-4e14-b546-e9ded5ecbbb2)

<br/>

## Density Estimator 장단점 

**장점**

1. 사람이 직접 확률을 설정할 필요 없이, **데이터를 통해 밀도 추정기를 학습**할 수 있다

2.  각 레코드를 확률값 기준으로 정렬하면, **이상치(이상한 데이터) 를 찾아낼 수 있다**

3.  **추론**을 할 수 있다 (ex: 특정 값이 주어졌을 때, 다른 변수의 조건부 확률을 계산할 수 있음)

4.  베이즈 분류기의 핵심 재료이다 (조건부 확률을 계산하기 위해 Density Estimator (밀도 추정기)를 활용)

<br/>

**단점**

전체 결합 확률을 무작정 통째로 학습하는 방식은 **단순하고, 무의미하며, 위험하다**

<br/>

**자료 x 추가로 공부한 것, 암기 및 이해는 위에 문장만 하기**

단순히 모든 조합의 확률을 그대로 외워버리는 방식은 다음과 같은 문제가 발생한다 

- 데이터가 조금만 부족해도 희귀한 조합의 확률이 0이 될 수 있음

- 차원이 많아질수록 가능한 조합 수가 폭발적으로 증가 → 차원의 저주

너무 단순하게 결합 확률만 외우는 건 학습이 아닌 기억이며, 실제 문제에선 일반화도 안 되고 위험하다

<br/>

## 이전 확률 파트 (9주차)랑 연관 지어서 공부

<br/>

## joint Density Estimator 단점 

**개념 정리**

**Density Estimator** : 확률 분포 P(x) 또는 P(x∣θ)를 추정하는 일반적인 방법 (ex: 사람의 키 분포를 측정하는 도구)

**Joint Density Estimator** : 여러 변수의 결합 확률 P(x1, x2, ... , xn)을 추정 (ex: 사람의 키와 몸무게와 나이의 동시 분포를 한꺼번에 측정하는 도구)

<br/>

**단점 1** : 테스트셋에 '처음 보는 조합'이 나오면 모델은 아예 불가능하다고 판단해버림 → 로그 가능도 -∞

**단점 2** : 차원이 높아질수록 공간이 기하급수적으로 커져서 고차원에서 밀도 추정, 거리 계산, 분류 등은 데이터 희소성 때문에 성능이 떨어짐 (차원의 저주)

그럼 각 단점에 대한 설명을 함께 살펴보자 

![IMG_3288](https://github.com/user-attachments/assets/596bbf3a-8a6c-491a-b7b3-0be646fa80d9)

결합 밀도 추정(joint density estimation)도 물론 위와 같이 과적합이 발생할 수도 있지만, **문제는 다른 모델보다 가장 심하게 과적합이 된다**

결합 밀도 추정(joint density estimation)은 모든 특성 조합의 확률을 학습하는데, **훈련 데이터에만 너무 맞춰서 일반화가 전혀 안 된다**

아래 상황을 함께 보자 

![image](https://github.com/user-attachments/assets/7966131d-ef3c-4d3a-b3ed-43d94afe7195)

위 상황처럼 'asia + 78to83' 조합은 훈련 데이터에 한 번도 안 나왔기 때문에,  모델은 실제로 가능할 수도 있는 상황이어도 이 조합의 확률을 0으로 학습하게 된다

**즉, 하나의 데이터라도 never 값이라면, 확률을 구하지 못한다**

<br/>

![image](https://github.com/user-attachments/assets/80775687-0b0d-4935-aa27-6a4e88f223cb)

제곧된 이미지와 같이 차원이 증가하면 **공간이 기하급수적으로 커져서**, 데이터를 채우기 위해 **필요한 샘플 수가 엄청나게 많아진다**

이러한 이유로 우리는 **과적합에 덜 취약한 밀도 추정기(density estimator)** 가 필요하다. 

여기서 나온 것이 바로 **Naïve Bayes Classifier**이다

<br/>

## Naïve Bayes Classifier

우리가 배웠던 베이즈 정리를 다시 떠올려보자

![스크린샷 2025-05-31 113004](https://github.com/user-attachments/assets/e263702c-7dc9-467e-82cd-9a82f3b3a6dd)

우리가 알고 싶은 것은 어떤 입력(X)이 주어졌을 때, 정답(Y)이 무엇일지 예측하는 것인데, 베이즈 정리는 그것을 확률적으로 구할 수 있게 해주는 공식이다

<br/>

이러한 베이즈 정리를 머신러닝에 사용한다고 생각해보자 

![IMG_3375](https://github.com/user-attachments/assets/11d76b09-78c4-4edc-8276-3acd4a6b0e8a)

해당 식은 어려울 것 없이 xi라는 feature가 많을 때를 가정한 것이다 

쉽게 말해 

- Y: 우리가 맞히고 싶은 정답(예: 고양이 or 강아지)

- X = x_i: 관측된 feature (예: 귀가 뾰족 or 뾰족한 얼굴형 등등)

- 이 수식은 어떤 데이터를 보고, 그것이 특정 클래스(y_k)일 확률을 계산하는 것.

<br/>

### 공식 색깔 별 의미 

- 파란칸 : 클래스(정답)의 사전 확률 (ex: 이메일이 100개 있는데 그중 30개가 스팸이면 P(spam) = 30/100 = 0.3) -> **계산이 쉬움**

- 초록색칸: 특성(feature)들이 전부 동시에 특정값이 될 확률 (ex: Y 값이 "play = yes"일 때, X 값은 (특징 값은) "sky = sunny ∧ temp = warm ∧ humid = high ∧ ...") -> **이런 경우의 수가 너무 많다는 것이 문제** 예를 들어 특성이 6개인데 각각 3가지 값이 있을 수 있으면 조합이 3⁶ = 729개가 나옴 -> **비현실적**

- 빨간칸 : Y값에 상관 없이 모든 특징의 확률을 계산 -> **어차피 모든 클래스에서 공통으로 들어가기 때문에 분자끼리만 비교하면 됨**

<br/>

이러한 문제로 베이지 정리를 머신러닝에 적용하려면 **데이터가 많아야 하고 계산이 매우 어렵고 비현실적**이다 

이것을 해결하기 위해 나온 것이 바로 **Naïve Bayes Classifier**이다

<br/>

## Naïve Bayes Classifier

**Naive**는 단순한, 멍청한 이라는 의미이다 

이전 문제와 같이 베이즈 정리를 머신러닝에 적용하면  X1, X2, X3가 동시에 특정 값을 가질 확률을 "x1이면서 x2이면서 x3인 경우를 구해야 한다 

문제는 특성이 많아질수록 (d 차원) **데이터 부족으로 확률 추정이 부정확해진다**

따라서 우리는 **모든 특성을 독립이라고 가정한다. 이것이 Naïve Bayes Classifier의 핵심이다**

<br/>

### 예시 

- **독립이 아닐 때** : P(sky=sunny ∧ temp=warm ∧ humid=high | play=yes) -> 이렇게 전체적인 확률을 하나씩 다 구해야함

- **독립이라고 가정할 때** : P(sky=sunny | play=yes) × P(temp=warm | play=yes) × P(humid=high | play=yes) -> 이렇게 하면 개별적인 확률만 알면 되므로 계산이 편리함

<br/>

### 주의할 점 

Naïve Bayes Classifier는 **모든 특성이 실제로 독립이 아닐 수 있다**. 말 그대로 가정만 한 것이다 

하지만 **충분히 괜찮은 성능**을 낼 수 있기 때문에 Naive Bayes는 실무에서 많이 사용한다 

<br/>

### Naïve Bayes Classifier 예시 

우리가 알고 싶은 것 : 이 날씨일 때 Play는 yes일까, no일까?

![image](https://github.com/user-attachments/assets/e420d40a-bde6-44bb-b9d5-ebc44e317590)

### step 1 : 사전확률 구하기

- play=yes: 3개

- play=no: 1개

<br/>

### step 2 : play가 yes일 때와 no일 때 조건부확률 구하기

### Play = yes 일 때 

- P(Sky = sunny | yes) = 3/3 = 1.0

- (Temp = warm | yes) = 3/3 = 1.0

- P(Humid = high | yes) = 2/3

- P(Wind = strong | yes) = 3/3 = 1.0

- P(Water = warm | yes) = 2/3

- P(Forecast = same | yes) = 2/3

<br/>

### Play = no 일 때 

- P(Sky = sunny | no) = 0/1 = 0.0

- P(Temp = warm | no) = 0/1 = 0.0

- P(Humid = high | no) = 1/1 = 1.0

- P(Wind = strong | no) = 1/1 = 1.0

- P(Water = warm | no) = 1/1 = 1.0

- P(Forecast = same | no) = 0/1 = 0.0

<br/>

### step 3 : 각각 확률을 모두 곱해서 비교 

ex : P(play=yes)⋅P(sunny∣yes)⋅P(warm∣yes)⋅P(high∣yes)⋅P(strong∣yes)⋅P(warm∣yes)⋅P(same∣yes) vs ...

예측 값이 큰 값을 최종적으로 선택 

<br/>

### Naïve Bayes Classifier 문제점

데이터가 적으면 어떤 값은 한 번도 등장하지 않아 **확률이 0이 될 수 있다**

어떤 경우든 확률이 0이 되어 전체 곱이 0이 되는 것은 모델이 **지나치게 학습 데이터에만 맞춘 것**일 수 있다 -> **overfitting** 유발 

이것을 해결하기 위해 **Laplace smoothing**이라는 방법을 사용한다

<br/>

## Laplace smoothing

Naive Bayes에서 어떤 값이 한 번도 등장하지 않으면 확률이 0 되고, 전체 확률 구할 때 조차 예측 불가능해진다 

따라서 Laplace smoothing을 이용한 것인데 핵심 아이디어가 **등장하지 않은 값도 아주 작은 확률을 주자**는 것이다 

<br/>

### Laplace smoothing 공식 

![IMG_3376](https://github.com/user-attachments/assets/ce4b8797-e18f-4ac7-becd-621205144719)

<br/>

### 공식 이해 

- **분모 시그마** : 클래스 Y = yk 에서 **특성 Xj**의 모든 값들이 등장한 횟수의 총합

- 예를 들어 feature이 Sky이고, Sky가 가질 수 있는 값들은 sunny, rainy, overcast 라고 하자. 이때, Play = yes인 데이터에서: sunny = 3번, rainy = 0번, overcast = 1번 나왔다면, 시그마 공식에 의해 3 + 0 + 1 = 4의 값을 가진다

- **∣=values(Xj)∣** : 특성 Xj가 가질 수 있는 고유한 값의 개수

- 예를 들어 Sky는 가능한 값이 sunny, rainy, overcast로 3가지라면, **∣values(Sky)∣= 3**이다

<br/>

### Laplace smoothing 예시 

Naïve Bayes Classifier에서 설명한 예시와 동일하다

![image](https://github.com/user-attachments/assets/e420d40a-bde6-44bb-b9d5-ebc44e317590)

### step 1 : 사전확률 구하기 (사전 확률은 스무딩 기법 x)

- P(Play=yes) = 3/4

- P(Play=no) = 1/4

<br/>

### step 2 : 첫 번째 조건부 확률 계산

P(Sky=sunny | play=yes) 를 계산해보자

Naïve Bayes Classifier 기법을 이용하면 P(Sky=sunny | play=yes = 3/3 = 1.0이 나왔었다 

하지만 Laplace smoothing를 사용하면 

- play = yes인 행은 3개. 그 중 sunny는 3번 (rainy는 play = yes에 존재하지 x)

- Laplace Smoothing 적용: (3 + 1) / (3 + 2) = 4/5  (분자는 sunny가 3번 나온 것에 +1, 분모는 sunny = 3번, rainy = 0 번 이여서 나온 3과 총 특징의 개수 2개를 (sunny, rainy는 총 2개) 더함)

**따라서 이 경우 P(Sky=sunny | play=yes) = 4/5 가 나온다**

<br/>

## Using the Naïve Bayes Classifier

-우리가 공부한 것을 정리해보면 어떤 입력 벡터 xi 가 주어졌을 때, 그것이 클래스 yk에 속할 확률은 다음과 같이 계산된다 

![IMG_3378](https://github.com/user-attachments/assets/e7fbe7ae-5ac6-487e-877c-08f81bc62c06)

위 공식에서 underflow(**소수점이 너무 작아지는 문제**)를 막기 위해 **로그 확률을 사용한다**

<br/>

![IMG_3380](https://github.com/user-attachments/assets/4b7fd370-76c9-4b0a-a8d6-9381a41827aa)

<br/>

### Naïve Bayes 분류기는 원래는 확률이 아닌 클래스만 예측한다

우리가 조심해야할 점이 Naïve Bayes 분류기는 **분모 P(X) 를 무시**하고 **분자만 계산하기 때문에** 원래는 확률이 아닌 **클래스만 예측**한다

**실제 확률을 구하기 위한 아이디어**는 해당 분자 값의 **분자 값들의 총합으로 나누는 것**이다

**𝛼 = 1 / 분자의 총합** 으로 하고 이 값을 분자에 곱해주면 **실제 확룰이 나온다**

![IMG_3383](https://github.com/user-attachments/assets/5b935ab6-9192-4ed1-b8c3-05ec299a4f7a)

<br/>

## Naïve Bayes Applications

Naïve Bayes는 다음과 같은 상황에서 사용한다

- 이메일이 스팸인지 아닌지 분류

- 회의 공지 이메일인지 분류

- 문서를 작성한 저자가 누구인지 분류

- 뇌 상태를 분류 (ex: 𝑃(BrainActivity∣WordCategory) -> 단어 종류가 주어졌을 때 뇌 활동이 어떻게 되는지를 학습)

즉, 단어나 표현의 등장 빈도를 보고 문서의 **범주(클래스)** 를 분류함. → **Naïve Bayes가 적합**

<br/>

## The Naïve Bayes Graphical Model

![image](https://github.com/user-attachments/assets/f39a4f30-9914-4f26-991f-b85c6ee95c28)

- 노드는 확률 변수 (Y, X₁, ..., X_d)

- Y → X_j로 향하는 화살표는 **의존성**을 뜻함
  
- 각 노드는 **조건부 확률 테이블(CPT)** 을 갖고, 그 값은 부모(Y)에 의해 결정됨

<br/>

여기서 **의존성**이란 각 특성 Xj의 값은 클래스 Y에 **의존해서 달라진다는 뜻**이다   

그림에서 Y -> X로 향하는 것을 클래스 Y가 X의 원인이자 부모 노드라고 한다 

- 클래스 Y가 ‘스팸’이면 → 특정 단어(Xj)가 나올 확률이 높아질 수 있고,

- Y가 ‘정상 메일’이면 → 다른 단어들이 더 자주 나올 수 있다 

<br/>

### The Naïve Bayes Graphical Model 예시 

![IMG_3384](https://github.com/user-attachments/assets/871ca314-9438-47ac-ab55-9ab06fe7c4b2)

### 만약 

x = (rainy,warm,normal) 일 때 경기 여부를 구하려면 아래와 같이 푼다 

![image](https://github.com/user-attachments/assets/a16dbb2a-a4e6-4ba5-b9ce-67b3f43edf84)

<br/>

### 중복 제거

추가적으로 중복 문제를 없애기 위해 동일한 정보는 제거한다 

![IMG_3385](https://github.com/user-attachments/assets/77416a0a-12bf-48a2-96fb-86c09c7e94e0)

<br/>

## Naïve Bayes 장단점 

### 장점 

- 학습이 빠르다 (데이터를 한 번만 훑으면 됨)

- 예측(분류) 속도도 빠르다

- 관련 없는 특성에 영향을 덜 받는다

- 연속형(real)과 이산형(discrete) 데이터 모두 처리 가능

- 스트리밍 데이터(계속 들어오는 데이터)도 처리할 수 있다

<br/>

### 단점 

- 특성들 간에 (조건부) 독립이라는 가정을 한다

<br/>

Naïve Bayes는 빠르고 단순하지만, 특성 간 독립 가정이라는 약점이 있는 분류기이다 
































