## 수식은 줄 것. 이해하는게 중요

### 임의 변수(Random Variables, RV)

- **정의** : 무작위 실험의 결과를 숫자로 매핑(mapping)하는 규칙

    - 임의변수 X는 확률공간 Ω(모든 가능한 사건의 집합)에서 실수 집합 ℝ로 가는 함수이다  
 
    - ex: 동전 던지기를 할 때 앞면은 1 뒷면을 0으로 지정해주는 것
 
    - 이렇게 해야 확률연산이 가능

<br/>

- 동전 던지기 예시
  
    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장임의변수그림.png)
 
    - 확률공간(Ω): {HH, HT, TH, TT} (동전 2번 던졌을 때 가능한 모든 경우)
 
    - X(앞면) = 1 이라고 정의도 가능
 
        - 여기서 앞면은 **사건(Event)** 이고
     
        - 임의 변수는 **사건을 숫자로 바꿔주는 함수**이다

<br/>

- **Range(X)** : 임의 변수가 가질 수 있는 값들의 집합

    - ex : 동전 2번 던지기 → Range(X)={0,1,2}
    
    - ex : 시험 합격 여부 → Range(X)={0,1}

<br/>

### 확률 공간 

- **표본공간 (Sample space)**

    - 무작위 실험의 **모든 가능한 결과 목록**

    - **Ω** 로 표기
 
        - ex: 실험의 모든 답안지 모음
 
        - ex:  결과 집합: Ω = {HH, HT, TH, TT}

        - ex:  HH: 두 번 다 앞면, HT: 첫 번째 앞면/두 번째 뒷면 …
<br/>

- **사건 공간 (Event space)**

    - 우리가 확률을 매길 수 있게 합의한 사건들의 모음
 
    - **𝓕** 로 표기
 
    - ex: 합격한 답안지 모음, 점수 60점 이하 답안지 모음, 동전 2번 던져서 앞면이 1번 나온 경우 등 
 
    - **표본공간의 부분집합들(멱집합)**
 
    - ex: 동전 2번 던지기 (Ω = {HH, HT, TH, TT})
    
        -   {HH}, {HT, TH}, {HH, HT}, …

<br/>

- **확률 측도 P (Probability measure)**

    - 사건에 **0~1 확률** 숫자를 붙이는 규칙 (공리 성립 필요)
 
    - 동전 2번 던지기
        
        - 표본 공간 Ω = {HH, TT, HT, TH}
       
        - 임의 변수 X = 앞면의 개수
       
        - X(HH)=2
       
        - X(HT)=X(TH) =1

        - X(TT) = 0

        - 𝑋:	Ω → {0,1,2} ⊂ ℝ

<br/>

- **정리**

    - 무작위 실험의 가능한 모든 결과 (Ω)

        - 표본공간, 즉 모든 경우의 수

    - 그 결과들로 정의된 사건들의 모음 (𝓕)

         - 우리가 다루고 싶은 사건 집합들

    - 각 사건에 확률을 배정하는 규칙 (P)

        - 확률측도, 사건별 확률 값

    - 임의변수 : 확률 공간 (Ω, 𝓕, P)에서 정의된 실수 값 함수
 
        - **X : Ω → ℝ**

<br/>

### 이산 임의 변수(Discrete Random Variables)

- **정의** :  임의변수 X가 취할 수 있는 값들의 집합(치역, Range(X))이 **셀 수 있는** 경우(countable set)

    - ex: 동전 던져서 앞면 개수 (0,1,2,…,n)

    - ex: 시험 합격/불합격 (0 또는 1)

    - ex: 버스에 탄 사람 수 (자연수)
 
    - 응용사례
 
        -  객체 분류: 사전에 정의된 개수의 이산형 클래스 개수로 입력의 클래스를 분류
    
        -  언어 모델링: 어휘 집합은 유한한 이산 집합으로 정의됨
      
        -  강화 학습: 행동 공간이 유한하게 정의되는 경우

<br/>

### 확률 질량 함수 (Probability Mass Function, PMF)

- 정의 : **이산** 임의변수 X가 특정 값 x를 가질 **확률**

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장PMF공식.png)
 
        - 위 공식에서 시그마의 의미는
     
            - X(w) = k : 임의변수 값이 k가 되는 경우만 골라서 하겠다는 의미이다

    - **p(x) = P(X = x)** 로 표기
 
        - p(x): 특정 값에만 대응되는 확률을 주는 함수 (mass function)
     
        - P(·): 전체 사건에 확률을 매기는 큰 함수 (measure)
     
        - 모든 확률의 값은 1

<br/>

- **성질**

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장PMF성질.png)
 
    - 마지막 공식의 의미는 사건 A가 여러 값으로 이루어져 있으면, 그 값들의 확률을 다 더하면 된다는 것이다
 
        - 동전 2번 던지기 예시에서
     
            - PMF:

                - P_X(0) = 1/4 (앞면 0개, TT)

                - P_X(1) = 1/2 (앞면 1개, HT·TH)

                - P_X(2) = 1/4 (앞면 2개, HH)
             
                - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장동전던지기그래프.png) 
             
            - Range(X) = {0,1,2}

            - 앞면이 1개 이상 나오는 사건 : A = {1,2}
         
            - P(X ∈ A) = P(X=1) + P(X=2)

            - = 1/2 + 1/4

            - = 3/4

<br/>

- 주사위 예시

![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장PMF주사위예시.png)

<br/>

### 확률 분포의 기댓값과 분산

- **기대값 (Expectation, Mean, Average)**

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장기대값공식.png)
 
    - 분포의 **중심**을 표현
 
    - 동일한 확률 실험을 **무한히 반복했을 때 평균적으로** 관찰되는 값

<br/>
 
- **분산 (Variance)**

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장분산공식.png)
 
    - 분포의 **흩어짐** 정도
 
    - 임의 변수가 기댓값 근처에 **얼마나 밀집** 또는 퍼져 있는지를 측정

<br/>

- 예시

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장기대값분산예시.png) 

<br/>

### 기대값의 선형성 

- **기대값의 선형성** : 두 확률변수를 더한 뒤 기대값을 구하는 것은, 각각의 기대값을 구해서 더하는 것과 같다

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장기대값의선형성공식.png) 

    - 위 공식은 **독립과 무관하게** 사용 가능
 
    - 머신러닝에서는 미니배치(batch)의 손실(loss)을 계산할 때각 데이터 손실의 평균을 바로 합의 기대값으로 다룬

<br/>

### 기댓값의 선형성 활용 문제 

- 문제 상황

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장개구리예시문제.png)
 
    - 개구리가 0에서 시작

   - 한 번 움직일 때:

   - 왼쪽(-1)로 갈 확률 = pL

   - 오른쪽(+1)로 갈 확률 = pR

   - 제자리에 있을 확률 = pS

   - 확률 합: pL + pR + pS = 1

   - 개구리가 2번 독립적으로 움직인 뒤의 위치 = X

   - 구할 것: E[X] (평균 위치)
     
<br/>

- **일반적인 풀이**

    - PMF 계산
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장개구리예시문제PMF풀이.png) 

    - 최종 기대값 계산
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장개구리예시문제일반풀이마지막.png)

    - 즉, 전체적인 풀이과정은
    
    - (1) 가능한 경우 k에 대한 모든 경우의 확률 구하기
 
    - (2) k 값 x P(k)의 최종 합 구하기 

<br/>

- **기대값의 선형성을 이용한 풀이**

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장개구리예시문제기대값의선형성풀이.png)
 
        - 각 사건에 대한 기대값이 Pr - PL인 이유도 마찬가지로 k의 값과 각 확률을 곱한 것이다
     
            - 자연스럽게 k = 0 일때는 곱해서 사라짐
         
        - 마지막 공식인 E[X] = E[X1] + E[X2]를 보면 X1이 발생할 기댓값은 (−1)⋅pL ​+ (0)⋅pS​ + (1)⋅pR​ 이고 X2가 발생할 기댓값과 (−1)⋅pL ​+ (0)⋅pS​ + (1)⋅pR​ 이다. 따라서 2(Pr - PL)이다

    - 위 식을 일반화하여 정리하면
 
        - (1) : 각 사건을 개별 사건으로 정의 ( X = X1 + X2 + ... + Xk)
     
        - (2) : 각 사건의 k 값 x P(k) 구하기
     
        - (3) : 각 기대값을 **더하기**

<br/>

### 무의식적 통계학자의 법칙 (LOTUS)
 
- 만약 우리가 확률 변수 X가 아니라 g(X)라는 변형된 값의 기대값을 구하고 싶을 때 g(x)에 대한 기댓값을 구하는 것이다 

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장LOTUS예시문제.png)
 
    - 즉, 기댓값을 계산할 때 k x P(k)에서 g(k) x P(k)가 되는 것
 
        - **P(k)는 변함 없음** 

<br/>

### 지시 변수 (Indicator Random Variable)

- 어떤 임의변수 X의 기대값을 구하고 싶은데 직접 PMF를 구하기는 복잡한 경우, X를 여러 개의 지시변수의 합으로 표현하면 계산을 단순화 할 수 있음

- 경우의 수가 2개일 때만 사용 가능

    -  ex: 주사위가 6이 나왔는가 ? 아닌가 ?
 
    - ex: 시험에 합격했는가 ? 아닌가 ?  

- 지시변수 IA는 "사건 A가 발생했는가?"를 나타냄

    - A가 일어나면 = 1

    - 안 일어나면 = 0
 
    - 즉, 지시변수의 기댓값은 아래와 같다
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장지시변수기댓값.png)
     
<br/>

- 지시변수 활용 예시

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장지시변수활용예시.png)

<br/>

### 독립 변수 (Independent Random Variables)

- 두 임의변수 X, Y가 **독립**이라는 것은 변수 X의 값이 다른 변수 Y의 값에 **아무 영향을 주지 않는다**는 것이다

    - 독립은 X⊥Y로 표기  

    - ex: X = 주사위 굴리기 결과, Y = 동전던지기 결과

<br/>

- **두 변수가 독립일 때 성질**

    - 성질 1 : X와 Y가 동시에 어떤 값을 가질 확률이, **각각의 확률 곱**으로 나눠질 수 있으면 독립이다
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장독립성질1.png)
     
    - 성질 2 : 특정 **집합** A, B 안에 속하는 사건들도 **똑같이 곱으로 분리**된다
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장독립성질2.png)
     
    - 위 성질을 일반화하면 아래와 같다
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장독립성질일반화.png)
     
    - 성질 3 : **조건부확**률 공식에서 식을 간략화할 수 있다
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장독립성질조건부확률.png)
     
    - 성질 4 : X와 Y가 **독립**일 때 두 값의 분산은 **각각의 분산을 더한 것과 같다**
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장독립성질분산.png)
  
<br/>

### 확률 분포 (Probability distribution)

- **정의** : 임의 변수가 가질 수 있는 값들에 대해 각각의 값이 **얼마나 자주 일어날 지**를 설명하는 규칙

    - 확률분포는 확률변수가 **어떻게 움직이는지**(자주 나오는 값, 드문 값 등)를 **한눈에 보여준다**
 
    - 이산 변수의 경우 PMF를 통해 표현 가능
 
    - PMF를 형성하는 파라미터로 분포 표현
 
<br/>

- 데이터를 어떻게 모델링할 지 고르는 분포 선택의 흐름도는 

    - (1) : 데이터가 이산(Discrete)인지, 연속(Continuous)인지 구분

    - (2) : 데이터가 대칭적(Symmetric)**인지, 비대칭(Asymmetric)인지 확인

    - (3) : 중심값 주변에 모여 있는지, 이상치(outlier)가 있는지, 값이 제한되어 있는지 등을 본다

<br/>

- 주요 이산 확룰 분포는 아래와 같다

    - **베르누이 분포 (Bernoulli Distribution)**

        - 한 번의 시행 → 성공/실패 두 가지 결과만 있음

        - 모수: 성공확률 p

    - **이항 분포 (Binomial Distribution)**

        - 동일한 베르누이 시행을 n번 반복했을 때 성공 횟수

        - 모수: 시행 횟수 n, 성공확률 p

    - **기하 분포 (Geometric Distribution)**

        - 첫 번째 성공이 나올 때까지 걸린 시행 횟수

        - 모수: 성공확률 p

    - **음이항 분포 (Negative Binomial Distribution)**

        - r번째 성공이 나올 때까지 걸린 시행 횟수

        - 모수: 성공횟수 r, 성공확률 p

    - **초기하 분포 (Hypergeometric Distribution)**

        - 모집단에서 복원 없이 추출했을 때 성공 개수

        - 모수: 모집단 크기 N, 성공 항목 수 K, 추출 개수 n

    - **포아송 분포 (Poisson Distribution)**

        - 단위 시간/공간 안에 사건 발생 횟수

        - 모수: 평균 발생률 lambda

    - **이산 균등 분포 (Discrete Uniform Distribution)**

        - 유한한 이산 집합에서 모든 값이 동일한 확률로 발생

        - 모수: 최소값 a, 최대값 b

<br/>

### 베르누이 분포 (Bernoulli Distribution)

- **정의** : 한 번의 시행(trial)에서 **성공/실패 두 가지 결과**만 가능한 확률 분포

    - ex: 동전 던지기(앞면, 뒷면), 불량 여부(불량, 정상)

    - 파라미터: p 
    
        - 성공 확률이 p
        
        - 실패 확률이 1-p
     
    - 가장 기본적인 이산 확률 분포
 
    - 많은 복잡한 분포의 구성 요소 역할을 수행

<br/>

- **베르누이 프로세스**

    - 동일한 베르누이 시행(Bernoulli trial)을 **독립**적으로 여러 번 반복하는 것이 베르누이 프로세스다
 
    - ex: 동전 던지기 예시에서 앞 면이 나올 확률이 0.6, 뒷면이 0.4 이고, 우리가 3번을 던져서 앞면, 뒷면, 앞면이 나올 확률을 구하고 싶다면
 
        - 0.6 x 0.4 x 0.6 으로 구하면 된다 
<br/>

- **베르누이 PMF**

    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장베르누이PMF.png)
 
    - 같은 의미로 다음과 같이 사용한다
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장베르누이쉬운공식.png) 

<br/>

- **베르누이 기댓값과 분산**

    - 베르누이 **기댓값** 

        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장베르누이기댓값.png) 

    - 베르누이 **분산**    
 
        - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장베르누이분산.png)
     
    - 기댓값 분산 증명 과정
 
    - ![System Resources](../../images/Probability%20Theory%20for%20AI%20images/03-1장베르누이기댓값분산증명.png)















































