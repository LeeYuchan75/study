## 차원 축소(Dimensionality Reduction)

- 차원 축소는 정답(label)이 없는 데이터만으로 구조를 찾아내기 때문에 보통 **비지도 학습(unsupervised learning) 방법**으로 간주

- 데이터 속에 숨겨진 저차원 구조를 발견하는 데 사용

<br/>

### 차원 축소 장점 

- 차원 축소는 특징 학습이나 표현 학습에도 매우 유용

- 차원을 줄이면 학습 속도가 빨라질 수 있다

- 데이터를 더 간단하게 압축하는 데에도 쓸 수 있다

<br/>

![image](https://github.com/user-attachments/assets/7ded4bc5-3ccd-47f3-83aa-208d3e3ad0a3)

- 왼쪽 그림 : 파란 점들을 한 직선안에 옮겨서 차원 축소

- 오른쪽 그림 : 3D 공간에서 나선형으로 꼬여 있는 데이터를 1차원 구조로 펼침(축소) -> 두 점 사이가 원래는 가까워 보였지만, 실제 구조상 멀리 떨어져 있었던 것을 알 수 있음

<br/>

### 차원이 높아지면 생기는 문제점 : 차원의 저주(Curse of Dimensionality) 

- 고차원 공간에서는 공간을 채우려면 지수적으로 많은 데이터가 필요

- 차원이 낮을수록 과적합 가능성↓ → 일반화 성능↑

- 이 문제를 해결하려면 차원 축소가 효과적인 방법

<br/>

### 선형적인 방식으로 차원을 줄이는 방법 (Linear Dimensionality Reduction)

- **투영 방향** : 고차원 데이터를 낮은 차원으로 줄일 때, 어떤 **방향(축)** 으로 정보를 "눌러서(flatten)" 표현할지를 정하는 **벡터**

- **projection matrix U (투영 행렬)** : D차원 데이터를 **K차원(보통 𝐾<𝐷)** 으로 줄이기 위한 투영 행렬

- 각 투영 방향(벡터)은 원래 차원과 같은 크기를 가짐

![IMG_3425](https://github.com/user-attachments/assets/c5b62eaa-0a44-4e50-8c5e-4a7ae1c5d754)

<br/>

**PCA 공식 그림으로 표현**

![image](https://github.com/user-attachments/assets/5699fe5e-d63f-48a5-8b67-b23f5e859313)

<br/>

위 풀이과정에서 언급했던 것처럼 어떤 기준으로 가장 좋은 𝑈를 정하는지 (ex:분산을 최대화? 재구성 오차 최소화?)를 정하는 것이 바로 **PCA**이다 

<br/>

## PCA (Principal Component Analysis)

**PCA** : 오래된 고전적인 선형 차원 축소 기법

**PCA 핵심** : u 방향으로 투영했을 때, 분산이 가장 높은 방향으로 축소하는 것 

PCA는 여러 관점으로 해석할 수 있다 (**시험문제 예상**)

- 데이터의 분산(정보)을 가장 많이 담는 방향을 찾는 것

- 원래 데이터와의 차이가 가장 작도록(오차가 최소가 되도록) 만드는 방향을 찾는 것

- 데이터를 표현하는 기준(좌표축, 즉 "기준 벡터")을 바꾸는 것 -> 새로 만들어진 특징들(feature)은 서로 **상관관계가 없어지게 됨**. 즉 , **서로 영향을 주지 않는 축들로 바뀜**

<br/>

## 투영(Projection)을 통한 분산 계산 

2차원 데이터를 하나의 방향 u₁ (단위 벡터)에 투영해 보자

![image](https://github.com/user-attachments/assets/8dbbde25-7b87-43a1-848a-ceb5a02579e9)

- 투영된 값은 u₁ᵀxₙ 으로 계산된다. 쉽게 u₁ᵀxₙ는 말해 직선으로 눌렀을 때의 값을 의미 

<br/>

### step 1 : 평균 구하기 

- 모든 점을 눌러본 다음 **평균**을 구하면 → u₁ᵀμ 가 나온다 (μ는 평균을 의미)

- 여기서 μ는 기존 2차원 데이터의 평균이다. 즉, 모든 2차원 데이터를 눌러서 얻은 u₁ᵀxₙ 값의 평균은 기존 2차원 평균에서 u 벡터를 곱한 것과 **동일하다**

<br/>

## step 2 : 분산 구하기 

<br/>

이렇게 구한 평균을 이용하여 **분산**을 구한다 (복습 : 분산 = (데이터 - 평균) ^ 2 의 평균)

![image](https://github.com/user-attachments/assets/7f7701a0-8ff2-4c62-a1c4-854cef609dec)

<br/>

여기서 **S**는 공분산이라는 의미이고 위 식에서 간단하게 표현만 한 것. 또한 위 식에서 시그마 내부에 u1ᵀ가 존재하는데 이것이 제곱이 되어 S 앞 뒤로 u1이 존재하는 것 

![image](https://github.com/user-attachments/assets/ec40c24a-fc52-43bb-9086-8f1d4297d0d9)

<br/>

## PCA cont'd : 최대 분산 방향 찾기

우리는 데이터를 u₁ 방향으로 눌렀을 때 분산을 가장 크게 하고 싶다 

<br/>

![image](https://github.com/user-attachments/assets/2b164cb6-78ae-4ad4-8a74-f45070a0f4ca)

- 하지만 분산을 계속 키우기만 하면 **u₁의 크기가 무한대**가 될 수 있다는 **문제가 생긴다**

- 그래서 **u₁의 길이(크기)를 1로 고정**해줘야 한다 -> **단위 벡터로 제한**

<br/>

![image](https://github.com/user-attachments/assets/7bafe61f-e1cd-4d83-8a23-df0c01dfc4f2)

<br/>

그래서 우리는 **라그랑주 승수법**을 이용하여 u의 범위를 정한다 

<br/>

## PCA cont'd 라그랑주 승수법 공식 및 핵심 개념 

![image](https://github.com/user-attachments/assets/a088a2ff-6882-4493-b62d-a642e336b066)

<br/>

위 공식을 공부하기 전 고유값과 고유벡터 개념을 복습하자 

어떤 정사각 행렬 𝐴에 대해, 만약 어떤 벡터 𝑢가 Au = λu를 만족한다면

- 𝑢는 고유벡터 (eigenvector)

- 𝜆는 그에 대응되는 고유값 (eigenvalue)

<br/>

**예시**

![IMG_3426](https://github.com/user-attachments/assets/d709f1a4-4f75-4334-9d96-b8e101822f44)

<br/>

라그랑주 승수법 공식을 미분하면 다음과 같이 결과가 나온다 

<br/>

![image](https://github.com/user-attachments/assets/182375e2-dc79-4643-918f-f685ff26b080)

- u₁은 공분산 행렬 S의 고유벡터

- 우리가 찾던 방향 u₁은 S의 고유벡터이며, 𝜆1은 그 고유벡터에 해당하는 고유값

- S에는 고유벡터가 여러 개(D개) 있고 그중 분산이 최대가 되는 고유벡터, 즉 **고유값이 가장 큰 벡터를 선택**

- **고유값이 클수록 분산을 크게 만듦**

- 이 벡터 𝑢1이 바로 **the first Principal Component (PC)** (**첫 번째 주성분 벡터**) 이다

- 두 번째, 세 번째 주성분도 똑같이 고유값 분해를 통해 구할 수 있다

<br/>

또한 길이 1을 유지하는 이유는 깊게 설명은 하지 않음 -> 아래 예시 참고 

![스크린샷 2025-06-05 165131](https://github.com/user-attachments/assets/b144c8e0-29c0-4665-a712-3e559493c081)

![스크린샷 2025-06-05 165136](https://github.com/user-attachments/assets/4018bf6f-4f0e-4a36-ad29-4462bdd3ca64)

<br/>

## p.47 유사 내용 공부 

<br/>

## p.49부터 공부 

















































