### 통계적 생성 모델 (Statistical Generative Models)

- 통계적 생성 모델은 확률 분포 Pθ(x) -> θ라는 파라미터를 이용해서 확률 분포 x를 추정할 수 있다는 의미

- **생성 모델** : 어떤 확률 규칙을 통해 데이터를 만든다는 것

![image](https://github.com/user-attachments/assets/dff58570-afa0-4948-8003-81674ec30a55)

<br/>

## 모델을 만들기 위해 우리가 알고 있거나 설정하는 것

- 어떤 분포를 사용할지 (ex : 가우시안 분포)

- 어떤 기준으로 모델을 학습시킬지 (ex: maximum likelihood)

- 어떤 알고리즘으로 최적화할지 등을 포함

<br/>

또한 Statistical Generative Models은  density estimator(확률 밀도 추정기)일 수도 있다

쉽게 말해 생성 모델은 데이터를 만들어내는 확률 분포인데 사실 생성 모델은 단순히 데이터를 그럴듯하게 흉내내는 확률 모형일 수도 있다는 것이다 

Statistical generative model은 단순히 데이터를 생성하는 것뿐 아니라, 데이터가 어디에 얼마나 분포해 있는지를 알아내는 도구가 될 수도 있다

![image](https://github.com/user-attachments/assets/bce2ea1a-196a-4986-98b9-598d1166d5a8)

<br/>

### Discriminative (판별 모델) & Generative Models (생성 모델) 차이점 

Discriminative (판별 모델)과 Generative Models (생성 모델) 차이점을 알아 보자 

<br/>

### Discriminative model (판별 모델)

- Discriminative model : 정답(y)이 무엇인지 바로 맞히는 게 목표 (ex: Regressions, SVMs)

- 수식 : P(y∣x) -> ex: 이 사진이 고양이일 확률은? → 바로 예측 가능

- 이 둘 사이를 어떻게 나눌까? → 선이나 경계만 배움

<br/>

### Generative Models (생성 모델)

-  Generative Models : 각 정답(y)이 어떤 입력(x)을 만들어내는지를 먼저 배우고, 그걸 바탕으로 정답(y)을 추론

-  수식 : P(x∣y)을 먼저 구하고 → Bayes' rule로 P(y∣x) 계산

- ex: "고양이들은 이런 특징을 가지고 있다!" → 그럼 이게 고양이일 확률은?

- 각 클래스가 어떤 모양으로 퍼져 있는지 배우자

![image](https://github.com/user-attachments/assets/cb3712d4-fb48-4a7d-865d-52c0f19a96e4)

<br/>

## Generative Model (생성 모델)

- 데이터를 만들어냈을 법한 확률적인 방법을 정의. 쉽게 말해서 이 모델은 "이 데이터가 이런 식으로 만들어졌을 수도 있겠다"는 확률적인 이야기를 만들어내는 모델

해당 파트를 공부하기 전 주요 개념을 먼저 알고 가자 

<br/>


### 핵심 개념 정리 

- **latent variable (잠재 변수) z로 표현** : 우리가 관측한 데이터 x를 만들기 전에 랜덤하게 선택된 숨은 정보

- **random variable (랜덤 변수) x로 표현** : z는 그냥 정해지는 게 아니라 확률적으로 랜덤하게 뽑아서 정해짐. 그래서 x는 z에 따라 바뀜

- **prior distribution p(z∣ϕ)** : 잠재 변수 z의 사전 분포(prior distribution). Φ는 z가 어떻게 분포될지를 결정하는 역할 (ex: "웃는 얼굴이 70%, 찡그린 얼굴이 30%")

ex: 생성 모델을 요리로 비유하면

- z = 요리 레시피 (ex. 매운맛, 재료 종류, 조리법)

- x = 완성된 음식 (ex. 짬뽕 사진)

<br/>

즉, **잠재변수 z**는 눈에 보이지는 않지만 **데이터의 본질을 담고 있는 정보**이다 

<br/>

### 본론으로 넘어와서 다시 설명 

![image](https://github.com/user-attachments/assets/59dc3ce4-7f9b-41fc-8b6e-7358ad617def)

- 각각의 관측 데이터 xn은 숨겨진 변수 zn과 연결되어 있다고 가정한다

  - zn​은 어떤 **사전 분포 p(z∣ϕ)** 를 따르는 무작위 변수라고 가정 (z는 처음부터 랜덤하게 뽑는 값, ϕ는 z를 뽑는 분포의 설정값 ex: '표정은 미소일 수도, 찡그림일 수도 있다'처럼 우연히 선택되는 요소)

- z가 주어졌을 때 x를 생성하는 **데이터 분포를 p(x∣z,θ)** 라고 가정 

- 즉, p(z∣ϕ)로 z를 설정하고 뽑힌 z를 이용해서 최종적인 x를 도출하는 것

<br/>

p(x∣z,θ)의 예를 들어보자

ex : 손글씨 숫자 생성 모델

- z: 숫자의 의미 (예: "4")

- x: 실제로 그린 숫자 이미지

<br/>

이때 p(x∣z,θ) : “숫자 4라는 뜻(z)을 주면, 어떻게 그릴지 결정하는 생성 규칙”

여기서 θ: “숫자의 굵기, 선의 흐림 정도, 이미지의 해상도 등등을 조정하는 내부 설정값”이다 

<br/>

## 데이터가 만들어지는 과정(generative story)

생성 모델은 데이터를 설명할 때 **generative story(생성 이야기)** 라는 개념으로 설명할 수 있다

1. 먼저, 무작위 잠재 변수 zn을 사전 분포 p(z∣ϕ)에서 뽑는다 (보이지 않는 설정값(ex: 나이, 표정, 각도 같은 숨은 정보)을 확률적으로 뽑는 것부터 시작)

2. zn이 주어졌을 때, 𝑥𝑛을 p(x∣z,θ)라는 분포를 이용해 생성

<br/>

이런 모델에는 로컬 변수, 글로벌 변수라는 두 종류의 변수가 있다

- 각각의 zn은 로컬(local) 변수 : 이는 각 데이터 𝑥𝑛​에 개별적으로 관련되어 있음

- (𝜙,ϕ) 는 글로벌(global) 변수 : 모든 데이터에 공통적으로 적용되는 설정값

- 보통은 로컬 변수로부터 글로벌 변수를 추론하거나, 그 반대도 가능

<br/>

### 왜 생성 모델을 쓰는가

생성 모델은 데이터를 만드는 과정을 **올바르게, 확률적으로 생각할 수 있는 방법**이다 

- 적절히 p(x∣z,θ)를 설정하면, 실수형(real), 이진(binary), 개수형(count) 등 다양한 데이터를 모델링할 수 있다

- 또한 이미 학습된 모델을 이용해 새로운 데이터를 만들거나, 상상(hallucinate)할 수 있다

<br/>

### 예시 

![image](https://github.com/user-attachments/assets/6eb04375-c817-4f22-8b1a-9d485b4d4f13)

이미 학습된 모델에서 흐릿한 얼굴 이미지를 z를 확률적으로 무작위로 뽑고, 그에 따라 x를 만들어내어 진짜 사람 얼굴처럼 정교하게 생성됨

<br/>

## 대표적인 생성 모델들

### 1. Mixture model

Mixture modeld은 여러 개의 간단한 분포를 섞어서 복잡한 데이터의 구조를 설명할 수 있게 하는 모델

클러스터링이나 확률 밀도 추정에 사용

![image](https://github.com/user-attachments/assets/82ecb1bf-05cf-4cd6-a5ca-8b1b2ec6f884)

- 데이터 x1, x2, ... xn이 K개의 분포를 섞어서 만들어졌다고 가정 (ex: 학생 키 데이터가 있으면 남자 키 분포, 여자 키 분포를 섞어서 전체 데이터가 만들어짐)

- 이 K개의 분포는 각각 θ1, ... θk 라는 파라미터를 가진 p(x∣θk)라고 하자 (ex: 남성 키는 평균 175cm, 표준편차 7cm인 정규분포, 여성 키는 평균 162cm, 표준편차 6cm인 정규분포, 여기서 평균과 표준편차가 파라미터)

- 각각의 𝑥n 이 어떤 분포에서 나왔는지는 모른다 (ex: x3이 남자에서 나왔는지 여자인지 우린 모름)

<br/>

아래의 생성 이야기를 가정해보자

1. 먼저, 그룹 하나를 랜덤하게 고른다 (zn ∈ {1,2,...,k}) 중 랜덤하게 하나 선택) ex: 남성 그룹(z=1)을 뽑음

2. 그 그룹에 맞는 분포로부터 𝑥𝑛을 생성 ex: 남성 키 평균 분포를 사용해서 키 데이터를 하나 생성

<br/>

혼합 모델에서는 z가 **이산형(discrete)** 이라서, p(z∣ϕ)는 **다항 분포(multinomial distribution)** 이다

다항 분포 예시 : 1번 그룹일 확률 0.2, 2번 그룹일 확률 0.3...”처럼 여러 개 중 하나를 고르는 확률을 따지는 분포

실제 데이터 분포 p(x∣θzn) 는 데이터의 종류에 따라 달라진다 (숫자 데이터면 정규분포, 이미지면 복잡한 신경망일 수도 있다)

따라서 Mixture models은 **복잡한 분포도 단순한 분포 여러 개를 겹쳐서 모델링할 수 있다** -> 밀도 추정, 클러스터링에 활용

<br/>

## p.11 ~ 12 대기 (시험범위 아닌 부분이 있어서 고려)

## 수업 내용 : GMM은 Kmeans 보다 느림, p.24 가우시안 분포를 줬을 때 EM 알고리즘으로 어떻게 표현할 수 있을지 

<br/>

## Gaussian Mixture Model (GMM)

**Gaussian Mixture Model (GMM)** : 데이터를 **여러 개의 가우시안 분포(Gaussian distributions)의 혼합(mixture)** 으로 모델링하는 **확률적 생성 모델(generative model)**

- 데이터를 그룹으로 나누기 위해 데이터를 만들어내는 방식(모델)을 사용한다

- 데이터를 K개의 정규분포(Gaussian)가 섞인 모델에서 만들어졌다고 가정

- πₖ는 k번째 정규분포가 데이터를 얼마나 많이 만들어냈는지를 나타내는 비율 (정규분포 : 평균을 중심으로 좌우 대칭인 종 모양 곡선(연속적))

- πₖ는 xₙ이 k번째 분포에서 나왔을 확률(사전확률)이라고 해석할 수 있다

- K개의 가우시안 분포의 비율들을 (π₁, π₂, ..., π_K) 하나로 묶은 벡터를 π라고 하자

- zₙ = k이라는 말은, K차원 중 **k번째만 1이고 나머지는 0인 벡터**를 의미 -> **size가 k인 one-hot vector**

- zₙ은 다항분포에서 나온 것이고, 수식으로는 πₖ를 사용해서 확률을 계산 (다항분포 : 여러 개의 선택지 중 하나가 선택되는 확률을 나타내는 분포 (이산적))

<br/>

### GMM: The Generative Story (GMM 생성 과정)

### 1. 먼저 π에 따라 zₙ을 고른다. 즉, 어떤 가우시안 분포에서 데이터를 만들지 정함

![image](https://github.com/user-attachments/assets/becc5b47-afa8-499f-9f39-20a8a875f015)

위 공식의 의미는 다음과 같다 

- zn : 군집(cluster) 번호 (예: 1번, 2번, 3번) **여기서 군집은 가우시안 분포의 집합을 의미**

- 𝜋 : 각 군집이 선택될 확률 (예: [0.5, 0.3, 0.2]) -> 1번 군집에서 나올 확률 50%, 2번 군집에서 나올 확률 30% ... **랜덤하게 결젇됨. 확률이 높은걸 고르는게 아님** (이 선택은 z에 대한 사전확률에서 이뤄진다, 즉 zₙ을 선택할 때 미리 정해진 확률 분포 π를 기준으로 무작위로 선택)

- 다항 분포로부터 군집 선택

<br/>

### 2. 만약 zₙ = k로 나왔다면, 이제 k번째 가우시안에서 xₙ을 만든다

![image](https://github.com/user-attachments/assets/5e0401c9-621e-4fd8-9cd1-16b0724bfbc0)

- μk : k번째 군집의 평균

- Σ𝑘 : k번째 군집의 분산 (퍼짐 정도)

- 𝑥𝑛 : 실제로 관측된 데이터 포인트

<br/>

만약 1번 과정에서 2번 군집이 선택되었다면, 2번 군집의  평균과 분포 모양에 맞춰서  실제 좌표 또는 점 하나를 랜덤하게 뽑아낸다

<br/>

## GMM(Gaussian Mixture Model)의 파라미터를 학습과정 

GMM의 파라미터  Θ = {𝜋k, μk, Σ𝑘} (총 K개의 군집 개수만큼) 부터 를 최대우도법(MLE)으로 추정해보자 

각 군집(클러스터)에 대해 평균, 분산, 그리고 군집별 비중 π도 함께 추정해야 한다 

GMM 학습 과정에서 "관측된 것"과 "숨겨진 것"을 구분해 설명하기 위해서 **conditional probability density function (p.d.f.)** 와  **marginal p.d.f**에 대해 알아보자 

<br/>

### conditional probability density function (p.d.f.)

![image](https://github.com/user-attachments/assets/857814a9-d4c8-4340-ba46-e24caf5d5ff0)

위 공식은 만약 x가 k번 군집(정규분포)에서 왔다고 알고 있다면, 그때 𝑥가 어떤 값일 확률은 **정규분포 N(μk,Σk)** 을 따른다는 뜻이다 

<br/>

### marginal p.d.f

![image](https://github.com/user-attachments/assets/f3cea58f-c2ce-4d63-b820-935ddc013d2a)

- 실제로는 x가 어느 군집에서 왔는지 모름 → z는 숨겨진 변수 (latent variable)

- 그래서 𝑥의 전체 확률(주변 확률)은, 모든 가능한 𝑧 = 𝑘 에 대해 확률을 다 더해야 한다 (이전 장에서 배운 조건부확률 계산과 동일)

- 여기서 말하는 𝑝(𝑥)는 단순한 확률이 아니라, GMM의 파라미터 Θ = {𝜋k, μk, Σ𝑘}가 주어졌을 때의 확률을 의미

- GMM을 학습하려면, 데이터가 잘 나올 확률이 최대가 되도록 p(x)에 대해 **최대우도 추정(MLE)** 을 해야 함

- 그런데 p(x)는 복잡한 식이기 때문에, 직접 계산해서 **최적화하기가 어렵다**, 이것은 아래 내용 정리 후 다음 파트에서 설명한다

<br/>

정리하면 

- **Conditional p.d.f.** : z가 주어졌을 때 x의 분포. 즉, “이 군집에서 나왔으면 x는 이렇게 분포할 것이다" 를 의미

- **Marginal p.d.f.** : z를 모를 때 x의 전체 분포. 우리가 실제로 계산해야 할 확률

<br/>

조건부 확률은 모델의 구조를 설명하는 데 필요함 → "군집 k를 선택했을 때, x는 이런 식으로 생성된다."

주변 확률은 실제로 학습(MLE)을 수행할 때 우리는 z를 모르기 때문에 필요함

따라서 p(x∣Θ)를 직접 계산해야 하고 이건 모든 z 가능성에 대해 합쳐야 하는 주변 확률를 의미

<br/>

## GMM을 학습할 때 왜 어려운가

아래 공식의 L 표현은 **로그 우도 함수 (log-likelihood)** 를 의미

![image](https://github.com/user-attachments/assets/e53e2b95-361a-4649-b9ce-289115c8b5ae)

- 내부에 존재하는 로그 : xn이 여러 군집 중 하나에서 왔을 수 있으니 그 모든 가능성을 합한 것

- 외부에 존재하는 로그 : 여러 개 데이터의 확률을 곱하면 너무 작아지니까, 계산을 안정적하기 위함
  
<br/>

- 우리는 𝑁 개의 데이터를 보고 GMM을 학습한다

- 데이터를 가장 잘 설명하는 파라미터를 찾기 위해 **log-likelihood** 를 최대화를 해야하는데 **문제는** 로그 안에 **합(sum)** 이 들어가 있어서 **계산이 복잡**해진다 

- 파라미터들(π, μ, Σ)**이 합과 로그 안에 같이 섞여 있어서 **미분해서 MLE 해를 구하기 어려워진다**

- 이것을 해결하기 위해 경사하강법 같은 반복적인 수치 계산도 가능하지만, 우리는 **EM 알고리즘 (Expectation Maximization)** 을 사용할 것이다

- EM 알고리즘 (Expectation Maximization)을 사용하는 이유 : **숨겨진 변수(z)** 가 있을 때 **파라미터 추정에 딱 맞는 일반적 해법**이기 때문

- 또한 만약 𝑧𝑛 이 이산(one-hot)이 아니라, 연속 변수이거나 가능한 경우가 매우 많다면 적분이 들어간 형태라 MLE는 훨씬 더 어려워짐. **따라서 EM이 더 필요해진다** -> 이후에 배움

<br/>

## EM의 핵심 아이디어 간단하게 설명

EM의 핵심 아이디어 : 숨겨진 변수(z)가 있을 때, 그것을 **추측(예측)하면서** 파라미터를 반복적으로 학습해나가는 것

추측을 이용하여 GMM을 학습하는 방법을 보자 

- 만약 각 데이터 𝑥𝑛이 어느 군집(z)을 선택했는지 안다면, **MLE(최대우도추정)는 훨씬 쉬워진다**

- 우리가 GMM 학습이 어려운 주된 이유 **로그 안에 sum이 있어서 미분이 어려웠기 때문**

- 하지만 zn을 알면 기존의 내부 로그는 각각의 모든 가능성을 계산했지만 어디서 왔는지 알면 그 부분만 계산하면 되기 때문에 훨씬 쉬워짐 

![image](https://github.com/user-attachments/assets/6d7b9eae-93da-4aa8-bf77-e9bf5daa5071)

<br/>

따라서 z를 안다면, 우리는 **전체 데이터의 우도"=**를 더 쉽게 계산할 수 있다

p(xn, zn) : complete data likelihood → z까지 포함된 **완전한 데이터의 확률**

p(xn) : incomplete data likelihood → z를 모르는 상태에서의 **불완전한 확률**

<br/>

- 문제는 우리는 실제 z값을 모르기 때문에 **추측**을 해야함

- 이 추측은 현재의 파라미터(π, μ, Σ)에 따라 이루어진다. 추측한 z를 가지고 다시 MLE 수행 → 파라미터를 업데이트 -> 이 과정을 반복

- 이런 반복 절차(iterative procedure)가 바로 **EM 알고리즘**이다

<br/>

## p.20 zn을 알고 있을 때 얼마나 

<br/>

## 	숨겨진 변수 z의 확률(기댓값) 계산 

E-step 계산 : 클러스터에 속할 확률(= posterior expectation of znk)을 어떻게 구하는지를 설명하는 EM 알고리즘의 핵심 부분

**Posterior expectation(확률적 기대값)** : 현재 파라미터 Θ = {𝜋k, μk, Σ𝑘}를 기준으로 각 xn이 군집 k에 속할 확률적 기대값을 계산할 수 있다

<br/>

**공식**

- znk : **xn**이 **군집 k**에 속하는지를 나타내는 이진 변수

- znk = 1 : xn이 군집 k에 속함

- znk = 0 : xn이 군집 k에 속하지 않음
  
![image](https://github.com/user-attachments/assets/e5148da9-2452-4f7a-b552-e04dcefa6f63)

<br/>

마지막을 보면 E[znk]가 πkN(xn∣μk,Σk)와 **비례한다**고 되어 있다. 즉, 정규화가 아직 안 된 상태이기 때문에 이것을 확률로 표현하면 아래와 같다 

![image](https://github.com/user-attachments/assets/58f03089-3e83-4d5b-9f4a-fdc79cff77aa)

이 값은 군집 k에 **속할 확률**이고, 모든 k에 대해 합치면 1이 된다

<br/>

이 값을 가지고 **complete log-likelihood**를 **근사 계산이 가능**하다

- log-likelihood logp(x) : 관측된 데이터 x만 가지고 계산하는 우도

- complete log-likelihood logp(x,z) : x와 숨겨진 변수 z까지 포함해서 계산하는 우도

<br/>

**complete log-likelihood 계산**

![image](https://github.com/user-attachments/assets/18d1cf3a-7ebf-45fd-917c-b2eb1934edeb)

<br/>

## 이 기댓값을 바탕으로 파라미터(θ)를 최적화

위에서 구한 complete log-likelihood값을 **최대한 크게 만드는 파라미터 𝜋𝑘, 𝜇𝑘, Σ𝑘를 찾는 게 핵심**이다 

![image](https://github.com/user-attachments/assets/18d1cf3a-7ebf-45fd-917c-b2eb1934edeb)

<br/>

파라미터에 대해 미분해서 극댓값을 구하면 MLE 공식이 나옴 

- 상수는 무시 가능 (note: constants w.r.t. 𝜇𝑘 or Σ𝑘 can be ignored)

![image](https://github.com/user-attachments/assets/6b80d7e2-cd36-4f8a-8b7c-19f699661203)

이 수식들은 결국 가중치를 반영한 정규분포의 평균/분산 추정 공식과 같다

- 𝑥𝑛 전체를 이용하되

- 각 𝑥𝑛이 군집 𝑘일 확률 𝛾𝑛𝑘만큼만 기여하게 만듦

- 혼합비율 𝜋𝑘도 계산 가능 (단, 합은 1이 되게)

<br/>

### 파라미터 업데이트 

### 1. 평균 μk 업데이트

![image](https://github.com/user-attachments/assets/a8d2b1d0-616f-407f-8202-6e083210aca4)

군집 𝑘의 평균은, **각 데이터 𝑥𝑛**의 기여도 𝛾𝑛𝑘만큼 가중합한 뒤 평균낸 것

γnk : 데이터 𝑥𝑛 이 군집 𝑘에 속할 확률

<br/>

### 2. 공분산 Σk 업데이트 

![image](https://github.com/user-attachments/assets/08a4592e-0ef7-4d9e-a87b-92a4f9af2916)

군집 𝑘의 공분산은, 편차 벡터에 대해 가중된 분산 계산

<br/>

### 3. 혼합비율 𝜋𝑘 업데이트 

![image](https://github.com/user-attachments/assets/b08c96d6-4829-4674-90ee-dfdaa390318d)

전체 N개 중 군집 𝑘에 속한 비중  

<br/>

- 군집 𝑘에 속한 데이터 개수를 세는 건데, 각 데이터가 완전히 속한 것(1)이 아니라 부분적으로 속함(확률로)을 반영해서 더한 것
​
- 한 데이터 𝑥𝑛은 여러 군집에 동시에 기여하지만, **기여량은 확률 𝛾𝑛𝑘에 비례**

- ex: γn1 = 0.8, 𝛾𝑛2 = 0.2 라면, 𝑥𝑛은 군집 1에 80%, 군집 2에 20% 속해 있는 것으로 간주

<br/>

## p.24 보고 최종 정리 

<br/>

## EM 알고리즘 예시

### step 1 : 초기화 

- 군집의 평균, 분산, 비율 랜덤 설정

- 모든 점에 대해 각 군집 확률이 1/3로 균일하게 배분 (빨간/파란/초록 파이 차트로 표시)

![image](https://github.com/user-attachments/assets/e369e37c-d2cd-40d5-ad98-258338abcd96)

<br/>

### step 2 : 각 점의 𝛾𝑛𝑘 계산 후 업데이트 반복 

**1차 시도**

![image](https://github.com/user-attachments/assets/0cce94b5-36fa-4ab1-a00a-d96bc09f3390)

<br/>

**2차 시도**

![image](https://github.com/user-attachments/assets/d5fceb00-b73f-4e77-9a26-6e6aaf03942c)

<br/>

**3차 시도**

![image](https://github.com/user-attachments/assets/8eaf1d1a-f052-499b-a238-739165d15313)

<br/>

**20번째 까지 시도 -> 수렴 완료 판단**

![image](https://github.com/user-attachments/assets/3ac23f2e-809c-4d22-9d6c-6ca912ce2137)

<br/>

각 반복마다

- 군집의 평균(μ), 분산(Σ), 비율(π)이 점점 정제됨

- 타원(정규분포)들이 더 정확하게 데이터 클러스터를 감싸게 됨

- 각 점의 소속 확률(γ)이 점점 뚜렷해짐 (파이차트가 한 색으로 치우침)

<br/>

### 실제 데이터(Bio Assay)에 적용한 결과

### 초기 데이터 

![image](https://github.com/user-attachments/assets/b3dfc09c-0302-4158-849b-62bdc98e1e0d)

<br/>

### EM 알고리즘 적용 후 

![image](https://github.com/user-attachments/assets/06dc1931-ab55-4b32-86f3-fe79a2911a6c)

![image](https://github.com/user-attachments/assets/dacafc60-323a-4265-a681-fe4c5090c3c8)

















































