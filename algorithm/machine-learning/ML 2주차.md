## Machine Learning (ML) 2주차 

![Image](https://github.com/user-attachments/assets/6d82c297-821e-4471-8e4a-74837bfb76cd)

hypothesis class : ML 절차에서 데이터를 맞추기 위해 하나를 선택해야 하는 함수 클래스

loss function : 모델이 데이터를 얼마나 잘 맞추는지를 측정하는 함수

optimizer : 가장 적합한 함수를 찾기 위한 탐색 방법 (방향을 정함)

회귀 : 숫자를 예측하는 것
<br/>

## ML과 DL 차이

Machine learning (ML): 사람이 직접 **특징(feature)과 가설(hypothesis)** 을 정하고, 모델이 최적의 파라미터를 학습 

Deep learning (DL): 신경망(Neural Network)이 데이터에서 자동으로 특징과 가설을 학습하여 최적의 구조를 찾아감. 즉, 컴퓨터가 **어떤 특징이 중요한지 알아서 찾음**

<br/>

## Linear function

![Image](https://github.com/user-attachments/assets/115e7430-577c-429b-a500-13eb0d75d06e)

1. x ∈ ℝ^d 는 입력 벡터이다. 이는 d 차원의 실수 벡터로, 흔히 특징(features) 또는 공변량(covariates) 라고 불림 

2. β ∈ ℝ^d 는 매개변수 벡터이다. 이는 선형 함수의 가중치(Weight) 역할을 하며, 모델이 학습하는 값이다  

3. y = f_β(x) 는 출력(label, response) 이다. x 를 입력했을 때 선형 함수를 통해 나오는 결과값이다

ℝ^d는 d차원 유클리드 공간(d-dimensional Euclidean space)을 의미, aka :  also known as의 줄임말, '~라고도 불리는' 또는 '일명'이라는 뜻

<br/>

## MSE 개념

![Image](https://github.com/user-attachments/assets/acfe5a6a-8a91-4941-ae88-4d765be98aca)

<br/>

## 고차원과 등고선 해석 

![Image](https://github.com/user-attachments/assets/5eff9c4b-5991-4973-bae1-655d4b65cfb2)

**1.MSE 손실 함수의 성질**

Convex (볼록) 형태: 손실 함수 L(β;Z)는 볼록(convex)한 **그릇 모양(bowl-shaped)** 을 가짐. 이는 MSE 손실 함수가 이차 함수(quadratic function) 형태이기 때문이다

볼록 함수는 단 하나의 전역 최솟값(global minimum) 을 가지므로, 경사 하강법(Gradient Descent) 등의 최적화 방법을 사용하면 항상 최적의 β 값에 수렴 가능.

<br/>

## 등고선 해석 

손실 함수의 등고선(contour plot) 을 나타낸 것이다

각 빨간색 타원형 곡선은 동일한 손실 값(등고선)을 가지는 지점을 의미함

가운데로 갈수록 손실 값이 작아지며, 중앙의 점이 최적의 β 값임

경사 하강법(Gradient Descent) 을 사용하면, 바깥쪽에서 시작하여 손실이 최소화되는 중앙으로 수렴하게 된다

<br/>

## What is a “Good” Mean Squared Error?

제로 MSE는 거의 달성할 수 없음. 선형 회귀 알고리즘이 잘 작동하는지 알고 있는 방법은

**1. Compare to simple baselines**

내 모델이 직접 짠 간단한 코드보다 더 나은 성능을 내는가를 확인하기 위해 baseline을 사용하여 판단함

예시 1) Constant Prediction(상수 예측)

방법: 모델을 학습하지 않고, 그냥 훈련 데이터의 평균(mean)을 예측값으로 사용

예를 들어, 목표 값 y의 평균이 50이라면, 모든 입력 x에 대해 항상 50을 예측하는 모델을 만들 수 있음

목적: 만약 복잡한 선형 회귀 모델이 단순 평균 예측보다 성능이 나쁘다면, 모델이 제대로 작동하지 않는다는 것을 의미

즉, 기본적인 기준선 모델보다 더 나은 성능을 내야 의미 있는 모델임

<br/>

예시 Handcrafted model (핸드크래프트 모델)

방법: 사람이 직접 간단한 규칙을 정해서 예측하는 모델을 만듦

예를 들어, "날씨가 흐리면 70, 맑으면 80" 같은 간단한 규칙 기반 모델

목적: 모델이 이런 단순한 규칙 기반 예측보다 나은 성능을 내야 함 

→ 즉, 기준선 모델보다 나은 성능을 내지 못하면, 더 복잡한 모델을 사용할 의미가 없음

<br/>

## 다양한 Loss function

![image](https://github.com/user-attachments/assets/9a7a196e-48bb-40cd-a1da-bcc9e77866e2)

**MAE 와 MSE 차이**

MSE : 2차 함수이기 때문에 1 이상일 때, 오차가 급격하게 증가함 -> 오차에 대해 더 많이 간섭

MAE : 1차함수로 표현되기 때문에 MSE보다 오차에 대해 간섭을 크게 안받음 

<br/>

## MRE에서 y로 나누는 이유

MRE(Mean Relative Error, 평균 상대 오차)는 오차를 비율(%)로 표현하기 위해 실제 값 y로 나눈다

데이터의 크기가 다를 때, 절대 오차(MAE)만으로는 비교가 어려울 수 있음

예를 들어,

A 모델의 예측 오차가 5kg

B 모델의 예측 오차가 5g

<br/>

이 경우, 절대 오차(5kg vs. 5g)로 보면 A 모델이 더 큰 오차를 가진 것처럼 보이지만,

만약 A 모델의 대상이 1000kg짜리 물체이고, B 모델의 대상이 50g짜리 물체라면

상대적으로 B 모델이 훨씬 더 큰 오차를 가진 것임

**MRE는 오차를 실제 값과 비교하여 비율로 변환하므로, 서로 다른 크기의 데이터를 공정하게 비교할 수 있다**

또한 다음과 같은 이유로 **MAE의 단점은 상대적인 오차 비교가 안된다는 것이다**

<br/>

## MRE 단점

MRE는 y값으로 나누어 비율을 구하기 때문에 실제 값이 0에 가까우면 MRE가 커지는 문제가 발생함

MRE(Mean Relative Error)의 단점 중 하나는 실제 값(y_i)이 0 또는 매우 작을 경우 값이 급격히 커지는 문제가 존재한다

예를 들어, 실제 값이 0.01이고 예측값이 0.1일 경우,

MRE = |0.1 - 0.01| / 0.01 = 0.09 / 0.01 = 9 (900%)

이렇게 MRE 값이 지나치게 커질 수 있기 때문에, 데이터에 따라 MAE(Mean Absolute Error)와 함께 사용하는 것이 좋다

<br/>

## 상관계수 

![Image](https://github.com/user-attachments/assets/aaa25f9a-2e31-4a8d-be69-dfa32af45f6e)

Pearson correlation (피어슨 상관계수)

정의: 두 변수 간의 선형 관계를 측정하는 지표. 이 값은 두 변수 간의 직선적 관계가 얼마나 강한지를 나타내며, 값의 범위는 -1에서 1 사이이다

1: 두 변수는 완벽하게 양의 선형 관계를 가짐

-1: 두 변수는 완벽하게 음의 선형 관계를 가짐

0: 두 변수 간에는 선형 관계가 없거나 매우 약한 관계만 존재

<br/>

## Pearson correlation 예시 

상황: 학생들의 공부 시간과 시험 점수 간의 상관 관계를 분석한다고 가정

![Image](https://github.com/user-attachments/assets/2349f672-7413-4383-97b0-4271a13e4b3a)

Pearson 상관 계수를 계산하려면, 먼저 두 변수 간의 선형 관계를 측정해야 함

두 변수 간의 Pearson 상관 계수가 1에 가까울수록 두 변수는 강한 양의 선형 관계를 가지고 있다는 것을 의미함.

이 경우, 공부 시간이 늘어날수록 시험 점수도 증가하는 경향이 보이므로 Pearson 상관 계수는 높을 것으로 예상


<br/>

2. Rank-order correlation (순위 상관 계수)

정의: 두 변수 간의 관계를 순위를 기반으로 측정하는 방법. 이 방법은 선형 관계뿐만 아니라 비선형 관계에도 유용하게 사용될 수 있다. 두 변수의 값에 대한 순위를 매기고, 그 순위들 간의 선형 관계를 측정함 

<br/>

구하는 방법

1. 변수 y와 yi 의 값을 각각 오름차순으로 정렬하여 순위를 매긴다

2. 데이터 포인트의 값을 그에 해당하는 순위로 대체

3. 두 변수의 순위들 간의 선형 상관 관계를 계산

<br/>

## Performance Metrics

손실 함수는 모델의 예측 값과 실제 값 간의 차이를 수치화한 값으로, 모델 학습에서 직접적으로 최적화 대상이 됨

**그러나 모든 성능 지표가 손실 함수로 적합한 것은 아니다**

<br/>

**성능지표와 손실함수 차이**

성능 지표는 모델의 예측 성능을 평가하기 위한 기준으로 사용됨. 예를 들어, 정확도(Accuracy), 정밀도(Precision), 재현율(Recall) 등이 성능 지표에 해당한다. 이러한 성능 지표들은 모델의 평가에 유용하지만, 직접적으로 모델을 최적화하는 데 사용되지는 않음

반면, 손실 함수는 모델을 학습시키기 위한 최적화 함수로, 모델이 학습하면서 개선해야 하는 목표를 정의함. 따라서 손실 함수는 스칼라 값으로 표현되며, 이를 최소화하는 방향으로 모델이 학습됨

<br/>

ML 문제에 적합한 성능 지표를 신중하게 선택해야 한다는 점이 강조

예를 들어, 자동차 운전 모델의 경우, 단순히 목적지에 도달했는지 여부만 평가할 수 없고, 다음과 같은 다양한 성능 지표를 고려할 수 있습니다:

얼마나 자주 성공적으로 목적지에 도달했는가?

얼마나 빨리 목적지에 도달했는가?

얼마나 많은 교통 법규를 위반했는가?

**이상적인 손실 함수는 모든 성능 지표를 포괄할 수 있어야 한다** 즉, 손실 함수가 모델의 학습을 이끌어가면서도 해당 성능 지표들을 반영하도록 설계되어야 함

또한, **손실 함수는 머신러닝에 적합해야 하며**, 실제로 모델 학습에 유용하고 계산 가능한 형태로 설계되어야 함

<br/>

## Feature Maps φ(𝑥) (특징맵)

![Image](https://github.com/user-attachments/assets/a199cafa-2b21-4a76-87ab-7908a8cd226c)

feature map 정의: 수학적으로 입력 데이터를 **고차원 특징 벡터로 변환하는 함수**

고차원으로 변환하는 이유는 **비선형 문제를 해결할 수 있기 때문이다**

예를 들어, 클래스 A와 B가 각각 원형으로 분포하고 있을 때, 2D 평면에서는 이들을 직선으로 구분할 수 없음 

고차원 공간으로 매핑하면, 이 원형으로 분포한 데이터가 고차원에서는 선형적으로 분리될 수 있다

<br/>

**hypothesis space (가설 공간)** 은 우리가 선택한 모델이 탐색하는 가능한 함수들의 집합을 의미함

특징 맵 φ(x)를 적용하면, 입력 데이터 x가 변형되면서, 모델이 학습할 수 있는 함수의 형태가 달라진다

특히, 비선형 변환을 수행하는 경우, 원래 선형적인 가설 공간에서 벗어나 더 복잡한 패턴을 학습할 수 있음

예를 들어, 선형 회귀 모델이 원래 1차 함수만 학습할 수 있었다면, 특징 맵을 통해 고차원 공간으로 변환하면 2차, 3차 등의 비선형 함수도 학습 가능해진다

<br/>

특징 맵은 단순한 변환이 아니라, 우리가 모델이 학습해야 할 함수에 대한 **prior knowledge(사전지식)** 을 반영하는 도구이다

예를 들어, 특정 형태(예: 제곱, 로그, 지수 함수 등)를 따른다는 것을 알고 있다면, 그에 맞는 특징 맵을 설계할 수 있다

<br/>

**용어 정리**

x: **input**이라 하고  원본 입력값이라는 의미

φ(x): **feature**이라하고 입력을 변환한 특징 벡터이다 

즉, 특징 맵 φ(x)을 적용하면, 원본 입력 x가 새로운 특징 공간으로 변환됨을 의미함

<br/>

## intercept term(절편항)

𝜙𝑥 =[1 𝑥1 … 𝑥𝑑]𝑇 에서 x에 모든 값에 0을 넣었을 때 나오는 상수를 절편항이라고 함 (함수에서 x절편 y절편과 같은 의미)

절편항이 중요한 이유는 **데이터가 원점을 지나지 않아도 모델이 적절한 예측을 할 수 있도록 하기 때문이다**

만약, 모든 데이터가 원점 (0,0) 부근에 있지 않을 때, 단순 선형 모델 (y = 𝛽 * x)을 사용하면 적절한 학습이 어려움.

하지만, 절편이 포함된 모델 (y = 𝛽0 + 𝛽1 * x)은 데이터의 평균적인 위치를 반영할 수 있다

<br/>

예를 들어 어떤 제품의 가격을 예측하는 모델이 있다고 가정할 때,

단순 모델: Price = 𝛽 × Feature

절편 포함 모델: Price = 𝛽0(초기 값) + 𝛽 × Feature

제품의 기본 가격(예: 최소 비용)이 존재한다면 절편이 이를 반영할 수 있음.

절편이 없을 경우 모델의 해석이 왜곡될 수 있고, 모델이 특정 데이터 분포에서 성능이 크게 떨어질 수 있다 

<br/>

## non-real input

범주형 데이터(예: 학력 수준: "고등학교", "대학", "석사", "박사")는 수치형 데이터가 아니므로 머신러닝 모델에서 직접 사용할 수 없음.

이를 해결하기 위해 수치로 변환하는 방식이 필요함.

ex) "고등학교"=[1,0,0,0],"대학"=[0,1,0,0],"석사"=[0,0,1,0],"박사"=[0,0,0,1]

이렇게 하면 학력 간의 거리 개념이 사라지고, 모델이 학력을 개별적인 범주로 인식할 수 있음.

<br/>

텍스트나 이미지 같은 데이터는 원래 형태 그대로는 수학적 연산이 어렵기 때문에, **수치적인 특징(feature)** 으로 변환하는 과정이 필요함 

특징 맵 ϕ(x)은 문장에서 중요한 단어를 찾아서 수치 벡터로 변환하는 역할을 함

예를 들어, 특정 단어가 포함되었는지를 이진 벡터(binary vector) 로 나타낼 수도 있다


<예시>

"bad" 	0   (문장에 없음)

"good"	1   (문장에 있음)

이렇게 변환하여 학습을 함 

<br/>

## 과적합 

feature을 많이 추가하면 더 표현력이 높은 가설 공간(hypothesis space)을 만들 수 있다 

하지만 **모든 데이터 포인트를 지나도록 모델을 만들면 과적합(overfitting) 문제가 발생할 수 있다.** -> 일반화 능력 필요 

![Image](https://github.com/user-attachments/assets/19428832-47c8-4b27-b5f4-ec7a90c82f38)

<br/>

**overfitting(과적합)**

복잡한 데이터를 맞출 수 있는 대용량 모델

훈련 데이터가 너무 적거나 다양하지 않아서 모델이 데이터의 세부 사항을 과도하게 학습하게 됩니다.

즉, **세부 사항까지 너무 잘 학습한 나머지, 일반적인 패턴을 놓치고 특정 데이터에만 최적화된 결과를 내는 현상이다**

<br/>

**underfitting(과소적합)**

모델이 너무 단순하여 훈련 데이터의 패턴을 제대로 학습하지 못하는 상황. 모델이 데이터의 중요한 특성을 놓치거나, 너무 단순하여 실제 데이터의 복잡한 관계를 반영하지 못하는 경우이다

충분한 데이터가 있지만, 모델이 데이터를 과소적합하여 좋은 성능을 내지 못하고, 모델이 중요한 패턴을 놓쳐서 정확성이 떨어짐 

![Image](https://github.com/user-attachments/assets/891d224c-c8c3-4a63-855d-2b91949471d4)

Ltrain은 train data의 Loss function 의미

<br/>

## generalization ability (일반화 능력)

일반화란 모델이 훈련 데이터에 포함되지 않은 새로운 데이터(즉, 테스트 데이터나 실제 데이터를 기준으로 성능을 발휘할 수 있는 능력)를 잘 처리하는 능력

일반화 능력이 좋은 모델은 훈련 데이터에서 학습한 내용을 새로운 데이터에 잘 적용할 수 있다. 

이런 모델은 훈련 데이터만을 기억하는 것이 아니라, 일반적인 패턴을 학습하여 과적합(overfitting)을 피하고, 실제 상황에서도 좋은 예측을 할 수 있다

<br/>

## Capacity

Capacity: 모델이 데이터를 적합(fit)할 수 있는 능력 (search space가 얼마나 넓은가)

**Higher capacity** ➔ more likely to **overfit** (model family has high variance)
 
**Lower capacity** ➔ more likely to **underfit** (model family has high bias)

<br/>

선형 회귀에서의 용량은 특징맵의 차원 d로 판단한다 

ex) 1차원 용량 -> 1, 2차원 용량 -> 2, n 차원 용량 -> n

<br/>

![image](https://github.com/user-attachments/assets/52e13859-9f3b-4997-82d3-b474f040ac67)

<br/>

## Bias & Variance

**Variance(분산)** : **모델이 데이터셋에 얼마나 민감하게 반응하는지**를 측정하는 값

분산은 학습된 여러 모델들 간의 차이가 평균적으로 얼마나 큰지를 나타냄

1. 분산이 높다면? → 학습 데이터가 조금만 달라져도 모델이 크게 변함 → 일반화 성능 저하 가능성 증가.

2. 분산이 낮다면? → 학습 데이터가 달라져도 모델이 크게 변하지 않음 → 데이터에 덜 민감.

<br/>

**Bias(편향)** : **모델이 본질적으로 오차를 내는 정도**를 측정하는 값

1. 편향이 높다면? → 모델이 데이터의 패턴을 잘 반영하지 못하고 단순한 가정을 함 → 과소적합(Underfitting) 가능성 증가.

2. 편향이 낮다면? → 모델이 데이터의 패턴을 잘 반영하여 "진짜" 함수에 가까운 결과를 낼 가능성이 높음.







































