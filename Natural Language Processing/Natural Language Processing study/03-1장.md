### 단어 출현 빈도를 활용한 벡터화

-  특정 문장이나 문서를 그 의미에 따라 벡터화하기 위한 방법 중 하나로써 **Bag-of-Words(BoW)** 를 활용 가능 

- 아래는 NLTK를 활용한 문서 토큰화를 한 뒤 파이썬 Counter 객체를 활용한 토큰별 출현 횟수 계산한 예시 코드이다

    - NLTK : 파이썬 NLP 라이브러리, BoW 실습에 활용된 도구 

```ruby
# NLTK의 TreebankWordTokenizer를 불러오기
from nltk.tokenize import TreebankWordTokenizer  

# 예시 문장 정의 (Harry가 더 빨리 상점에 갈수록 집에 더 빨리 도착한다는 의미)
sentence = """The faster Harry got to the store, the faster Harry, the faster, would get home."""

# 토크나이저 객체 생성
tokenizer = TreebankWordTokenizer()

# 문장을 소문자로 변환한 뒤 토큰화 실행
tokens = tokenizer.tokenize(sentence.lower())

# 토큰 출력
print(tokens)
# 출력 결과:
# ['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster',
#  'harry', ',', 'the', 'faster', ',', 'would', 'get', 'home', '.']


# collections 모듈의 Counter를 불러오기 (토큰 빈도수 계산용)
from collections import Counter  

# 토큰 리스트를 Bag-of-Words 형식으로 변환 (각 단어의 등장 횟수를 센다)
bag_of_words = Counter(tokens)

# 가장 많이 등장한 상위 4개 단어 출력
print(bag_of_words.most_common(4))
# 출력 결과: [('the', 4), (',', 3), ('faster', 3), ('harry', 2)]
# - 'the'는 4번 등장
# - ','(쉼표)는 3번 등장
# - 'faster'는 3번 등장
# - 'harry'는 2번 등장
```
<br/>

- **코드 해석**

    - 가장 많이 출현한 토큰들 중 관사 ‘the’와 문장부호 ‘,’는 의미 해석에는 크게 도움이 되지 않음
 
        - 따라서 일반적인 경우에 **불용어 처리** 
     
        - 불용어를 제외하면 많이 등장한 단어는 faster, harry ->  어느 정도 문장의 의미를 대변할 수 있는 단어

<br/>

- **Bag-of-Words(BoW) 방식의 한계점**

    - 그러나 단어의 출현 횟수만으로 문장이나 문서를 벡터로 나타낸다면 문장, **문서의 길이가 제각각**이기 때문에 긴 문장에서와 짧은 문장에서의 벡터값 분포가 매우 달라짐
 
        - 어떤 짧은 문장은 5번 등장도 많은 반면 긴 문서는 1000번 정도는 나와야 많이 나왔다고 할  수 있음
     
        - **해결책** : **정규화**를 하여 **비율**로 표현

<br/>

- 위 예시 문장에서 'harry'의 출현 빈도를 구해보자

```ruby
# 'harry' 단어가 등장한 횟수 가져오기
times_harry_appears = bag_of_words['harry']

# 전체 토큰 수 계산
num_unique_words = len(tokens)

# TF 계산: 단어 출현 횟수 / 전체 토큰 
tf = times_harry_appears / num_unique_words

# 소수점 4자리까지 반올림
print(round(tf, 4))
# 출력 예시: 0.1818
```

- 이와 같은 식으로 구한 특정 단어의 출현 빈도를 **TF(Term Frequency)** 라고 하며 전통적인 NLP에서의 주요 특징 중 하나로 널리 활용되고 있음 

- TF 활용 예시 코드

```ruby
TF(“dog,” document_A) = 3/30 = .1
TF(“dog,” document_B) = 100/580000 = .00017
```

<br/>

## 실제 문서를 활용한 TF 

- 아래 문장은 nlpia 라이브러리에서 테스트용으로 제공하는 샘플 문서 데이터이다

    - from nlpia.data.loaders import kite_text 로 불러올 수 있다 

![System Resources](../../images/Natural%20Language%20Processing%20images/kite문장예시.png)

<br/>

```ruby
from collections import Counter  
from nltk.tokenize import TreebankWordTokenizer  

# 토크나이저 객체 생성
tokenizer = TreebankWordTokenizer()

# kite_text 불러오기 (교재/예시에서 제공하는 텍스트: "A kite is traditionally ..." 로 시작하는 문단)
from nlpia.data.loaders import kite_text  

# 텍스트를 소문자로 변환 후 토큰화
tokens = tokenizer.tokenize(kite_text.lower())

# 단어별 출현 횟수 계산
token_counts = Counter(tokens)

# 단어별 등장 빈도 출력
print(token_counts)
# 출력 예시:
# Counter({'the': 26, 'a': 20, 'kite': 16, ',': 15, ...})

# 해당 결과를 보면  ‘a’, ‘the’ 등의 관사가 최상위에 위치해 있어서 불용어 제거를 수행하고자 함
# 아래 코드부터 불용어를 제거하는 코드이다


```




















































