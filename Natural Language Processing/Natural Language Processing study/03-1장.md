### 단어 출현 빈도를 활용한 벡터화

-  특정 문장이나 문서를 그 의미에 따라 벡터화하기 위한 방법 중 하나로써 **Bag-of-Words(BoW)** 를 활용 가능 

- 아래는 NLTK를 활용한 문서 토큰화를 한 뒤 파이썬 Counter 객체를 활용한 토큰별 출현 횟수 계산한 예시 코드이다

    - NLTK : 파이썬 NLP 라이브러리, BoW 실습에 활용된 도구 

```ruby
# NLTK의 TreebankWordTokenizer를 불러오기
from nltk.tokenize import TreebankWordTokenizer  

# 예시 문장 정의 (Harry가 더 빨리 상점에 갈수록 집에 더 빨리 도착한다는 의미)
sentence = """The faster Harry got to the store, the faster Harry, the faster, would get home."""

# 토크나이저 객체 생성
tokenizer = TreebankWordTokenizer()

# 문장을 소문자로 변환한 뒤 토큰화 실행
tokens = tokenizer.tokenize(sentence.lower())

# 토큰 출력
print(tokens)
# 출력 결과:
# ['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster',
#  'harry', ',', 'the', 'faster', ',', 'would', 'get', 'home', '.']


# collections 모듈의 Counter를 불러오기 (토큰 빈도수 계산용)
from collections import Counter  

# 토큰 리스트를 Bag-of-Words 형식으로 변환 (각 단어의 등장 횟수를 센다)
bag_of_words = Counter(tokens)

# 가장 많이 등장한 상위 4개 단어 출력
print(bag_of_words.most_common(4))
# 출력 결과: [('the', 4), (',', 3), ('faster', 3), ('harry', 2)]
# - 'the'는 4번 등장
# - ','(쉼표)는 3번 등장
# - 'faster'는 3번 등장
# - 'harry'는 2번 등장
```
<br/>

- **코드 해석**

    - 가장 많이 출현한 토큰들 중 관사 ‘the’와 문장부호 ‘,’는 의미 해석에는 크게 도움이 되지 않음
 
        - 따라서 일반적인 경우에 **불용어 처리** 
     
        - 불용어를 제외하면 많이 등장한 단어는 faster, harry ->  어느 정도 문장의 의미를 대변할 수 있는 단어

<br/>

- **Bag-of-Words(BoW) 방식의 한계점**

    - 그러나 단어의 출현 횟수만으로 문장이나 문서를 벡터로 나타낸다면 문장, **문서의 길이가 제각각**이기 때문에 긴 문장에서와 짧은 문장에서의 벡터값 분포가 매우 달라짐
 
        - 어떤 짧은 문장은 5번 등장도 많은 반면 긴 문서는 1000번 정도는 나와야 많이 나왔다고 할  수 있음
     
        - **해결책** : **정규화**를 하여 **비율**로 표현

<br/>

- 위 예시 문장에서 'harry'의 출현 빈도를 구해보자

```ruby
# 'harry' 단어가 등장한 횟수 가져오기
times_harry_appears = bag_of_words['harry']

# 전체 토큰 수 계산
num_unique_words = len(tokens)

# TF 계산: 단어 출현 횟수 / 전체 토큰 
tf = times_harry_appears / num_unique_words

# 소수점 4자리까지 반올림
print(round(tf, 4))
# 출력 예시: 0.1818
```

- 이와 같은 식으로 구한 특정 단어의 출현 빈도를 **TF(Term Frequency)** 라고 하며 전통적인 NLP에서의 주요 특징 중 하나로 널리 활용되고 있음 

- TF 활용 예시 코드

```ruby
TF(“dog,” document_A) = 3/30 = .1
TF(“dog,” document_B) = 100/580000 = .00017
```

<br/>

## 실제 문서를 활용한 TF 

- 아래 문장은 nlpia 라이브러리에서 테스트용으로 제공하는 샘플 문서 데이터이다

    - from nlpia.data.loaders import kite_text 로 불러올 수 있다 

![System Resources](../../images/Natural%20Language%20Processing%20images/kite문장예시.png)

<br/>

```ruby
from collections import Counter  
from nltk.tokenize import TreebankWordTokenizer  

# 토크나이저 객체 생성
tokenizer = TreebankWordTokenizer()

# kite_text 불러오기 (교재/예시에서 제공하는 텍스트: "A kite is traditionally ..." 로 시작하는 문단)
from nlpia.data.loaders import kite_text  

# 텍스트를 소문자로 변환 후 토큰화
tokens = tokenizer.tokenize(kite_text.lower())

# 단어별 출현 횟수 계산
token_counts = Counter(tokens)

# 단어별 등장 빈도 출력
print(token_counts)
# 출력 예시:
# Counter({'the': 26, 'a': 20, 'kite': 16, ',': 15, ...})




# 해당 결과를 보면  ‘a’, ‘the’ 등의 관사가 최상위에 위치해 있어서 불용어 제거를 수행하고자 함
# 아래 코드부터 불용어를 제거하는 코드이다




import nltk  

# 불용어(stopwords) 데이터를 다운로드 (영어 기준)
nltk.download('stopwords', quiet=True)  

# 영어 불용어 목록 불러오기
stopwords = nltk.corpus.stopwords.words('english')  

# 기존 tokens 리스트에서 불용어에 해당하지 않는 단어만 남김
tokens = [x for x in tokens if x not in stopwords]  

# 단어별 출현 횟수 계산
kite_counts = Counter(tokens)  

# 결과 출력 (예: 'kite'가 16번, 'wing'이 5번 등장 등)
print(kite_counts)
# Counter({'kite': 16, 'traditionally': 1, 'tethered': 2, 'heavier-than-air': 1,
#          'craft': 2, 'wing': 5, 'surfaces': 1, 'react': 1, 'air': 2, ..., 'made': 1})



# 위 코드를 보면 불용어가 제가된 것을 볼 수 있다
# 아래 코드는 출현 빈도(TF) 구현 코드이다



# 문서 벡터를 저장할 빈 리스트 생성
document_vector = []  

# 문서의 전체 단어 수(토큰 수)
doc_length = len(tokens)  

# 가장 많이 등장한 단어 순으로 순회하면서 TF 계산
# .most_common() : collections.Counter 클래스가 제공하는 메서드
# .most_common()은 단어(토큰)과 그 빈도를 많이 등장한 순서대로 정렬된 (key, value) 튜플 형태로 반환함
for key, value in kite_counts.most_common():  
    # TF = (단어 등장 횟수 / 전체 단어 수)
    document_vector.append(value / doc_length)  

# 최종적으로 TF 벡터 출력
print(document_vector)
# 예시 출력:
# [0.07207, 0.06757, 0.03603, ..., 0.00450]
# 결과를 보면 빈도가 높은 단어부터 TF 값이 저장됨
```

<br/>

### 벡터를 통한 유사도 계산

- 아래 코드 예시를 통해 유사도를 확인해보자 

```ruby
from nltk.tokenize import word_tokenize


# 비교할 세 문장 준비
docs = [  "The faster Harry got to the store, the faster and faster Harry would get home."
]
docs.append("Harry is hairy and faster than Jill.")
docs.append("Jill is not as hairy as Harry.")


doc_tokens = []  # 문장별 토큰 리스트를 담을 그릇
for doc in docs:
    doc_tokens += [sorted(word_tokenize(doc.lower()))]  # 각 문장을 소문자화 + 단어 토큰화 + 정렬해서 리스트에 저장


# 첫 번째 문장의 토큰 개수
len(doc_tokens[0])  
# 출력 결과: 17
# (예: ['.', ',', 'and', 'faster', 'faster', 'faster', 'get', 'got', 'harry', 'harry', 'home', 'store', 'the', 'the', 'to', 'would', '...'])

# 세 문장의 모든 토큰을 하나의 리스트로 합침
all_doc_tokens = sum(doc_tokens, [])  
len(all_doc_tokens)  
# 출력 결과: 33 (3문장에서 나온 전체 토큰 개수)

# 고유 단어(중복 제거) 집합을 만든 뒤 정렬 → 어휘집(lexicon)
lexicon = sorted(set(all_doc_tokens))  
len(lexicon)  
# 출력 결과: 18 (고유 단어 개수)

lexicon  
# 출력 결과:
# [',', '.', 'and', 'as', 'faster', 'get', 'got', 'hairy',
#  'harry', 'home', 'is', 'jill', 'not', 'store', 'than',
#  'the', 'to', 'would']

```

- 현재 모든 3개 문서에 등장하는 토큰이 18개 있으므로 각 문서를 표현하기 위해서 18개의 성분을 가진 벡터가 필요함

    - 특정 문서에 **등장하지 않는 단어**는 출현 횟수/빈도가 **0**이 됨
 
    - 또한 각 벡터의 성분의 위치(index)에 따라 대응되는 토큰이 **동일하게 유지**되어야 함
 
        - 즉, 인덱스 0은 항상 ,, 인덱스 1은 항상 ., 인덱스 2는 항상 and 로 고정이 되어야 비교가 가능하다
     
- 아래 코드는 위 코드를 이어서 유사도를 계산하는 코드이다

<br/>

```ruby
from collections import OrderedDict  # 일반 dict와 달리 입력한 순서를 유지
zero_vector = OrderedDict((token, 0) for token in lexicon) # 휘집에 있는 단어들을 하나씩 꺼내서 각 단어를 key로 하고, 값은 0으로 설정



# zero_vector 출력 예시 : OrderedDict([(',', 0), ('.', 0), ('and', 0), ('as', 0),
#             ('faster', 0), ('get', 0), ('got', 0), 
#             ('hairy', 0), ('harry', 0), ('home', 0), 
#             ('is', 0), ('jill', 0), ('not', 0), 
#             ('store', 0), ('than', 0), ('the', 0), 
#             ('to', 0), ('would', 0)])



import copy  # 위에서 만든 zero_vector
doc_vectors = []   # 문서별 TF 벡터를 담을 리스트

for doc in docs:
    vec = copy.copy(zero_vector)     # zero_vector를 복사해서 새로운 벡터 생성, copy.copy()를 두 번 한 이유는 모듈 이름과 함수 이름이 동일하기 때문 
    tokens = tokenizer.tokenize(doc.lower())   # 문장을 소문자로 바꾸고 토큰화
    token_counts = Counter(tokens)   # 각 단어별 출현 횟수 계산
    
    for key, value in token_counts.items():
        vec[key] = value / len(tokens)   # 출현 횟수 ÷ 문서 길이 → TF 값 저장
    
    doc_vectors.append(vec)   # 완성된 문서 벡터를 리스트에 추가


# 이렇게 벡터화가 되어야 코사인을 이용해서 유사도를 구할 수 있음
```

<br/>

### 코드 분석 

- 첫번째 문장 : "The faster Harry got to the store, the faster and faster Harry would get home."

    - 토큰 개수: 17개
   
    - 출현 단어: faster(3), harry(2), got(1), get(1), to(1), the(2), store(1), ,(1), .(1), would(1), home(1)
 
    - TF 값 = 횟수 ÷ 토큰 개수 = 횟수 ÷ 17
 
    - 출력 예시 : OrderedDict([(',', 0.0588), ('.', 0.0588), ('and', 0.0), ('as', 0.0),
             ('faster', 0.1765), ('get', 0.0588), ('got', 0.0588),
             ('hairy', 0.0), ('harry', 0.1176), ('home', 0.0588),
             ('is', 0.0), ('jill', 0.0), ('not', 0.0), ('store', 0.0588),
             ('than', 0.0), ('the', 0.1176), ('to', 0.0588), ('would', 0.0588)])

<br/>

- 두번쨰 문장 : "Harry is hairy and faster than Jill."

    - 토큰 개수: 8개
    
    - 출현 단어: harry(1), is(1), hairy(1), and(1), faster(1), than(1), jill(1), .(1)

    - TF 값 = 횟수 ÷ 8 = 0.125
 
    - 출력 예시 : OrderedDict([(',', 0.0), ('.', 0.125), ('and', 0.125), ('as', 0.0),
             ('faster', 0.125), ('get', 0.0), ('got', 0.0),
             ('hairy', 0.125), ('harry', 0.125), ('home', 0.0),
             ('is', 0.125), ('jill', 0.125), ('not', 0.0), ('store', 0.0),
             ('than', 0.125), ('the', 0.0), ('to', 0.0), ('would', 0.0)])

<br/>

- 세번째 문장 : "Jill is not as hairy as Harry."

    - 토큰 개수: 8개

    - 출현 단어: jill(1), is(1), not(1), as(2), hairy(1), harry(1), .(1)

     - TF 값 = 횟수 ÷ 8
 
     - 출력 예시 : OrderedDict([(',', 0.0), ('.', 0.125), ('and', 0.0), ('as', 0.25),
             ('faster', 0.0), ('get', 0.0), ('got', 0.0),
             ('hairy', 0.125), ('harry', 0.125), ('home', 0.0),
             ('is', 0.125), ('jill', 0.125), ('not', 0.125), ('store', 0.0),
             ('than', 0.0), ('the', 0.0), ('to', 0.0), ('would', 0.0)])
       
<br/>

- 여기서 **코사인 θ 값 자체가 유사도 점수**이다 아래 예시를 보자

- ex: lexicon = [‘harry’, ‘faster’, ‘jill’] 라고 가정

    - 문서 A 벡터 = [0.2, 0.3, 0.0] 가정 

    - 문서 B 벡터 = [0.1, 0.3, 0.1] 가정
 
        - 구하고자 하는 코사인 값은은 아래 공식이다
     
            -  ![System Resources](../../images/Natural%20Language%20Processing%20images/03-1장코사인공식.png)
 
        - 내적 공식은 아래와 같다 
 
            -  ![System Resources](../../images/Natural%20Language%20Processing%20images/03-1장내적공식.png)
         
        -  크기 공식은 아래와 같다
     
            -  ![System Resources](../../images/Natural%20Language%20Processing%20images/03-1장크기공식.png)
 
        - 따라서 최종 계산은 아래와 같다
           
            - 내적 = (0.2×0.1) + (0.3×0.3) + (0.0×0.1) = 0.02 + 0.09 + 0 = 0.11

            - A 크기 = √(0.2² + 0.3² + 0.0²) = √(0.04 + 0.09) = √0.13 ≈ 0.3606

            - B 크기 = √(0.1² + 0.3² + 0.1²) = √(0.01 + 0.09 + 0.01) = √0.11 ≈ 0.3317

            - 코사인 유사도 = 0.11 / (0.3606 × 0.3317) ≈ 0.92  
              
<br/>

### 코사인 유사도 추가 내용 

- 위에서 벡터화를 한 정보를 바탕으로 **코사인**을 이용하여 유사도를 구해보자

    - TF의 경우를 생각해보면 절대 빈도가 아니라 단어들 사이의 TF **비율**에 근거해서 유사도를 판단한다고 할 수 있음
 
        - 쉽게 말해 문서 길이가 다르니까 단순히 "몇 번 나왔다"는 횟수로 비교하면 불공정하다는 의미
     
    - 자연어 문서의 벡터 공간에서 **차원 수**는 **어휘집의 토큰 개수**와 같음
 
        - 이와 같은 차원 수를 K 또는 |V|로 표현함
     
            - 위 예시 코드인 경우 k = 18  
     
        -  

    















































