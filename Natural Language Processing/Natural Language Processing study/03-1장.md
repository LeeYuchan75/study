### 단어 출현 빈도를 활용한 벡터화

-  특정 문장이나 문서를 그 의미에 따라 벡터화하기 위한 방법 중 하나로써 **Bag-of-Words(BoW)** 를 활용 가능 

- 아래는 NLTK를 활용한 문서 토큰화를 한 뒤 파이썬 Counter 객체를 활용한 토큰별 출현 횟수 계산한 예시 코드이다

    - NLTK : 파이썬 NLP 라이브러리, BoW 실습에 활용된 도구 

```ruby
# NLTK의 TreebankWordTokenizer를 불러오기
from nltk.tokenize import TreebankWordTokenizer  

# 예시 문장 정의 (Harry가 더 빨리 상점에 갈수록 집에 더 빨리 도착한다는 의미)
sentence = """The faster Harry got to the store, the faster Harry, the faster, would get home."""

# 토크나이저 객체 생성
tokenizer = TreebankWordTokenizer()

# 문장을 소문자로 변환한 뒤 토큰화 실행
tokens = tokenizer.tokenize(sentence.lower())

# 토큰 출력
print(tokens)
# 출력 결과:
# ['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster',
#  'harry', ',', 'the', 'faster', ',', 'would', 'get', 'home', '.']


# collections 모듈의 Counter를 불러오기 (토큰 빈도수 계산용)
from collections import Counter  

# 토큰 리스트를 Bag-of-Words 형식으로 변환 (각 단어의 등장 횟수를 센다)
bag_of_words = Counter(tokens)

# 가장 많이 등장한 상위 4개 단어 출력
print(bag_of_words.most_common(4))
# 출력 결과: [('the', 4), (',', 3), ('faster', 3), ('harry', 2)]
# - 'the'는 4번 등장
# - ','(쉼표)는 3번 등장
# - 'faster'는 3번 등장
# - 'harry'는 2번 등장
```
<br/>

- **코드 해석**

    - 가장 많이 출현한 토큰들 중 관사 ‘the’와 문장부호 ‘,’는 의미 해석에는 크게 도움이 되지 않음
 
        - 따라서 일반적인 경우에 **불용어 처리** 
     
        - 불용어를 제외하면 많이 등장한 단어는 faster, harry ->  어느 정도 문장의 의미를 대변할 수 있는 단어

<br/>

- **Bag-of-Words(BoW) 방식의 한계점**

    - 그러나 단어의 출현 횟수만으로 문장이나 문서를 벡터로 나타낸다면 문장, **문서의 길이가 제각각**이기 때문에 긴 문장에서와 짧은 문장에서의 벡터값 분포가 매우 달라짐
 
        - 어떤 짧은 문장은 5번 등장도 많은 반면 긴 문서는 1000번 정도는 나와야 많이 나왔다고 할  수 있음
     
        - **해결책** : **정규화**를 하여 **비율**로 표현

<br/>

- 위 예시 문장에서 'harry'의 출현 빈도를 구해보자

```ruby
# 'harry' 단어가 등장한 횟수 가져오기
times_harry_appears = bag_of_words['harry']

# 전체 토큰 수 계산
num_unique_words = len(tokens)

# TF 계산: 단어 출현 횟수 / 전체 토큰 
tf = times_harry_appears / num_unique_words

# 소수점 4자리까지 반올림
print(round(tf, 4))
# 출력 예시: 0.1818
```

- 이와 같은 식으로 구한 특정 단어의 출현 빈도를 **TF(Term Frequency)** 라고 하며 전통적인 NLP에서의 주요 특징 중 하나로 널리 활용되고 있음 

- TF 활용 예시 코드

```ruby
TF(“dog,” document_A) = 3/30 = .1
TF(“dog,” document_B) = 100/580000 = .00017
```

<br/>

## 실제 문서를 활용한 TF 

- 아래 문장은 nlpia 라이브러리에서 테스트용으로 제공하는 샘플 문서 데이터이다

    - from nlpia.data.loaders import kite_text 로 불러올 수 있다 

![System Resources](../../images/Natural%20Language%20Processing%20images/kite문장예시.png)

<br/>

```ruby
from collections import Counter  
from nltk.tokenize import TreebankWordTokenizer  

# 토크나이저 객체 생성
tokenizer = TreebankWordTokenizer()

# kite_text 불러오기 (교재/예시에서 제공하는 텍스트: "A kite is traditionally ..." 로 시작하는 문단)
from nlpia.data.loaders import kite_text  

# 텍스트를 소문자로 변환 후 토큰화
tokens = tokenizer.tokenize(kite_text.lower())

# 단어별 출현 횟수 계산
token_counts = Counter(tokens)

# 단어별 등장 빈도 출력
print(token_counts)
# 출력 예시:
# Counter({'the': 26, 'a': 20, 'kite': 16, ',': 15, ...})




# 해당 결과를 보면  ‘a’, ‘the’ 등의 관사가 최상위에 위치해 있어서 불용어 제거를 수행하고자 함
# 아래 코드부터 불용어를 제거하는 코드이다




import nltk  

# 불용어(stopwords) 데이터를 다운로드 (영어 기준)
nltk.download('stopwords', quiet=True)  

# 영어 불용어 목록 불러오기
stopwords = nltk.corpus.stopwords.words('english')  

# 기존 tokens 리스트에서 불용어에 해당하지 않는 단어만 남김
tokens = [x for x in tokens if x not in stopwords]  

# 단어별 출현 횟수 계산
kite_counts = Counter(tokens)  

# 결과 출력 (예: 'kite'가 16번, 'wing'이 5번 등장 등)
print(kite_counts)
# Counter({'kite': 16, 'traditionally': 1, 'tethered': 2, 'heavier-than-air': 1,
#          'craft': 2, 'wing': 5, 'surfaces': 1, 'react': 1, 'air': 2, ..., 'made': 1})



# 위 코드를 보면 불용어가 제가된 것을 볼 수 있다
# 아래 코드는 출현 빈도(TF) 구현 코드이다



# 문서 벡터를 저장할 빈 리스트 생성
document_vector = []  

# 문서의 전체 단어 수(토큰 수)
doc_length = len(tokens)  

# 가장 많이 등장한 단어 순으로 순회하면서 TF 계산
# .most_common() : collections.Counter 클래스가 제공하는 메서드
# .most_common()은 단어(토큰)과 그 빈도를 많이 등장한 순서대로 정렬된 (key, value) 튜플 형태로 반환함
for key, value in kite_counts.most_common():  
    # TF = (단어 등장 횟수 / 전체 단어 수)
    document_vector.append(value / doc_length)  

# 최종적으로 TF 벡터 출력
print(document_vector)
# 예시 출력:
# [0.07207, 0.06757, 0.03603, ..., 0.00450]
# 결과를 보면 빈도가 높은 단어부터 TF 값이 저장됨
```

<br/>

### 벡터를 통한 유사도 계산

- 아래 코드 예시를 통해 유사도를 확인해보자 

```ruby
from nltk.tokenize import word_tokenize


# 비교할 세 문장 준비
docs = [  "The faster Harry got to the store, the faster and faster Harry would get home."
]
docs.append("Harry is hairy and faster than Jill.")
docs.append("Jill is not as hairy as Harry.")


doc_tokens = []  # 문장별 토큰 리스트를 담을 그릇
for doc in docs:
    doc_tokens += [sorted(word_tokenize(doc.lower()))]  # 각 문장을 소문자화 + 단어 토큰화 + 정렬해서 리스트에 저장


# 첫 번째 문장의 토큰 개수
len(doc_tokens[0])  
# 출력 결과: 17
# (예: ['.', ',', 'and', 'faster', 'faster', 'faster', 'get', 'got', 'harry', 'harry', 'home', 'store', 'the', 'the', 'to', 'would', '...'])

# 세 문장의 모든 토큰을 하나의 리스트로 합침
all_doc_tokens = sum(doc_tokens, [])  
len(all_doc_tokens)  
# 출력 결과: 33 (3문장에서 나온 전체 토큰 개수)

# 고유 단어(중복 제거) 집합을 만든 뒤 정렬 → 어휘집(lexicon)
lexicon = sorted(set(all_doc_tokens))  
len(lexicon)  
# 출력 결과: 18 (고유 단어 개수)

lexicon  
# 출력 결과:
# [',', '.', 'and', 'as', 'faster', 'get', 'got', 'hairy',
#  'harry', 'home', 'is', 'jill', 'not', 'store', 'than',
#  'the', 'to', 'would']

```

- 현재 모든 3개 문서에 등장하는 토큰이 18개 있으므로 각 문서를 표현하기 위해서 18개의 성분을 가진 벡터가 필요함

    - 특정 문서에 **등장하지 않는 단어**는 출현 횟수/빈도가 **0**이 됨
 
    - 또한 각 벡터의 성분의 위치(index)에 따라 대응되는 토큰이 **동일하게 유지**되어야 함
 
        - 즉, 인덱스 0은 항상 ,, 인덱스 1은 항상 ., 인덱스 2는 항상 and 로 고정이 되어야 비교가 가능하다
     
- 아래 코드는 위 코드를 이어서 유사도를 계산하는 코드이다

<br/>

```ruby
from collections import OrderedDict  # 일반 dict와 달리 입력한 순서를 유지
zero_vector = OrderedDict((token, 0) for token in lexicon) # 휘집에 있는 단어들을 하나씩 꺼내서 각 단어를 key로 하고, 값은 0으로 설정



# zero_vector 출력 예시 : OrderedDict([(',', 0), ('.', 0), ('and', 0), ('as', 0),
#             ('faster', 0), ('get', 0), ('got', 0), 
#             ('hairy', 0), ('harry', 0), ('home', 0), 
#             ('is', 0), ('jill', 0), ('not', 0), 
#             ('store', 0), ('than', 0), ('the', 0), 
#             ('to', 0), ('would', 0)])



import copy  # 위에서 만든 zero_vector
doc_vectors = []   # 문서별 TF 벡터를 담을 리스트

for doc in docs:
    vec = copy.copy(zero_vector)     # zero_vector를 복사해서 새로운 벡터 생성, copy.copy()를 두 번 한 이유는 모듈 이름과 함수 이름이 동일하기 때문 
    tokens = tokenizer.tokenize(doc.lower())   # 문장을 소문자로 바꾸고 토큰화
    token_counts = Counter(tokens)   # 각 단어별 출현 횟수 계산
    
    for key, value in token_counts.items():
        vec[key] = value / len(tokens)   # 출현 횟수 ÷ 문서 길이 → TF 값 저장
    
    doc_vectors.append(vec)   # 완성된 문서 벡터를 리스트에 추가


# 이렇게 벡터화가 되어야 코사인을 이용해서 유사도를 구할 수 있음
```

<br/>

### 코드 분석 

- 첫번째 문장 : "The faster Harry got to the store, the faster and faster Harry would get home."

    - 토큰 개수: 17개
   
    - 출현 단어: faster(3), harry(2), got(1), get(1), to(1), the(2), store(1), ,(1), .(1), would(1), home(1)
 
    - TF 값 = 횟수 ÷ 토큰 개수 = 횟수 ÷ 17
 
    - 출력 예시 : OrderedDict([(',', 0.0588), ('.', 0.0588), ('and', 0.0), ('as', 0.0),
             ('faster', 0.1765), ('get', 0.0588), ('got', 0.0588),
             ('hairy', 0.0), ('harry', 0.1176), ('home', 0.0588),
             ('is', 0.0), ('jill', 0.0), ('not', 0.0), ('store', 0.0588),
             ('than', 0.0), ('the', 0.1176), ('to', 0.0588), ('would', 0.0588)])

<br/>

- 두번쨰 문장 : "Harry is hairy and faster than Jill."

    - 토큰 개수: 8개
    
    - 출현 단어: harry(1), is(1), hairy(1), and(1), faster(1), than(1), jill(1), .(1)

    - TF 값 = 횟수 ÷ 8 = 0.125
 
    - 출력 예시 : OrderedDict([(',', 0.0), ('.', 0.125), ('and', 0.125), ('as', 0.0),
             ('faster', 0.125), ('get', 0.0), ('got', 0.0),
             ('hairy', 0.125), ('harry', 0.125), ('home', 0.0),
             ('is', 0.125), ('jill', 0.125), ('not', 0.0), ('store', 0.0),
             ('than', 0.125), ('the', 0.0), ('to', 0.0), ('would', 0.0)])

<br/>

- 세번째 문장 : "Jill is not as hairy as Harry."

    - 토큰 개수: 8개

    - 출현 단어: jill(1), is(1), not(1), as(2), hairy(1), harry(1), .(1)

     - TF 값 = 횟수 ÷ 8
 
     - 출력 예시 : OrderedDict([(',', 0.0), ('.', 0.125), ('and', 0.0), ('as', 0.25),
             ('faster', 0.0), ('get', 0.0), ('got', 0.0),
             ('hairy', 0.125), ('harry', 0.125), ('home', 0.0),
             ('is', 0.125), ('jill', 0.125), ('not', 0.125), ('store', 0.0),
             ('than', 0.0), ('the', 0.0), ('to', 0.0), ('would', 0.0)])
       
<br/>

- 여기서 **코사인 θ 값 자체가 유사도 점수**이다 아래 예시를 보자

- ex: lexicon = [‘harry’, ‘faster’, ‘jill’] 라고 가정

    - 문서 A 벡터 = [0.2, 0.3, 0.0] 가정 

    - 문서 B 벡터 = [0.1, 0.3, 0.1] 가정
 
        - 구하고자 하는 코사인 값은은 아래 공식이다
     
            -  ![System Resources](../../images/Natural%20Language%20Processing%20images/03-1장코사인공식.png)
 
        - 내적 공식은 아래와 같다 
 
            -  ![System Resources](../../images/Natural%20Language%20Processing%20images/03-1장내적공식.png)
         
        -  크기 공식은 아래와 같다
     
            -  ![System Resources](../../images/Natural%20Language%20Processing%20images/03-1장크기공식.png)
 
        - 따라서 최종 계산은 아래와 같다
           
            - 내적 = (0.2×0.1) + (0.3×0.3) + (0.0×0.1) = 0.02 + 0.09 + 0 = 0.11

            - A 크기 = √(0.2² + 0.3² + 0.0²) = √(0.04 + 0.09) = √0.13 ≈ 0.3606

            - B 크기 = √(0.1² + 0.3² + 0.1²) = √(0.01 + 0.09 + 0.01) = √0.11 ≈ 0.3317

            - 코사인 유사도 = 0.11 / (0.3606 × 0.3317) ≈ 0.92  
              
<br/>

### 코사인 유사도 추가 내용 

- 위에서 벡터화를 한 정보를 바탕으로 **코사인**을 이용하여 유사도를 구해보자

    - TF의 경우를 생각해보면 절대 빈도가 아니라 단어들 사이의 TF **비율**에 근거해서 유사도를 판단한다고 할 수 있음
 
        - 쉽게 말해 문서 길이가 다르니까 단순히 "몇 번 나왔다"는 횟수로 비교하면 불공정하다는 의미
     
    - 자연어 문서의 벡터 공간에서 **차원 수**는 **어휘집의 토큰 개수**와 같음
 
        - 이와 같은 차원 수를 K 또는 |V|로 표현함
     
            - 위 예시 코드인 경우 k = 18  

<br/>

### 벡터 유사도 계산 코드 

```ruby
import math

# 유사도를 계산하기 위한 코사인 함수
def cosine_sim(vec1, vec2):
    # vec1과 vec2의 값(value)들만 추출해서 리스트로 만듦
    # 위에서 구한 OrderedDict([(',', 0.0), ('.', 0.125), ('and', 0.125), ...]) 값에서 .values() 를 해주면  [0.0, 0.125, 0.125, ... ] 로 나옴 
    vec1 = [val for val in vec1.values()]
    vec2 = [val for val in vec2.values()]

    # 내적(dot product) 값을 저장할 변수를 0으로 초기화
    dot_prod = 0
    # vec1의 각 요소(v)와 인덱스(i)를 순회
    for i, v in enumerate(vec1):
        # vec1의 값과 vec2의 같은 위치에 있는 값을 곱해 내적 값에 더함
        dot_prod += v * vec2[i]

    # vec1의 크기(magnitude)를 계산. 각 요소를 제곱하여 모두 더한 뒤 제곱근을 구한다 
    mag_1 = math.sqrt(sum([x**2 for x in vec1]))
    # vec2의 크기(magnitude)를 계산합니다.
    mag_2 = math.sqrt(sum([x**2 for x in vec2]))

    # 최종적으로 코사인 유사도 공식을 적용하여 내적을 두 벡터의 크기의 곱으로 나눈 값을 반환합니다.
    return dot_prod / (mag_1 * mag_2)
```
    
- 코사인 유사도는 -1에서 +1 사이의 값을 가지며, **값이 높을수록 유사도**가 높다

    - 두 벡터가 얼마나 비슷한지 숫자로 나타낸 것이다
 
    - **음수**의 경우 **반대 방향**을 의미하며, -1이면 정반대의 방향이다
 
        - ex: "그 영화는 정말 재미있고 훌륭해" 와 "그 영화는 정말 지루하고 형편없어"는 상반되는 문
 
    - **0**이면 **공통점이 전혀 없다**는 것을 의미
 
        - ex: "나는 사과를 좋아해." 와 "코끼리는 아프리카에 산다." 는 아무 관련이 없음

<br/>

### 역문서 빈도 (IDF, Inverse Document Frequency)

- **역문서 빈도 (IDF, Inverse Document Frequency)**

    - 각 단어에 대해 해당 단어가 출현한 문서 수를 전체 문서 수에 대해 나눈 것

    - 전체 문서 수 / 해당 단어 출현 문서 수
 
    - IDF 값이 높을수록 특정 문서에서만 등장하는 단어가 되며 이는 각 문서의 **주제를 대표하는 단어**일 확률이 높아짐
 
        - '그리고', '나는', '매우' 같은 단어는 IDF 값이 낮게 나옴
     
        - '인공지능', '양자역학' 같은 단어는 특정 주제를 다루는 몇 개의 문서에만 등장할 것임 -> 희귀함 

<br/>

- 추가적으로 '역'문서 빈도인 이유는

    - DF를 보면, 흔한 단어일수록 값이 높고(↑), 중요한(희귀한) 단어일수록 값이 낮은(↓) 것과 반대되기 때문이다
      
<br/>

### 역문서 빈도 (IDF, Inverse Document Frequency) 코드  

- 위에서 사용한 kite 위키피디아 문서 중 history 영역의 예시 문서를 사용함 

```ruby
# Counter는 단어의 개수를 쉽게 세기 위해 사용하는 도구
from collections import Counter


# word_tokenize는 문장을 단어 단위로 쪼개주는 함수(토크나이저)
from nltk.tokenize import word_tokenize import nltk


# intro_text 변수에 소문자로 변환된 텍스트를 할당
intro_text = kite_text.lower()
# intro_text를 단어 단위(토큰)로 쪼갬
intro_tokens = tokenizer.tokenize(intro_text)


# history_text를 소문자로 변환
history_text = history_text.lower()
# history_text를 단어 단위(토큰)로 쪼갬
history_tokens = tokenizer.tokenize(history_text)


# intro 문서의 전체 단어(토큰) 수를 계산
intro_total = len(intro_tokens)
# history 문서의 전체 단어(토큰) 수를 계산
history_total = len(history_tokens)


# TF(단어 빈도) 값을 저장할 공간(딕셔너리)을 만듦
intro_tf = {}
history_tf = {}

# intro 문서에 있는 각 단어의 개수를 셈
intro_counts = Counter(intro_tokens)
# intro 문서에서 'kite'의 TF를 계산함 (kite 등장 횟수 / 전체 단어 수)
intro_tf['kite'] = intro_counts['kite'] / intro_total

# history 문서에 있는 각 단어의 개수를 셉니다.
history_counts = Counter(history_tokens)
# history 문서에서 'kite'의 TF를 계산합니다.
history_tf['kite'] = history_counts['kite'] / history_total

# 'kite'의 TF 계산 결과를 출력합니다.
print('Term Frequency of "kite" in intro is: {}'.format(intro_tf['kite']))
print('Term Frequency of "kite" in history is: {}'.format(history_tf['kite']))

# 출력값은 아래와 같음
# Term Frequency of "kite" in intro is: 0.0440771349862259
# Term Frequency of "kite" in history is: 0.0202020202020204
# 약간의 차이는 존재하지만, 이 수치만으로 두 문서에서 kite의 중요도가 다르다고 단정짓기는 어려


# intro 문서에서 'and'의 TF를 계산
intro_tf['and'] = intro_counts['and'] / intro_total
# history 문서에서 'and'의 TF를 계산
history_tf['and'] = history_counts['and'] / history_total


# 'and'의 TF 계산 결과를 출력
print('Term Frequency of "and" in intro is: {}'.format(intro_tf['and']))
print('Term Frequency of "and" in history is: {}'.format(history_tf['and']))


# 출력값은 아래와 같음 
# 결과: Term Frequency of "and" in intro is: 0.027548209366391185
# 결과: Term Frequency of "and" in history is: 0.0303030303030304



# 아래 코드부터는  kite, and, china에 대한 IDF 비교하는 코드이다



# 'kite'가 포함된 문서 수를 세기 위한 변수를 0으로 설정
num_docs_containing_kite = 0
# 'and'가 포함된 문서 수를 세기 위한 변수를 0으로 설정
num_docs_containing_and = 0
# 'china'가 포함된 문서 수를 세기 위한 변수를 0으로 설정
num_docs_containing_china = 0

# 두 문서를 차례대로 확인하는 반복문
for doc in [intro_tokens, history_tokens]:
    # 현재 문서에 'kite'가 있으면, 해당 변수에 1을 더함
    if 'kite' in doc:
        num_docs_containing_kite += 1
    # 현재 문서에 'and'가 있으면, 해당 변수에 1을 더함
    if 'and' in doc:
        num_docs_containing_and += 1
    # 현재 문서에 'china'가 있으면, 해당 변수에 1을 더함
    if 'china' in doc:
        num_docs_containing_china += 1

# 'kite'의 IDF를 계산하여 출력 (전체 문서 수: 2)
print('kite IDF: ', 2 / num_docs_containing_kite)
# 'and'의 IDF를 계산하여 출력
print('and IDF: ', 2 / num_docs_containing_and)
# 'china'의 IDF를 계산하여 출력
print('china IDF: ', 2 / num_docs_containing_china)


# 출력값은 아래와 같다
# kite IDF: 1.0
# and IDF: 1.0
# china IDF: 2.0
# 즉, china 한 문서(history)에만 등장했으므로 희귀성이 높음 -> 특정 문서를 더 잘 설명해주는 키워드임
```







































