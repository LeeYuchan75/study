### 불용어(stopword)

- **불용어(stopword)**

    - 자주 출현하지만 그 의미는 중요치 않은 단어들을 의미함
 
    - ex: 관사(a, an, the)나 접속사(and, or)
 
    - 어떤 토큰들을 불용어로 처리하여 제외할지는 상황에 따라 다름
    
    - 단, NLTK 등의 라이브러리에서 일반적으로 쓰는 불용어 리스트는 존재
 
        - 라이브러리마다 불용어 리스트가 다를 수 있음  
 
    - 전통적인 NLP에서는 불용어 제거가 필수적, 하지만 딥러닝 기반 NLP에서는 그 중요성이 예전보다는 감소함 (스스로 처리하기 때문)
  
<br/>

- scikit-learn과 NLTK 불용어의 합집합 및 교집합 관련 예시

```ruby
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as sklearn_stop_words
from nltk.corpus import stopwords

len(sklearn_stop_words) # scikit-learn이 제공하는 불용어 개수
### 출력 : 318

len(stopwords.words('english')) # NLTK에서 제공하는 영어 불용어 개수를 세어줌 
### 출력 : 179

len(stop_words.union(sklearn_stop_words)) # NLTK와 scikit-learn 불용어를 합쳐서 중복 없이 모은 총 개수를 계산
### 출력 : 378

len(stop_words.intersection(sklearn_stop_words)) # 두 라이브러리 불용어 목록에서 공통으로 들어 있는 단어 개수를 계산
### 출력 : 119
```

<br/>

### 정규화(normalization)

- **정규화(normalization)** : 실질적으로는 의미가 같으나 표현 방법이 다른 단어들을 통합하는 과정

    - 대소문자 통합
 
        - 장점 : 약 절반 정도 토큰 수를 줄일 수 있음
     
            -  기계학습 모델의 과적합을 줄여줌
         
            -  검색에 활용될 경우 검색 엔진의 재현율(recall) 개선에 기여할 수 있음
         
            -  만약 통합되지 않으면 Age와 age가 다른 단어로 인식
     
        - 단점 : 대문자로 인해 지명, 상표 등 고유명사를 인식이 어려움 -> 성능 저하
     
            - 의도적으로 쓰인 대문자를 구분, 문장의 시작을 소문자로 적용하는 방법이 존재
         
                - 하지만, 이 경우 "Tom is here."처럼 사람 이름으로 시작할 경우 문제 발생
             
                - **즉, 상황에 따라 깊이 생각해보고 적용할 필요가 있음**
    
    - 어간 추출
 
        - 어간 : 복수형(-s), 진행형(-ing) 등을 떼어난 근본 단어 
    
    - 표제어 추출
 
        - 표제어 : 기본형

<br/>

### 재현율(recall) & 정밀도(precision)

- 검색 성능을 평가할 때 쓰는 두 가지 지표

    - **재현율(recall)**: 실제로 관련 있는 문서 중에서 얼마나 많이 찾아냈는가?

    - **정밀도(precision)**: 찾아낸 문서 중에서 실제로 관련 있는 문서가 얼마나 되는가?

<br/>

- 일반적으로 어휘 정규화는 검색의 **재현율을 높이지만 정밀도를 낮추게 됨**

    - 정규화를 하면 많은 문서를 찾을 수 있지만, 그중에는 원하지 않는 결과도 섞일 가능성이 커짐 
    
    - ex: "Apple" (회사 Apple을 찾고 싶음) -> 정규화로 "apple"로 변환 -> **정밀도 저하**
    
    - 이에 검색 엔진에서 정규화를 회피하는 수단으로 “” 기능을 제공
     
       - "" : 정규화 없이 그대로 검색할 수 있게 기능

<br/>

### 어간 추출(stemming) 기법

- **어간 추출(stemming) 기법**

    - 추출된 어간은 반드시 사전에 나오는 형태여야 할 필요는 없고, 한 어간의 다양한 형태를 대표할 수 있으면 됨
 
    - ex: study, studies, studying, studied 를 studi로 추출을 하여 하나로 묶어도 괜찮음
 
       - 어간 추출은 완벽한 단어가 목표가 아니라 하나로 **같은 뿌리를 공유하는 단어를 하나로 묶는게 핵심**

<br/>

- 어간 추출 또한 정규화의 일종이므로 **재현율을 높이지만 정밀도는 감소**

    - ex:  “Dr. House’s call” 라는 미국 드라마 제목을 어간추출 해주면 “dr house call”
 
        - 결과적으로 드라마가 아니라 “의사 집에 전화하다” 같은 엉뚱한 의미로 검색될 수 있음 

<br/>

### 간단한 어간 추출 예시 

- 규칙
 
    - 단어가 둘 이상의 s로 끝나면, 어간은 그 단어 자체이며 별도의 접미사 없음
    
    - 단어가 하나의 s로 끝나면 어간은 s를 제외한 부분이며, s가 접미사가 됨
   
    - 단어가 s로 끝나지 않으면 어간은 그 단어 자체이고 별도의 접미사가 없음

<br/>

```ruby
>>> def stem(phrase):  
...     # phrase(문자열)을 입력으로 받아서 어간을 추출하는 함수 정의
...     
...     return ' '.join(   # 처리된 단어들을 공백으로 연결하여 반환
...         [re.findall('^(.*ss|.*?)(s)?$',  # 정규표현식으로 단어 끝의 s를 구분
...                      word)[0][0].strip("'")  # 매칭된 그룹 중 어간 부분만 추출
...          for word in phrase.lower().split()]  # 입력 문자열을 소문자로 바꾸고 단어 단위로 나눈 후 반복
...     )

>>> stem('houses')
'house'  
# "houses" → 끝의 s를 제거하고 "house"만 남김

>>> stem("Doctor House's calls")
'doctor house call'
# 1) "Doctor" → 소문자로 변환 → "doctor"
# 2) "House's" → s 제거 → "house"
# 3) "calls" → s 제거 → "call"
# 모두 합쳐서 'doctor house call' 반환
```

-  PorterStemmer 예시

<br/>

```ruby
>>> from nltk.stem.porter import PorterStemmer  
# NLTK 라이브러리에서 PorterStemmer 클래스를 불러옴 (영어에서 가장 널리 쓰이는 어간 추출기 중 하나)

>>> stemmer = PorterStemmer()  
# PorterStemmer 객체 생성 → stemmer라는 이름으로 사용

>>> ' '.join([stemmer.stem(w).strip("'") for w in "dish washer's washed dishes".split()])  
# 1) 문자열 "dish washer's washed dishes"를 공백 기준으로 나눔 → ["dish", "washer's", "washed", "dishes"]
# 2) 각 단어에 대해 stemmer.stem(w) 적용 → 어간 추출
#    - "dish"     → "dish"
#    - "washer's" → "washer"
#    - "washed"   → "wash"
#    - "dishes"   → "dish"
# 3) strip("'") : 단어 양쪽의 작은 따옴표(')가 있으면 제거
# 4) 변환된 결과를 공백으로 join → 하나의 문자열로 합침

# 최종 결과
'dish washer wash dish'
```

<br/>

### 표제어 추출(lemmatization)

- **표제어 추출(lemmatization)**

    - 토큰들을 해당 의미의 근본적인 형태인 어근 수준으로 정규화하는 방식
 
        - ex: am, are, is → be 

    - 형태적인 부분보다 **의미적인 부분에 집중**하여 정규화를 수행
 
        - 고급 표제어 추출기 : 각 토큰의 품사는 물론 주변 문맥까지 고려
 
            - 어간 추출 ex:  running → runn 과 같이 어색한 결과
     
            - 표제어 추출 ex: better → good 으로 사전에 있는 기본형으로 변환

<br/>

## p.15 시간 남으면 코드 공부 

<br/>

### 감정 분석(sentiment analysis)

- **감정 분석(sentiment analysis)**

   - 글(단어, 문장 등)에 담긴 감정을 찾아내고, 그것이 긍정인지 부정인지 등을 나누어 점수화하는 작업

<br/>

- **구현 기본 아이디어**

    - 텍스트에서 특정 키워드들을 찾고 키워드들에 부여된 점수를 취합하는 방식

    - **(키워드, 감정 점수)** 쌍들을 담은 딕셔너리가 필요함
 
        - ex: happy → +1, sad → -1 

    - 키워드를 정확히 매칭하기 위한 **토큰화 방식**과 **정규화 여부**가 중요
 
        - ‘Why are you doing this?’ -> 일반 톤 
        
        - ‘WHY ARE YOU DOING THIS?’” -> 화난 톤으로 인식할 수 있음

<br/>

### 규칙 기반 감정 분석 알고리즘

- 조지아 공과대학에서 만든 규칙 기반 감정 분석 알고리즘을 살펴보자 

```ruby
>>> from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
>>> sa = SentimentIntensityAnalyzer()
>>> sa.lexicon # 토큰-감정 점수 쌍들이 들어 있음 

>>> sa.polarity_scores(text=\
{
    ':(': -1.9,   # VADER는 이모티콘들을 활용 
    ':)': 2.0,    # ":)" → 긍정 점수, ":(" → 부정 점수

...


'pls': 0.3,       # pls와 plz 모두 'please'의 줄임말 -> 같은 점수로 통합해야함 
'plz': 0.3,
...
'great': 3.1,
... }

>>> [(tok, score) for tok, score in sa.lexicon.items()  # sa.lexicon 에서 (단어, 감정 점수)를 가져옴 
 ... if " " in tok]      # 그중에서 공백(" ")이 들어 있는 토큰만 고름 
[("('}{')", 1.6),
 ("can't stand", -2.0),
 ('fed up', -1.8),
 ('screwed up', -1.5)]   # 즉, VADER에는 약 7,500개의 단어 중 띄어쓰기가 들어간 토큰은 3개이다. 그 중 2개는 n-그램이고, 나머지 1개는 키스 "('}{')" 이모티콘 이다 

>>>  sa.polarity_scores(text=\
 ... "Python is very readable and it's great for NLP.")
{'compound': 0.6249, 'neg': 0.0, 'neu': 0.661, 'pos': 0.339}
# pos : 긍정 점수
# neg : 부정 점수
# neu : 중립 점수
# compound : 종합 점수

 
 ... "Python is not a bad choice for most applications.")
{'compound': 0.431, 'neg': 0.0, 'neu': 0.711, 'pos': 0.289}
# 위 문장에서 'not' 을 처리하는 게 핵심이다
# 'not bad'의 긍정 점수는 'great'보다 약간 낮을 정도로 높다
# VADER 사전에 없는 단어는 그냥 무시
# "not good", "not bad" 같은 대표적인 조합은 규칙으로 따로 처리
# 하지만 "not only good but also amazing" 같은 복잡한 긴 구문은 개별적 단위로 계산 
```
































































