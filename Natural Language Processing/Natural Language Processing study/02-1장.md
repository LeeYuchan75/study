### 음절(Syllable)

- **음절** : 가장 작은 발화의 단위

    - 한국어는 초성, 중성, 종성으로 이루어져 있음
 
    - 한국어 음절 구성은 아래와 같이 네 종류로 구성됨
 
![System Resources](../../images/Natural%20Language%20Processing%20images/음절예시.png)

<br/>

### 형태소(Morpheme)

- **형태소** : 의미를 가지는 가장 작은 단위

    - 더 쪼개면 의미가 사라지는 최소 단위
    
    - ex: "학교들" → "학교"(의미 O) + "들"(복수 의미 O)
 
        - 여기서 더 쪼개면 의미 없음 → “학-교”는 의미 없음 

    - 의미의 유무에 따라 구분

        - **실질 형태소**: 실제 의미가 있는 것 (책, 밥, 먹다)

        - **형식 형태소**: 문법적 기능만 있는 것 (가, 을, 에)
     
    - 자립성 유무에 따라 구분

        - **자립 형태소**: 혼자 쓸 수 있는 것 (책, 사람)

        - **의존 형태소**: 반드시 다른 말과 함께 써야 함 (가, 을, 다)
     
        - 실질 형태소인데도 의존적인 경우 : 접두사 "새-" (새집, 새사람) → 의미는 있지만 혼자 못 씀

<br/>

### 어절(Word Phrase)

- **어절** : 한 개 이상의 형태소가 모여 말하기/쓰기 단위가 됨

    - 말할 때는 끊어 읽는 단위, 글에서는 띄어쓰기 단위

    - ex: “오늘도 나는 국어문법을 공부한다”

        - “오늘+도” / “나+는” / “국어문법+을” / “공부한다” → 총 4개의 어절.

<br/>

### 자연어 처리에서의 벡터화 단위

- 텍스트 정보를 분석하기 위한 최소 단위는 아래와 같

    - **문자 단위(char-level)**
 
        - 글자 하나하나(T, H, E)를 숫자 특징으로 바꿔서 모델에 넣음
     
        - 한자와 같이 각 문자가 고유한 의미를 갖는 표의문자를 분석하기 용이함
    
    - **단어 단위(word-level)**
 
        - 단어 전체("THE")를 하나의 단위로 숫자 특징으로 바꿔서 모델에 넣음

        - 어, 한글과 같은 표음문자는 일반적으로 단어 단위가 되어서야 의미를 가짐 -> 단어 단위가 용이함 

<br/>

- **토큰(token)** : 자연어 처리에서 집중하는 최소 의미 단위를 뜻하는 단어로써 어절, 형태소등의 
단위로 구분 가능함

    - 영어는 띄어쓰기 단위로 구분해도 큰 문제가 없는 편이라 **어절**을 주로 활용
    
    - 한국어는 ‘은‘, ‘는‘, ‘를‘ 등의 어미가 붙어있으므로 **형태소 단위**를 보다 선호함


- **토큰화(tokenization)** : 주어진 문장, 문서 등 raw 자연어 입력을 토큰 단위로 변경해주는 작업

    -  **토크나이저(tokenizer)** : 토큰화를 담당하는 객체 혹은 모듈
 
    -  전통적인 NLP 뿐 아니라 딥러닝을 활용하는 최신 NLP에서도 토큰화는 여전히 중요한 요소

<br/>

### 단어 토큰화 및 벡터화 예시

1. 문장이 토큰 리스트로 분리 

2. vocab 생성 및 정렬 

3. one-hot 벡터 출력

```ruby
import numpy as np

sentence = "Thomas Jefferson began building Monticello at the age of 26."  # 예시 문장

token_seq = str.split(sentence)   #  split로 토큰화
vocab = sorted(set(token_seq))    # 중복 제거 후 정렬 


# 출력 결과 : ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26.']


num_tokens = len(token_seq)   # 문장 속 단어 개수 = 10 
vocab_size = len(vocab)   # vocab 단어 수 = 10 (중복 제거한 단어의 총 수)

onehot_vectors = np.zeros((num_tokens, vocab_size), int)

for i, word in enumerate(token_seq):         # eunmerate로 리스트를 (인덱스, 값) 형태를 만듦 -> 행렬을 만들기 유리, 원 핫 인코딩은 보편적으로 행렬로 표현

    onehot_vectors[i, vocab.index(word)] = 1

print(onehot_vectors)

print(onehot_vectors)
# 출력 (각 행이 문장의 단어를 one-hot으로 표현):
# [[0 0 0 1 0 0 0 0 0 0]   # Thomas
#  [0 1 0 0 0 0 0 0 0 0]   # Jefferson
#  [0 0 0 0 0 0 1 0 0 0]   # began
#  [0 0 0 0 0 0 0 1 0 0]   # building
#  [0 0 1 0 0 0 0 0 0 0]   # Monticello
#  [0 0 0 0 0 1 0 0 0 0]   # at
#  [0 0 0 0 0 0 0 0 0 1]   # the
#  [0 0 0 0 1 0 0 0 0 0]   # age
#  [0 0 0 0 0 0 0 0 1 0]   # of
#  [1 0 0 0 0 0 0 0 0 0]]  # 26.
```

<br/>

### 원-핫 인코딩 (One-hot encoding)

- **원-핫 인코딩** : 범주형 데이터(예: 단어, 클래스)를 **벡터 형태로 변환**하는 방법

    - 컴퓨터는 숫자만 이해하기 때문에 자연어(문자)를 바로 계산할 수 없음
    
        - 반드시 숫자 벡터로 변환 필요  

    - 다중 클래스 분류 문제에서 label을 만들던 방식과 유사
 
    -  NLP 분야에서는 모델 학습에 사용하는 데이터셋을 **말뭉치(corpus)**
 
        - 말뭉치 정의: 자연언어 연구를 위해 특정한 목적을 가지고 언어의 표본을 추출한 집합
 
- 원-핫 인코딩을 하기 위해서
  
    - 말뭉치 내 모든 단어에 대해 중복을 제거한 리스트를 만든다
   
    - 단어 개수만큼 크기를 가진 벡터(feature vector)를 만든다

    - **각 단어는 자기 위치(index)에만 1, 나머지는 전부 0으로 채운다**
 
        - 이와 같이 벡터 또는 행렬의 값이 **대부분 0**으로 표현되는 것을 **희소 표현**(sparse 
representation)이라고 함

        - 이와 반대로 벡터 또는 행렬의 거의 **대부분 값이 0 이외의 값**을 갖고 있는 것을 **밀집 표현**
(dense representation)이라고 함


![System Resources](../../images/Natural%20Language%20Processing%20images/원핫인코딩결과예시.png)


![System Resources](../../images/Natural%20Language%20Processing%20images/희소표현_밀집표현_예시.png)

<br/>

### 원-핫 인코딩의 한계

- 단어 수가 늘어남에 따라 벡터의 크기가 같이 늘어남 -> **메모리 비효율이 굉장히 심함**

    - 단어 사전(vocab)이 1만 개라면, "cat" 하나도 길이가 1만짜리 벡터로 표현해야 함
 
    - 만개 중 하나만 1이고 9999개가 0이므로 메모리가 너무 큼 

- 이것을 **Bag-of-words로 해결**

<br/>

### 원-핫 인코딩의 해결책 : Bag-of-words

- Bag-of-words 방식을 쓰면 벡터의 길이는 토큰 수와 같다는 점은 원-핫 벡터와 유사함

    - 하지만, 이러한 벡터가 단어 하나를 나타내는 것이 아니라 문장이나 문서를 나타내게 되어 메모리 **효율성이 훨씬 개선됨**
 
    - ex: 문장1: I love NLP / 문장2: I love AI -> Vocabulary: ['I', 'love', 'NLP', 'AI']
 
        - 원 핫 인코딩
     
            - "I" → [1, 0, 0, 0]

            - "love" → [0, 1, 0, 0]

            - "NLP" → [0, 0, 1, 0]

            - "AI" → [0, 0, 0, 1]
         
        - Bag-of-Words
     
            - 문장1 "I love NLP" → [1, 1, 1, 0] (I, love, NLP는 등장 / AI는 없음)

            - 문장2 "I love AI" → [1, 1, 0, 1] (I, love, AI는 등장 / NLP는 없음)
          
<br/>

### 내적 기반의 유사도

- Bag-of-words 방식으로 나타내어진 문장이나 문서의 유사성을 비교할 때 **벡터 내적**을 사용함

    - 문장 1 ex : "Thomas Jefferson began building Monticello at the age of 26."
 
        - 문장 1 벡터 : [1, 1, 1, 1, 1, 1, 0, ...] 
 
    - 문장 2 ex : "He moved into the South Pavilion. Turning Monticello into a neoclassical masterpiece."
 
        -  문장 2 벡터 : [0, 0, 0, 0, 1, 1, 1, ...]

    - 두 문장의 공통 단어 : "Monticello" 와 "the" (BoW 기본형은 불용어도 포함, 현재 예시에서는 the도 포함한다고 가정)
 
        - 이 경우 내적 값은 1x1 + 1x1 = 2 가 됨 (나머지 곱은 모두 0)
     
        - 두 문장의 유사도는 2이고 이것을 상대값으로 이용해서 다른 문장들도 비교하는 방식을 사용

<br/>

- 코드 예시 

```ruby
# 두 개의 문장을 정의 (문장 0, 문장 1)
sentences = "Thomas Jefferson began building Monticello at the age of 26.\n"
sentences += "He moved into the South Pavilion. Turning Monticello into a neoclassical masterpiece."

# 단어 등장 여부를 저장할 딕셔너리 생성
corpus = {}

# enumerate: 각 문장을 인덱스(i)와 함께 순회
for i, sent in enumerate(sentences.split("\n")):
    
    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())
    # sent.split(): 문장 별로 단어 추출
    # dict((tok, 1) for tok in sent.split()): 추출 단어마다 1의 값을 매김
    # corpus['sent0'], corpus['sent1'] 같은 형태로 저장 


df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T

# 내적 값 출력 
similarity = df.sent0.dot(df.sent1)

print("\n내적값 (similarity):", similarity)
```

- 위 코드 과정

- 예시 문장 : sentences = "I like cats.\nYou like dogs."

- sentences.split("\n")
    
    - ["I like cats.", "You like dogs."]

- enumerate(...)
    - (0, "I like cats.")
    - (1, "You like dogs.")

- 첫 번째 문장 처리 (i=0, sent="I like cats.")

    - sent.split() → ["I", "like", "cats."]

    - dict((tok,1) for tok in sent.split())

        - {"I":1, "like":1, "cats.":1}

    - 저장 결과: corpus["sent0"] = {"I":1, "like":1, "cats.":1}

- 두 번째 문장 처리 (i=1, sent="You like dogs.")

    - sent.split() → ["You", "like", "dogs."]

    - dict((tok,1) for tok in sent.split())
        
         - {"You":1, "like":1, "dogs.":1}

    - 저장 결과: corpus["sent1"] = {"You":1, "like":1, "dogs.":1}

- 최종 결과

  - 내적 값 구하기  

<br/>

- 영어 NLP에서 널리 쓰이는 NLTK(Natural Language ToolKit) 사용 예시

```ruby
from nltk.tokenize import TreebankWordTokenizer

# Tokenize: 문장(text)을 작은 단위로 쪼개는 것
# Tokenizer: 이 작업을 수행하는 도구(함수, 클래스, 알고리즘)
# 즉, TreebankWordTokenizer() 는 토크나이저 중 하나임 

tokenizer = TreebankWordTokenizer() 
tokenizer.tokenize(sentence) # sentence는 위 코드에서 문장을 사용 

# 출력 결과
# ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26', '.']
```
































