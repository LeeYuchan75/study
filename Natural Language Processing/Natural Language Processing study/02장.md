### 음절(Syllable)

- **음절** : 가장 작은 발화의 단위

    - 한국어는 초성, 중성, 종성으로 이루어져 있음
 
    - 한국어 음절 구성은 아래와 같이 네 종류로 구성됨
 
![System Resources](../../images/Natural%20Language%20Processing%20images/음절예시.png)

<br/>

### 형태소(Morpheme)

- **형태소** : 의미를 가지는 가장 작은 단위

    - 더 쪼개면 의미가 사라지는 최소 단위
    
    - ex: "학교들" → "학교"(의미 O) + "들"(복수 의미 O)
 
        - 여기서 더 쪼개면 의미 없음 → “학-교”는 의미 없음 

    - 의미의 유무에 따라 구분

        - **실질 형태소**: 실제 의미가 있는 것 (책, 밥, 먹다)

        - **형식 형태소**: 문법적 기능만 있는 것 (가, 을, 에)
     
    - 자립성 유무에 따라 구분

        - **자립 형태소**: 혼자 쓸 수 있는 것 (책, 사람)

        - **의존 형태소**: 반드시 다른 말과 함께 써야 함 (가, 을, 다)
     
        - 실질 형태소인데도 의존적인 경우 : 접두사 "새-" (새집, 새사람) → 의미는 있지만 혼자 못 씀

<br/>

### 어절(Word Phrase)

- **어절** : 한 개 이상의 형태소가 모여 말하기/쓰기 단위가 됨

    - 말할 때는 끊어 읽는 단위, 글에서는 띄어쓰기 단위

    - ex: “오늘도 나는 국어문법을 공부한다”

        - “오늘+도” / “나+는” / “국어문법+을” / “공부한다” → 총 4개의 어절.

<br/>

### 자연어 처리에서의 벡터화 단위

- 텍스트 정보를 분석하기 위한 최소 단위는 아래와 같

    - **문자 단위(char-level)**
 
        - 글자 하나하나(T, H, E)를 숫자 특징으로 바꿔서 모델에 넣음
     
        - 한자와 같이 각 문자가 고유한 의미를 갖는 표의문자를 분석하기 용이함
    
    - **단어 단위(word-level)**
 
        - 단어 전체("THE")를 하나의 단위로 숫자 특징으로 바꿔서 모델에 넣음

        - 어, 한글과 같은 표음문자는 일반적으로 단어 단위가 되어서야 의미를 가짐 -> 단어 단위가 용이함 

<br/>

- **토큰(token)** : 자연어 처리에서 집중하는 최소 의미 단위를 뜻하는 단어로써 어절, 형태소등의 
단위로 구분 가능함

    - 영어는 띄어쓰기 단위로 구분해도 큰 문제가 없는 편이라 **어절**을 주로 활용
    
    - 한국어는 ‘은‘, ‘는‘, ‘를‘ 등의 어미가 붙어있으므로 **형태소 단위**를 보다 선호함


- **토큰화(tokenization)** : 주어진 문장, 문서 등 raw 자연어 입력을 토큰 단위로 변경해주는 작업

    -  **토크나이저(tokenizer)** : 토큰화를 담당하는 객체 혹은 모듈
 
    -  전통적인 NLP 뿐 아니라 딥러닝을 활용하는 최신 NLP에서도 토큰화는 여전히 중요한 요소

<br/>

### 단어 토큰화 및 벡터화 예시

```ruby
import numpy as np

sentence = "Thomas Jefferson began building Monticello at the age of 26."  # 예시 문장

token_seq = str.split(sentence)   #  split로 토큰화
vocab = sorted(set(token_seq))    # 중복 제거 후 정렬 


# 출력 결과 : ['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26.']


num_tokens = len(token_seq)   # 문장 속 단어 개수 = 10 
vocab_size = len(vocab)   # vocab 단어 수 = 10 (중복 제거한 단어의 총 수)

onehot_vectors = np.zeros((num_tokens, vocab_size), int)

for i, word in enumerate(token_seq):         # eunmerate로 리스트를 (인덱스, 값) 형태를 만듦 -> 행렬을 만들기 유리, 원 핫 인코딩은 보편적으로 행렬로 표현

    onehot_vectors[i, vocab.index(word)] = 1

print(onehot_vectors)

print(onehot_vectors)
# 출력 (각 행이 문장의 단어를 one-hot으로 표현):
# [[0 0 0 1 0 0 0 0 0 0]   # Thomas
#  [0 1 0 0 0 0 0 0 0 0]   # Jefferson
#  [0 0 0 0 0 0 1 0 0 0]   # began
#  [0 0 0 0 0 0 0 1 0 0]   # building
#  [0 0 1 0 0 0 0 0 0 0]   # Monticello
#  [0 0 0 0 0 1 0 0 0 0]   # at
#  [0 0 0 0 0 0 0 0 0 1]   # the
#  [0 0 0 0 1 0 0 0 0 0]   # age
#  [0 0 0 0 0 0 0 0 1 0]   # of
#  [1 0 0 0 0 0 0 0 0 0]]  # 26.









































































