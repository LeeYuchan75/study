### TF-IDF 장단점 

- **장점**

    - TF-IDF 벡터는 문서에서 특정 단어들이 **얼마나 중요한지 추정**하는데 큰 도움이 됨
 
    -  n-그램에 대해서도 TF-IDF를 계산 가능
 
        - 단어뿐 아니라 연속된 단어 묶음(예: '인공지능 연구')에도 적용 가능
     
<br/>

- **한계점**

    - (1) : TF-IDF 벡터의 경우 유의어들이 **서로 다른 토큰일 경우** 아예 다른 벡터 성분으로 표현됨
 
        - ex: '자동차'와 '승용차'처럼 같은 뜻이어도 TF-IDF는 다른 단어로 취급
     
        - 따라서 **같은 주제를 다루지만** 구성하는 단어가 다른 두 문장, 문서의 경우 TF-IDF **벡터 공간에서 가깝지 않을 확률**이 높음 

    - (2) : 어간 추출 및 표제어 추출은 이를 좀 더 보완할 수 있으나 이 역시 완벽하지 않고, 토큰 단계에서 유의어 등을 통합 처리하지 못하면 벡터상에서 다른 차원으로 갈라지게 됨
 
        - 즉, 단어 뿌리(예: 'run', 'running' → 'run')를 맞추는 기법을 쓰면 조금 나아지지만, 완벽히 해결되지 않는다
     
    - (3) : TF-IDF 벡터를 더하거나 빼면 문서의 단어 빈도에 대한 정보가 나오며 곱하더라도 각 차원을 나타내는 단어 간의 상관관계에 대한 정보만 나온다
 
        - 즉, TF-IDF 값끼리 연산해도 단순히 "단어 개수/상관 정도"만 알 수 있고, 의미는 알 수 없다
     
        - 단어의 의미에 대한 **정보를 추출할 수는 없다**
     
<br/>

- **한계점에 대한 가능성**

    - **단어의 의미를 벡터**로 표현할 수 있다면 이를 기반으로 문서/문장의 벡터도 표현 가능하고 단어 벡터끼리의 연산도 가능한 등 TF-IDF 벡터를 사용할 때 보다 더 많은 가능성이 열린다
 
        - 단어 벡터를 나타내기 위해 각 행은 단어를, 각 열은 특정 주제나 속성을 나타낸 행렬을 **단어-주제 행렬**이라고 하며 TF-IDF만으로는 이러한 행렬을 구할 수 없음

<br/>

### 단어-주제 벡터 

- 각 단어별로 각 주제에 대해 어느 정도의 속성을 갖고 있는지를 나타냄

    - 이와 같은 행렬을 만들 수 있다면, [(‘Car’), (‘Animal’), (‘Environment’)] 로 이루어진 각 단어에 대한 벡터를 단어의 의미를 나타내는 벡터로 활용할 수 있음
 
    - ![System Resources](../../images/Natural%20Language%20Processing%20images/04-1장단어주제벡터예시자료car동물환경.png)
 
    - 기본적인 원리를 설명하자면
 
        - (1) : 불용어를 제외한 토큰을 행렬의 열 부분에 나열하고 주제를 행 부분에 나열
     
        - (2) : 각 단어의 TF-IDF 값에 해당 단어와 주제의 연관도를 곱해서 모두 취합함
     
            - 가중치는 기계학습을 통해 학습 (단어와 주제의 관계에 따라 학)

<br/>

- 주제 공간 벡터의 예시는 아래와 같다

    - ![System Resources](../../images/Natural%20Language%20Processing%20images/04-1장주제벡터함수예시자료.png)

<br/>

### 잠재 의미 분석(Latent Semantic Analysis, LSA) 아이디어와 배경 

- **잠재 의미 분석(Latent Semantic Analysis, LSA)** :  단어와 문서 속에 직접 드러나지 않는 숨은 의미 관계를 수학적으로 찾아내는 기법

- 앞에서 가중치는 컴퓨터가 직접 학습을 하여 정해준다고 배웠다

    - 어떻게 컴퓨터가 알고리즘적으로 단어의 의미를 파악하여 주제와의 연관도를 매길 수 있을까?
 
    - 20세기에 활동한 영국의 언어학자 J.R. Firth는 '단어를 알려면 그 일행을 봐야 한다.'라는 말을 남겼으며 여기에서 힌트를 얻어서 의미 분석 및 이후의 **워드 임베딩** 기술이 발전하여 이것을 활용하고 있음
 
        - **워드 임베딩의 원리** : 어떤 단어의 의미는 **같이 출현하는 단어들로부터 추론**할 수 있다는 의미
 
        - BoW 및 TF-IDF에서는 이미 같이 출현하는 단어들의 상대적인 빈도 정보를 포함하고 있음

<br/>

- 따라서 의미 분석에서는 여러 문서들의 TF-IDF 벡터를 모은 문서-벡터 행렬을 기반으로 단어들의 의미를 분석함

    - 가장 핵심이 되는 기법으로 단어의 숨은 의미를 파악한다는 뜻의 잠재 의미 분석(LSA)가 있음
 
    - LSA의 목적 : 단어가 가진 **숨겨진 의미(문맥적 의미)** 를 찾아내는 것
 
    - LSA는 TF-IDF행렬(또는 BoW 행렬)을 분석해서 단어들을 주제들로 요약하는 알고리즘의 일종
 
        - LSA보다 더 단순한 기법으로 LDA(선형 판별 분석)도 존재함 

 ### LDA 분류기 

 - 각 class의 **TF-IDF 벡터 각각의 무게중심(벡터 평균)** 을 계산한다

     - 두 무게중심을 잇는 **직선을 나타내는 벡터**를 구한다
  
     - 각 TF-IDF 벡터들을 무게중심을 이은 벡터에 투영하여 길이를 계산 후, 어느 class에 속하는지 판별한다
  
         - 쉽게 말해서 각 메시지 벡터를 그 기준축에 내리고(투영) 길이를 구한 뒤, 그 값이 스팸 쪽에 가까운지 정상 쪽에 가까운지 확인한다 

 - 스펨문자 데이터셋 예시

     -  총 4837개의 데이터 중 638개의 스팸 메시지가 존재하는 상황

- 데이터 준비 코드 
  
```ruby
import pandas as pd
from nlpia.data.loaders import get_data

pd.options.display.width = 120  # DataFrame 출력 시 가로 폭 설정

sms = get_data('sms-spam')  # SMS 스팸 데이터셋 불러오기

# 인덱스 생성: sms0, sms1 ... 스팸이면 느낌표 추가
# 느낌표는 출력을 위한 것일 뿐 의미있게 쓰이지는 않음 
index = ['sms{}!'.format(i, '!'*j) for (i, j) in zip(range(len(sms)), sms.spam)]

# DataFrame으로 변환, 인덱스 지정
sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)

# spam 컬럼을 정수형(int)으로 변환
sms['spam'] = sms.spam.astype(int)

print(len(sms))        # 전체 데이터 개수 : 4837 (해당 값은 예시)
print(sms.spam.sum())  # 스팸 메시지 개수 : 638 (해당 값은 예시)



#### 학습 데이터 출력 결과는 아래와 같음 ####

#        spam                    text  
# sms0    0  Go until jurong point, crazy.. Available only ...     
# sms1    0  Ok lar... Joking wif u oni...                       
# sms2    1  Free entry in 2 a wkly comp to win FA Cup fina...     
# sms3    0  U dun say so early hor... U c already then say...     
# sms4    0  Nah I don't think he goes to usf, he lives aro...     
# sms5    1  FreeMsg Hey there darling it's been 3 week's n...     




from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize.casual import casual_tokenize

# TF-IDF 모델 생성 (casual_tokenize 사용)
tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize)

# sms.text(문자 메시지들)를 TF-IDF 행렬로 변환 후 배열 형태로 저장
tfidf_docs = tfidf_model.fit_transform(sms.text).toarray()

print(tfidf_docs.shape)  # (문서 개수, 단어 개수) : (4837, 9232) (예시 값),  어휘집의 토큰 수는 9232임을 알 수 있다
# 즉, ML 관점에서 feature가 9000개가 넘는데 데이터 개수는 feature 수의 절반을 조금 넘는 수준에 특히 스팸문자 개수는 638개에 불과함 -> 단어(특징)의 수가 데이터 수에 비해 너무 많고, 스팸 데이터가 상대적으로 적어서 모델이 학습하기 어렵다
print(sms.spam.sum())    # 스팸 메시지 개수 : 638 (예시 값)

```

<br/>

- 무게중심 구하는 코드 

```ruby
# 스팸 여부를 True/False로 변환해 마스크 생성
mask = sms.spam.astype(bool).values  

# 스팸 문서들의 TF-IDF 평균 벡터 (스팸 무게중심)
spam_centroid = tfidf_docs[mask].mean(axis=0)

# 정상 문서들의 TF-IDF 평균 벡터 (정상 메세지 무게중심)
ham_centroid = tfidf_docs[~mask].mean(axis=0)

# 소수 둘째 자리까지 반올림해 출력
print(spam_centroid.round(2)) # 예시 출력 : [0.06, 0. , 0. , 0. , 0. , 0. , ...]

print(ham_centroid.round(2)) # 예시 출력 : [0.02, 0.01, 0. , 0. , 0. , 0. , ...]




# 스팸 무게중심과 정상 무게중심의 차이 벡터 = 분류 기준축
direction_vector = spam_centroid - ham_centroid

# 각 문서(TF-IDF 벡터)를 기준축에 투영한 값 = spamness score
spamminess_score = tfidf_docs.dot(direction_vector)

# 소수 둘째 자리까지 반올림해 출력 
print(spamminess_score.round(2)) # 예시 출력 : [-0.01, -0.02, 0.04, ...])
```

<br/>

- 정규화 및 성능 확인 코드 (참고만)

```ruby
from sklearn.preprocessing import MinMaxScaler

# 스팸 점수를 0~1 사이로 정규화
sms['lda_score'] = MinMaxScaler().fit_transform(
    spamminess_score.reshape(-1, 1)
)

# 임계값 0.5 기준으로 스팸(1) / 정상(0) 분류
sms['lda_predict'] = (sms.lda_score > 0.5).astype(int)

# 스팸 여부, 예측 결과, 점수를 확인 (앞 6개만)
print(sms[['spam', 'lda_predict', 'lda_score']].round(2).head(6))



# 위 코드는 정규화 후 분류
# 아래 코드는 분류 성능 확인 



# 정확도 계산 (실제값 - 예측값의 절대오차 평균)
accuracy = (1. - (sms.spam - sms.lda_predict).abs().sum() / len(sms)).round(3)
print(accuracy)  # 예시: 0.977

# 혼동 행렬 (Confusion Matrix)
from pugnlp.stats import Confusion
print(Confusion(sms['spam lda_predict'.split()]))
```

<br/>

### LDA 내용 정리 

- 선형 판별 분석에서는 class 정보에 기반하여 벡터 공간 상에서 하나의 주제(위 예시에서는 스팸)에 대한 **벡터**를 도출하고 그에 기반하여 **주제에 대한 연관도**를 각 단어 및 문서 단위로 계산함

    - ![System Resources](../../images/Natural%20Language%20Processing%20images/04-1장LDA벡터예시자료.png) 

    - 각 포인트(메시)가 문서의 TF-IDF 벡터이고, 이를 spamness 벡터에 투영한 값이 문서-주제 벡터값이 됨
 
    - 또한 **각 단어**는 벡터 공간에서의 **축**으로 볼 수 있으므로 각 단어의 spamness 연관도 또한 spamness 벡터의 값으로부터 도출 가능함
 
        - TF-IDF 벡터 공간에서는 dim0, dim1, dim2 등 각각의 축이 단어에 해당하므로 해당 축을 나타내는 벡터와 spamness 벡터를 내적하여 단어별 spamness 정도를 도출하는 방식

<br/>

- **LDA**는 원래 잠재 의미 분석을 위한 방법인 것은 아니고, 말 그대로 **선형 판별을 위한 간단한 방법**이지만, **LSA의 핵심 원리가 쓰였다고** 볼 수 있음

    - 단어-주제 관계를 도출한다는 점에서 LSA와 아이디어가 겹 

- LSA는 하나의 주제가 아니라 우리가 **원하는 수의 주제**에 대해 벡터 공간 상에서 주제별 벡터를 도출하고, 이를 바탕으로 각 단어와 문서를 주제 벡터로 표현하는 기법이라고 보면 됨

    - 이를 위해 기본적으로 **SVD(특잇값 분해)** 가 활용되며 **PCA**를 이용한 의미 분석도 가능함

<br/>

### 특이값 분해 



















































