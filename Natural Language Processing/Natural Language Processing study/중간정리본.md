### 간략화, 장단점, 특징 정리 

### 1주차

- **자연어 처리가 어려운 이유**

    - **언어의 중의성** : 동음이의어 (ex: 배), 문맥을 고려해야함 (ex:"정말 잘한다"가 비꼬는 말투)

    - **규칙의 예외**
 
    - **언어의 유연성 및 확장성**
 
<br/>

- **자연어 처리 발전 과정**

    - 1950~60년대 초창기: 규칙 기반 NLP (조건문/패턴 매칭, 단순 챗봇) → 의미 이해 불가, 한계 많음

    - 아이디어 전환: 문자 단위가 아니라 단어 단위 의미로 해석 → 벡터화하여 기계학습 활용

    - **Bag-of-Words** 등장
 
        - 유의미한 단어들의 등장 횟수를 벡터로 표현 (불용어 제외)
     
        - 유사성을 비교할 때 **내적** 활용
        
        - 장점 : 원 핫 인코딩의 메모리 문제 해결 

        - 단점 : 단어들의 순서 및 문법 정보를 손상 (주제만 파악 가능)

    - Bag-of-words 초기 극복 아이디어
 
        - **구문 트리** 사용
     
            - 구문 트리 : 문법적 구조(syntactic structure) 를 표현하는 도구

        
<br/>   

- **NLP 파이프라인**

    - **파싱**: 특징 및 수치 자료 추출 -> 입력 문장에서 필요한 정보(단어, 품사 등)를 뽑음

    - **분석**: 특징 추가 생성 및 결합 -> 뽑아낸 정보를 조합해서 더 의미 있는 표현을 만듦

    - **생성**: 응답문 작성 -> 챗봇이 대답할 문장을 만듦

    - **실행**: 대화 계획 관리 및 응답문을 선택하여 출력 진행 -> 여러 후보 중 적절한 답을 골라서 사용자에게 보여줌

<br/>

### 2주차 

- **원 핫 인코딩**

    - **장점** : 단순하고 직관적, 범주형 데이터를 수학적으로 다룰 수 있음

    - **단점** : 벡터 차원이 커짐(메모리 **비효율**)

    - **해결책** : Bag-of-words
 
        -  벡터의 길이는 토큰 수와 같지만, 문장 자체를 벡터로 사용 -> **메모리 절감**

<br/>

- **n-그램(n-gram)** : **긴 표현(여러 단어)을 하나의 의미 단위**로 보고 싶을 때 사용

<br/>

- **정규화(normalization)** : 질적으로는 의미가 같으나 표현 방법이 다른 단어들을 통합하는 과정

  - 대소문자 통합
 
      - 장점 : 토큰 수 감소, 과적합 감소, 재현율 개선 기여
   
      - 단점 : 고유명사 인식 어려움, 의도적 대문자 구분 x 

  - 어간 추출
 
      - 단어의 뿌리만 추출 
 
      - ex: running → runn

  - 표제어 추출
 
      - 의미적인 부분에 집중
   
      - ex: better → good

<br/>

- **재현율(recall) & 정밀도(precision)**

    - **재현율(recall)**: 실제로 관련 있는 문서 중에서 얼마나 많이 찾아냈는가?

    - **정밀도(precision)**: 찾아낸 문서 중에서 실제로 관련 있는 문서가 얼마나 되는가?
 
    - 일반적으로 어휘 정규화는 **검색의 재현율을 높이지만 정밀도를 낮추게 됨**














