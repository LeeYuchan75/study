### TF-IDF

- **TF-IDF** : 기본적인 개념은 **TF와 IDF를 곱한 특징점**으로써 단어의 출현 빈도와 특정 문서에서의 희귀도를 **모두 고려**한 NLP에서의 특징점

    - 이렇게 하면 'and': TF는 높지만, IDF가 매우 낮아서 TF-IDF 점수가 낮아. (중요하지 않음)
 
    - '손흥민': 특정 축구 기사에서 TF가 높고, 다른 일반 기사에서는 잘 안 나와 IDF도 높으므로 TF-IDF 점수가 매우 높아
 
    - 전통적인 특징점 계산 방식이지만 **매우 효과적**이며 간단한 작업에서는 최근까지도 여전히 TF-IDF 기반으로 NLP 작업을 수행하고 있다
 
    - **어휘집에 없는 토큰**에 대해서는 TF-IDF 값을 **생성할 수 없음**

<br/>

### TF-IDF 코드 

- 03-1장에서 예시로 사용한 kite의 코드를 그대로 이어서 설명함

```ruby

# intro 문서에서 'china'의 TF를 계산
intro_tf['china'] = intro_counts['china'] / intro_total
# history 문서에서 'china'의 TF를 계산
history_tf['china'] = history_counts['china'] / history_total

# 전체 문서 수를 2로 설정
num_docs = 2
# 각 문서의 IDF 값을 저장할 공간(딕셔너리)을 만
intro_idf = {}
history_idf = {}

# 'and'의 IDF 값을 계산. (전체 문서 수 / 'and'가 포함된 문서 수)
intro_idf['and'] = num_docs / num_docs_containing_and
history_idf['and'] = num_docs / num_docs_containing_and
# 'kite'의 IDF 값을 계산
intro_idf['kite'] = num_docs / num_docs_containing_kite
history_idf['kite'] = num_docs / num_docs_containing_kite
# 'china'의 IDF 값을 계산
intro_idf['china'] = num_docs / num_docs_containing_china
history_idf['china'] = num_docs / num_docs_containing_china



# 위 코드는 기본 세팅을 위한 03-1장 예시 코드
# 아래 코드부터 TF-IDF 본격적인 설명



# TF-IDF 점수를 저장할 공간을 만듦
intro_tfidf = {}

# 'and'의 TF-IDF 점수를 계산 (TF * IDF)
intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']
# 'kite'의 TF-IDF 점수를 계산
intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']
# 'china'의 TF-IDF 점수를 계산
intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']

# history 문서의 TF-IDF 점수를 저장할 공간을 만듦 
history_tfidf = {}

# history 문서에서 각 단어의 TF-IDF 점수를 계산
history_tfidf['and'] = history_tf['and'] * history_idf['and']
history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']
history_tfidf['china'] = history_tf['china'] * history_idf['china']

# intro 문서의 최종 TF-IDF 결과
intro_tfidf
# 결과: {'and': 0.027548209366391185, 'kite': 0.0440771349862259, 'china': 0.0}

# history 문서의 최종 TF-IDF 결과
history_tfidf
# 결과: {'and': 0.0303030303030304, 'kite': 0.0202020202020204, 'china': 0.0202020202020204}
```

- 위 코드를 보면 TF-IDF를 계산할 때 TF의 값과 IDF의 값을 곱하여 나타냈다

    - TF로 빈도의 값과 IDF로 희귀도의 값으로 유연하게 사고하는 것이다
 
    - 이번에는 NLP를 공부하며 예시로 사용한 "Harry" 예시 문장을 TF-IDF를 해보자 

 <br/>

```ruby
# 분석의 대상이 될 세 개의 문장을 정의
doc_0 = "The faster Harry got to the store, the faster Harry, the faster, would get home."
doc_1 = "Harry is hairy and faster than Jill."
doc_2 = "Jill is not as hairy as Harry."


# 문서들의 TF-IDF 벡터를 저장할 리스트를 생성
document_tfidf_vectors = []
# 처리할 문서 목록을 만듦 
documents = [doc_0, doc_1, doc_2]
# 각 문서를 순회하는 반복문
for doc in documents:
    # 한 문서를 대표할 벡터(값을 담는 공간)를 초기화
    vec = copy.copy(zero_vector)

    # 문서를 소문자로 바꾸고 단어(토큰) 단위로 쪼갬
    tokens = tokenizer.tokenize(doc.lower())

    # 문서에 있는 각 단어의 개수를 샘 
    token_counts = Counter(tokens)
    
    # (단어, 단어 개수) 쌍으로 반복문을 실행
    for key, value in token_counts.items():

        # 이 단어가 몇 개 문서에 나왔는지 세는 변수를 초기화
        docs_containing_key = 0

        # 전체 문서를 다시 확인하며
        for _doc in documents:
            # 문서 안에 해당 단어가 있으면 카운트를 1 올
            if key in _doc:
                docs_containing_key += 1
        
        # 해당 단어의 TF를 계산합니다. (단어 등장 횟수 / 전체 단어 수)
        tf = value / len(tokens)
        
        # 단어가 한 개 이상의 문서에 포함되었다면
        if docs_containing_key:
            # IDF를 계산
            idf = len(documents) / docs_containing_key
        # 어떤 문서에도 포함되지 않았다면 (이론상 발생하기 어려움)
        else:
            idf = 0
        
        # 최종 TF-IDF 점수를 계산하여 벡터에 단어와 함께 저장
        vec[key] = tf * idf
    # 완성된 한 문서의 TF-IDF 벡터를 전체 리스트에 추가함함
    document_tfidf_vectors.append(vec)


# 질문 문장을 정의
query = "How long does it take to get to the store?"
# 질문 문장을 대표할 벡터를 초기화
query_vec = copy.copy(zero_vector)

# 질문을 소문자로 바꾸고 단어 단위로 쪼갬
tokens = tokenizer.tokenize(query.lower())
# 질문에 있는 각 단어의 개수를 
token_counts = Counter(tokens)

# 질문에 있는 (단어, 단어 개수) 쌍으로 반복문을 실행
for key, value in token_counts.items():
    # 이 단어가 몇 개 문서에 나왔는지 세는 변수를 초기화
    docs_containing_key = 0

    # 전체 문서들을 확인하며
    for _doc in documents:
        # 문서 안에 해당 단어가 있으면 카운트를 1 올림
        if key in _doc.lower():
            docs_containing_key += 1
    
    # 만약 단어가 어떤 문서에도 없었다면, IDF를 계산할 수 없으므로 건너뜀
    # zero_vector에서 가져왔으므로 건들지 않으면 자동으로 0
    if docs_containing_key == 0:
        continue

    # 질문 문장에서 해당 단어의 TF를 계산
    tf = value / len(tokens)

    # IDF를 계산
    idf = len(documents) / docs_containing_key

    # 최종 TF-IDF 점수를 계산하여 질문 벡터에 저장
    query_vec[key] = tf * idf

# 질문 벡터와 첫 번째 문서 벡터의 코사인 유사도를 계산하고 출력
print(cosine_sim(query_vec, document_tfidf_vectors[0]))
# 결과: 0.5235048549676834
# query 문장과 doc_0의 문장을 비교하면 ‘to’, ‘the’, ‘store’ 가 겹침
# 

# 질문 벡터와 두 번째 문서 벡터의 코사인 유사도를 계산하고 출력
print(cosine_sim(query_vec, document_tfidf_vectors[1]))
# 결과: 0.0


# 질문 벡터와 세 번째 문서 벡터의 코사인 유사도를 계산하고 출력
print(cosine_sim(query_vec, document_tfidf_vectors[2]))
# 결과: 0.0
```

<br/>

### 로그 정규화 

- IDF의 기본적인 개념만 고려한다면 위 코드처럼 계산해도 충분하지만, 실제로는 총 문서 수가 **굉장히 많을 수 있다는 문제**가 있음

- 따라서 단순히 전체 문서 수에서 등장 문서 수를 나눈 IDF 값의 **분포가 매우 커질 수 있다**

    - 구글 검색을 했을 때 어떤 단어는 10억 개 문서 중 단 2~3개에만 나타날 수 있다. 이렇게 되면 IDF 값이 기하급수적으로 커져서 다른 단어들과의 중요도 차이가 너무 벌어지게 됨

<br/>

- 이에 대한 **정규화** 방안으로 일반적으로 IDF는 **log**를 취한 값을 활용
  
    - ![System Resources](../../images/Natural%20Language%20Processing%20images/03-2장IDF로그공식.png)
 
    - 특정 단어의 IDF 값이 혼자 너무 튀는 것을 방지하고 전체적인 데이터 분포를 안정적으로 만들어줌
 
    - 또한 전체 문서에서 등장 횟수가 0인 단어가 있다면 IDF 분모가 0이 되므로 이를 예방하기 위해 **분모에 1을 더해주는 형식**이 주로 활용되고 있음

<br/>

### IDF 모듈 사용 

```ruby
from sklearn.feature_extraction.text import TfidfVectorizer

# 이전 코드에서 사용한 예시 문장
corpus = [doc_0, doc_1, doc_2]

# TfidfVectorizer 객체를 생성
# min_df=1은 최소 1개 이상의 문서에 나타난 단어만 어휘로 간주하라는 의미
vectorizer = TfidfVectorizer(min_df=1)

# corpus 데이터를 학습(fit)하고 TF-IDF 행렬로 변환(transform)
model = vectorizer.fit_transform(corpus)

# 결과를 사람이 보기 쉬운 밀집 행렬(dense matrix) 형태로 출력
print(model.todense())

# 예시 출력
# [[0.         0.         0.42662402 0.18698644 0.18698644 0.        ]
#  [0.22087441 0.18698644 0.         0.         0.         0.18698644]
#  ...
```

<br/>

### TF-IDF 예시 문제 

![System Resources](../../images/Natural%20Language%20Processing%20images/03-2장TF-IDF예시문제.png)

- 풀이과정의 흐름은 아래와 같다 

    - (1) : 토큰화
 
    - (2) : 불용어 제거
 
    - (3) : 어휘집 (Vocabulary) 생성 (등장하는 모든 토큰)
 
    - (4) : TF (Term Frequency) 구하기
 
    - (5) : IDF (Inverse Document Frequency) 구하기 

<br/>

- 풀이과정

    - **토큰화**
 
        - Sent1: the, cat, sat, on, the, mat, .

        - Sent2: the, dog, sat, under, the, tree, .

        - Sent3: the, cat, chased, the, dog, .

    - **불용어 제거**
 
        - Sent1 → cat, sat, on, mat, . (총 5개)

        - Sent2 → dog, sat, under, tree, . (총 5개)

        - Sent3 → cat, chased, dog, . (총 4개)
     
    - **어휘집 (Vocabulary) 생성**
 
        - 등장하는 모든 토큰:['.', 'cat', 'chased', 'dog', 'mat', 'on', 'sat', 'tree', 'under'] (총 9개 단어)
     
    - **TF 구하기**
 
        - **핵심** : 각 sent 별로 단어의 빈도를 표현
     
        - sent 1 을 보면 불용어를 제거한 뒤 토큰은 [cat, sat, on, mat, .] 이고, 해당 토큰에서 'cat' 이라는 단어는 전체 5개의 단어 중 한번만 나온다. 즉 모든 단어가 sent 1에서 한번씩만 나오므로 각각 비율은 동등하게 1/5 = 0.2 이다
     
            - Sent1 (길이=5):

                 - cat=0.2, sat=0.2, on=0.2, mat=0.2, .=0.2

            - Sent2 (길이=5):

                 - dog=0.2, sat=0.2, under=0.2, tree=0.2, .=0.2

            - Sent3 (길이=4):
                
                 - cat=0.25, chased=0.25, dog=0.25, .=0.25
    <br/>

    - **IDF 구하기**
 
        - **핵심** : 전체 문서 수 / 해당 토큰이 나온 문서 수 구하기
     
        - ![System Resources](../../images/Natural%20Language%20Processing%20images/03-2장TF-IDF예시문제IDF풀이과정자료사진.png)

    <br/>
    
    - **최종 TF-IDF 값**
 
        - ![System Resources](../../images/Natural%20Language%20Processing%20images/03-2장TF-IDF예시문제TF-IDF풀이과정자료사진.png)

<br/>

### BM25

(해당 개요는 시험 x )
- TF-IDF는 단어별로 가중치 벡터를 만든 다음 문서와 쿼리 벡터 사이의 유사도를 코사인 유사도 등으로 따로 계산해야 함

    - 문서 길이나 TF(단어 빈도)가 커지면 단순히 값이 계속 커져버림 → 조정이 부족함
 
    - BM25는 TF-IDF를 개선한 발전형 모델

<br/>

(여기서부터 시험)

- 줄여서 BM25라고 널리 불리는 Okapi BM25는 1990년대에 London’s City University에서 만든 
Okapi 정보검색 시스템에 쓰인 쿼리-문서 매칭 기법 알고리즘

    - 오픈소스 검색엔진 중 가장 유명한 ElasticSearch에서도 현재 활용하고 있음

<br/>

- **BM25 공식**

    - ![System Resources](../../images/Natural%20Language%20Processing%20images/03-2장BM25공식자료사진.png)
 
        - qi는 쿼리에서 i번째 토큰 (어절, 형태소, n-gram 등 가능)
     
        - k는 1.2 ~ 2.0 정도로 활용
     
            - 간단한 예시를 위해 b=0라고 가정하면, k=0이라면 TF 값이 변해도 우측 term이 변하지 않음
            - 그러나 k=1일 때는 TF*2 / TF+1 형식이 되어 TF 값이 높아짐에 따라 우측 term 값이 증가함
 
        - IDF : 역문서 빈도
     
            - IDF를 계산할 때 분모가 0이 되는 문제를 해결하기 위해 아래와 같은 공식을 사용함
         
            - ![System Resources](../../images/Natural%20Language%20Processing%20images/03-2장BM25IDF분모0문제해결공식.png)
         
            - 위 공식을 사용하면 and 나 the 같이 흔한 단어를 더 강한 패널티를 줘서 검색 점수에 거의 영향을 주지 않게 한다 -> 희귀한 단어의 가치가 올라감 
         
                - ![System Resources](../../images/Natural%20Language%20Processing%20images/03-2장BM25그래프비교사진.png)
             
                - 해당 그래프를 보면 가장 가파르게 떨어지는게 위 공식을 적용한 선이다 

    - 기존 TF-IDF는 단어와 문서 벡터를 만들고, 그 뒤 코사인 유사도 같은 별도의 연산을 통해 유사도를 계산했지만, BM25는 쿼리와 문서를 직접 넣으면 **곧바로 유사도 점수가 결과**로 나온다
 
<br/>

- **핵심**

    - **TF-IDF 문제점** : 단어가 **많이 반복**되면 점수가 **무한정 커져서 긴 문서가 유리**해짐
      
    - **BM25 개선점** : 단어가 일정 횟수 이상 나오면 더 이상 큰 가치를 주지 않고, **상한선(limit)** 에 수렴

- **BM25 vs TF-IDF 비교 최종정리**

    - IDF 차이

        - TF-IDF: 단어가 많이 나오는 문서일수록 중요도가 천천히 줄어듦.

        - BM25: 흔한 단어의 중요도를 훨씬 더 빠르게 줄여버림. → "the, and" 같은 단어는 거의 무시.

    - TF 차이

        - TF-IDF: 단어가 나오면 나올수록 점수가 끝없이 커짐.

        - BM25: 단어가 일정 횟수 이상 나오면 더 이상 점수를 크게 올리지 않음 (포화, saturation).

    - 문서 길이 보정

        - TF-IDF: 같은 단어 수라도 문서가 길면 TF 값이 낮아져 불리.

        - BM25: 평균 문서 길이를 기준으로 보정 → 긴 문서가 불리하지 않게 조정.

    - 출력 형태

        - TF-IDF: 문서 벡터를 만들어서 코사인 유사도로 비교해야 함.

        - BM25: 각 단어별 점수를 계산해서 바로 합산 → 최종 점수가 바로 "쿼리와 문서의 유사도".




















































































