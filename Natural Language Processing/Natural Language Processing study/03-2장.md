### TF-IDF

- **TF-IDF** : 기본적인 개념은 **TF와 IDF를 곱한 특징점**으로써 단어의 출현 빈도와 특정 문서에서의 희귀도를 **모두 고려**한 NLP에서의 특징점

    - 이렇게 하면 'and': TF는 높지만, IDF가 매우 낮아서 TF-IDF 점수가 낮아. (중요하지 않음)
 
    - '손흥민': 특정 축구 기사에서 TF가 높고, 다른 일반 기사에서는 잘 안 나와 IDF도 높으므로 TF-IDF 점수가 매우 높아
 
    - 전통적인 특징점 계산 방식이지만 **매우 효과적**이며 간단한 작업에서는 최근까지도 여전히 TF-IDF 기반으로 NLP 작업을 수행하고 있다
 
    - **어휘집에 없는 토큰**에 대해서는 TF-IDF 값을 **생성할 수 없음**

<br/>

### TF-IDF 코드 

- 03-1장에서 예시로 사용한 kite의 코드를 그대로 이어서 설명함

```ruby

# intro 문서에서 'china'의 TF를 계산
intro_tf['china'] = intro_counts['china'] / intro_total
# history 문서에서 'china'의 TF를 계산
history_tf['china'] = history_counts['china'] / history_total

# 전체 문서 수를 2로 설정
num_docs = 2
# 각 문서의 IDF 값을 저장할 공간(딕셔너리)을 만
intro_idf = {}
history_idf = {}

# 'and'의 IDF 값을 계산. (전체 문서 수 / 'and'가 포함된 문서 수)
intro_idf['and'] = num_docs / num_docs_containing_and
history_idf['and'] = num_docs / num_docs_containing_and
# 'kite'의 IDF 값을 계산
intro_idf['kite'] = num_docs / num_docs_containing_kite
history_idf['kite'] = num_docs / num_docs_containing_kite
# 'china'의 IDF 값을 계산
intro_idf['china'] = num_docs / num_docs_containing_china
history_idf['china'] = num_docs / num_docs_containing_china



# 위 코드는 기본 세팅을 위한 03-1장 예시 코드
# 아래 코드부터 TF-IDF 본격적인 설명



# TF-IDF 점수를 저장할 공간을 만듦
intro_tfidf = {}

# 'and'의 TF-IDF 점수를 계산 (TF * IDF)
intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']
# 'kite'의 TF-IDF 점수를 계산
intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']
# 'china'의 TF-IDF 점수를 계산
intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']

# history 문서의 TF-IDF 점수를 저장할 공간을 만듦 
history_tfidf = {}

# history 문서에서 각 단어의 TF-IDF 점수를 계산
history_tfidf['and'] = history_tf['and'] * history_idf['and']
history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']
history_tfidf['china'] = history_tf['china'] * history_idf['china']

# intro 문서의 최종 TF-IDF 결과
intro_tfidf
# 결과: {'and': 0.027548209366391185, 'kite': 0.0440771349862259, 'china': 0.0}

# history 문서의 최종 TF-IDF 결과
history_tfidf
# 결과: {'and': 0.0303030303030304, 'kite': 0.0202020202020204, 'china': 0.0202020202020204}
```

- 위 코드를 보면 TF-IDF를 계산할 때 TF의 값과 IDF의 값을 곱하여 나타냈다

    - TF로 빈도의 값과 IDF로 희귀도의 값으로 유연하게 사고하는 것이다
 
    - 이번에는 NLP를 공부하며 예시로 사용한 "Harry" 예시 문장을 TF-IDF를 해보자 

 <br/>

```ruby
# 분석의 대상이 될 세 개의 문장을 정의
doc_0 = "The faster Harry got to the store, the faster Harry, the faster, would get home."
doc_1 = "Harry is hairy and faster than Jill."
doc_2 = "Jill is not as hairy as Harry."


# 문서들의 TF-IDF 벡터를 저장할 리스트를 생성
document_tfidf_vectors = []
# 처리할 문서 목록을 만듦 
documents = [doc_0, doc_1, doc_2]
# 각 문서를 순회하는 반복문
for doc in documents:
    # 한 문서를 대표할 벡터(값을 담는 공간)를 초기화
    vec = copy.copy(zero_vector)

    # 문서를 소문자로 바꾸고 단어(토큰) 단위로 쪼갬
    tokens = tokenizer.tokenize(doc.lower())

    # 문서에 있는 각 단어의 개수를 샘 
    token_counts = Counter(tokens)
    
    # (단어, 단어 개수) 쌍으로 반복문을 실행
    for key, value in token_counts.items():

        # 이 단어가 몇 개 문서에 나왔는지 세는 변수를 초기화
        docs_containing_key = 0

        # 전체 문서를 다시 확인하며
        for _doc in documents:
            # 문서 안에 해당 단어가 있으면 카운트를 1 올
            if key in _doc:
                docs_containing_key += 1
        
        # 해당 단어의 TF를 계산합니다. (단어 등장 횟수 / 전체 단어 수)
        tf = value / len(tokens)
        
        # 단어가 한 개 이상의 문서에 포함되었다면
        if docs_containing_key:
            # IDF를 계산
            idf = len(documents) / docs_containing_key
        # 어떤 문서에도 포함되지 않았다면 (이론상 발생하기 어려움)
        else:
            idf = 0
        
        # 최종 TF-IDF 점수를 계산하여 벡터에 단어와 함께 저장
        vec[key] = tf * idf
    # 완성된 한 문서의 TF-IDF 벡터를 전체 리스트에 추가함함
    document_tfidf_vectors.append(vec)


# 질문 문장을 정의
query = "How long does it take to get to the store?"
# 질문 문장을 대표할 벡터를 초기화
query_vec = copy.copy(zero_vector)

# 질문을 소문자로 바꾸고 단어 단위로 쪼갬
tokens = tokenizer.tokenize(query.lower())
# 질문에 있는 각 단어의 개수를 
token_counts = Counter(tokens)

# 질문에 있는 (단어, 단어 개수) 쌍으로 반복문을 실행
for key, value in token_counts.items():
    # 이 단어가 몇 개 문서에 나왔는지 세는 변수를 초기화
    docs_containing_key = 0

    # 전체 문서들을 확인하며
    for _doc in documents:
        # 문서 안에 해당 단어가 있으면 카운트를 1 올림
        if key in _doc.lower():
            docs_containing_key += 1
    
    # 만약 단어가 어떤 문서에도 없었다면, IDF를 계산할 수 없으므로 건너뜀
    # zero_vector에서 가져왔으므로 건들지 않으면 자동으로 0
    if docs_containing_key == 0:
        continue

    # 질문 문장에서 해당 단어의 TF를 계산
    tf = value / len(tokens)

    # IDF를 계산
    idf = len(documents) / docs_containing_key

    # 최종 TF-IDF 점수를 계산하여 질문 벡터에 저장
    query_vec[key] = tf * idf

# 질문 벡터와 첫 번째 문서 벡터의 코사인 유사도를 계산하고 출력
print(cosine_sim(query_vec, document_tfidf_vectors[0]))
# 결과: 0.5235048549676834
# 질문 벡터와 두 번째 문서 벡터의 코사인 유사도를 계산하고 출력
print(cosine_sim(query_vec, document_tfidf_vectors[1]))
# 결과: 0.0
# 질문 벡터와 세 번째 문서 벡터의 코사인 유사도를 계산하고 출력
print(cosine_sim(query_vec, document_tfidf_vectors[2]))
# 결과: 0.0
```
