### parameter & hyperparameter

- **매개변수(Parameter)**
  
    - 모델이 학습을 통해 직접 조정하는 값 → 예: 가중치 w, 편향 b

- **하이퍼파라미터(Hyperparameter)**

    - **사람이 사전에 정해주는** 값, 학습 과정에서 자동으로 바뀌지 않음

 <br/>
 
- 퍼셉트론의 매개변수 : w(가중치) 와 b(편향)

- 퍼셉트론의 하이퍼파라미터 : E (에폭 수)

- 아래는 실제 업데이트 과정을 보여준다

    - w := w + err × x 에서 퍼셉트론의 정답은 y = 0 또는 1 이므로 err은 −1, 0, +1 중 하나 

    - n개의 x 데이터를 가장 잘 설명할 수 있는 가중치를 계속 찾는 과정임 

<br/>

![System Resources](../../images/Artificial%20Neural%20Network%20images/파라미터코드예시.png)

<br/>

### Geometric Intuition (기하학적 직관)

- **내적 w ⋅ x**는 두 벡터가 **얼마나 같은 방향인지**(정렬 정도)를 알려준다

    - w ⋅ x > 0 이면 두 벡터가 비슷한 방향(각도 < 90°)
 
    - w ⋅ x = 0 이면 두 벡터는 직각(90°)
 
    - w ⋅ x < 0 이면 두 벡터는 반대 방향(각도 > 90°)
 
    - wTx=∥w∥∥x∥cos(θ)를 보면 이해가 쉽다 

<br/>

- w나 x를 몇 배 키우거나 줄여도 **분류 결과는 같음**

    - 분류는 **각도(방향)로 결정**되며, 스케일(크기)을 키워도 결과는 같지만 학습 속도에는 영향이 있음

<br/>

- 결정경계 거리 공식
    
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/결정경계거리공식.png)
 
    - ∣w⋅x+b∣ 값 자체가 '확신 정도'를 나타냄

    - 경계에서 멀수록 → 분류 확신 ↑
 
    - **퍼셉트론이 작동하는 이유**: **틀린 점을 반대편**으로 밀어서 올바른 쪽에 위치시키기 때문

<br/>

### 예시 문제 

- w = [2,1], b = 3 일 때 다음을 답하시오

    - x1 = [2,0]
 
        - w⋅x1 + b = 4 + (-3) = 1 → 양수 → 클래스 +1
   
    - x2​=[0,3]
 
        - w·x₂ + b = 3 + (-3) = 0 -> 경계 위 (보통은 -1로 )
    
    - x3​=[−1,0]
 
        - w·x₃ + b = -2 + (-3) = -5 < 0 -> 음수 -> 클래스 -1  

<br/>

### 편향과 결정 경계의 관계 

- 편향이 없을 때 결정 경계는 w ⋅ x = 0

    - 경계를 **자유롭게 움직일 수 없고 유연성이 떨어짐**
 
- 편향이 있으면 경계 방정식은 w ⋅ x + b = 0

    - 이제 경계는 원래 직선과 평행한 채 어디든 이동 가능
 
    - 편향 b가 원점으로부터의 거리(수직 이동)를 결정
 
        - b > 0 : 양수 클래스가 되기 쉬워짐
     
        - b < 0 : 음수 클래스가 되기 쉬워짐 

<br/>

### 특수한 경우 

- 여기서 x1과 x2를 그냥 x와 y로 생각하면 쉽다
  
- Case 1 : 결정 경계는 x1 = 0, 즉 y축

    -  y축 오른쪽(x1 > 0)은 양성(positive class)


- Case 2 : x1​ + x2 ​= 0, 즉 x2​=−x1

    - 이것도 x1과 x2를 x, y 로 생각하면 y = -x 직선이다
   
    - 즉, x1 + x2 > 0 은, -x 직선보다 위에 존재하면 +1, 아래면 -1과 동치이다 

 
- Case 3 : w를 2배한 경우 -> (2w)·x + 2b = 2(w·x + b) = 0

    - 가중치 크기를 키우면 **직선(경계) 위치는 똑같고**, 단지 점이 선에서 얼마나 멀리 있는지 **확신 정도**만 바뀐다

<br/>

### Convergence Bounds (수학적 상한선)

- **Convergence Bounds** : 학습 전에 퍼셉트론이 학습 과정에서 **최대 몇 번**까지 틀릴 수 있는지를 보장하는 수학적 상한선

- 퍼셉트론은 얼마나 빨리 학습하는가?

- 최대 오류 횟수는 (R/γ)^2 이하 ((R/γ)제곱 이하) (이후 증명)

    - R : 원점(0,0)으로부터 각 데이터 점까지의 거리 중 **최댓값**
 
    - γ (마진) : 결정 경계와 가장 가까운 점까지의 거리 (**최소값**)
 
        - 마진(margin)이 크면 (클래스가 멀리 떨어져 있으면) -> 빠르게 학습한다
     
        - 마진(margin)이 작으면 (클래스가 거의 겹쳐 있으면) -> 학습이 느리다
     
        - 데이터가 너무 크면 -> 학습 속도가 느려진다
      
<br/>

### Convergence Bounds 공식 및 증명 

- 아래는 가중치가 최소 어느정도 증가하고 최대 어느정도까지 증가하는지를 보여준다

    - 이것을 통해 **퍼셉트론**은 무한히 학습하지 않고 **수렴**한다는 것을 보여준다 

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/수학적상한선.png)

<br/>

- **lower bound (하한) 증명**

    - 처음에는 가중치 w0가 0이고, w*가 이상적인 분리기(정답 가중치)라고 하자
 
    - 만약 샘플을 잘못 분류하면, 가중치를 yixi만큼 업데이트 한다
 
        - 즉, w(t+1) = w(t) + yixi
     
        - t: 업데이트 횟수(시간 step)
        
        - i: 현재 틀린 샘플의 인덱스(데이터 번호) 
     
    - 여기서 양 변에 w*를 내적해보자 
 
        -  w(t+1)​⋅w∗ = (wt​ + yi​xi​)​⋅w∗ = w(t)​⋅w + yi(xi​⋅w∗)
     
        - 즉, 새로운 가중치 w(t+1)과 w∗ 의 내적은 **이전 값 + yi(xi​⋅w∗)** 이다
     
        - 따라서 t번째에서 t+1번째로 가중치를 업데이트 할 때, 최적의 방향으로 yi(xi​⋅w∗) 만큼 밀어준다
 
    - w∗는 최적 분리기라 항상 샘플을 올바르게 분류하므로, 내적 값이 **최소 𝛼**( 𝛼는 마진 ) 이상이다
 
        - 즉 최적의 방향으로 yi(xi​⋅w∗) 밀어주는데, 이 값이 최소 마진보다는 크다는 것이다
     
        - 따라서 yi(xi​⋅w∗) ≥ α
 
    - 따라서 매번 실수를 할 때마다 w(t+1)​⋅w∗ ≥ wt​⋅w∗ + α 이다
 
        - 만약 1번 실수 한다면 : w1​⋅w∗ ≥ α
     
        - 2번 실수 한다면 : w2​⋅w∗ ≥ w1​⋅w∗ + α ≥ 2α 
     
        - 3번 실수 한다면 : w3​⋅w∗ ≥ w2​⋅w∗ + α ≥ 3α
     
        - 즉, i번 실수한다면 : **wi​⋅w∗ ≥ iα** 이고 **선형적으로 증가**한다는 결론이 나온다
     
            -  위 이미지에서 마진 공식 α = min yj​(xj​⋅w∗)를 보면 모든 샘플 xj에 대해 가장 작은 마진 값을 **고정**한 것이므로 상수 취급이 된다. 따라서 선형적으로 증가한다
         
            -  학습이 i번 업데이트되면, 현재 가중치 wᵢ는 최적 해 w*와 **최소한 iα만큼 가까워진다**
         

    - 이것을 길이로 일반화 해주면 (Cauchy–Schwarz 부등식 사용)
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/lower_bound최종공식.png)
     
    - 정리하면 w가 정답 방향 𝑤∗ 쪽으로 선형적으로 증가한다는 것이고
 
        - 길이로 표현해서 제곱을 나타낸 공식을 보면 **실제 가중치 벡터 크지는 이차 함수** 꼴로 증가하는 걸 볼 수 있다
     
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/bound그래프.png)
 

<br/>

- **upper bound (상한) 증명**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/upper_bound증명.png)
 
    - 5번 과정에서 β 는 맨 위 이미지 공식에서처럼, 최대 거리를 고정한 것
 
    - 6번 과정을 자세히 보면
 
        - 첫 번쨔 업데이트 : ||w1||^2 <= ||w0||^2 + 𝛽 = 0 + 𝛽 = 𝛽

        - 두 번째 업데이트 : ||w2||^2 <= ||w1||^2 + 𝛽 <= 𝛽 + 𝛽 = 2𝛽

        - 세 번째 업데이트: ||w3||^2 <= ||w2||^2 + 𝛽 <= 2𝛽 + 𝛽 = 3𝛽
 
 <br/>

- **최대 오류 횟수는 (R/γ)^2 이하 증명** 

    - The Squeeze (압축 논법)로 증명 : 지금까지 얻은 상·하한식을 모아 최종 결론(최대 오류 횟수)을 구하겠다
 
    - lower bound 증명 : **wᵢ·w∗ ≥ iα**
 
        - 학습이 i번 업데이트되면, 현재 가중치 wᵢ는 최적 해 w*와 최소한 iα만큼 가까워진다

    - upper bound 증명 : **‖wᵢ‖² ≤ iβ**
 
        - wᵢ의 크기 제곱은 iβ 이하이다
     
    - Cauchy-Schwarz 부등식 사용 : (wᵢ·w∗)² ≤ ‖wᵢ‖² · ‖w∗‖²
 
        - 이후 대입해주면 (iα)² ≤ iβ · ‖w*‖²
     
        - = i²α² ≤ iβ · ‖w*‖²
     
            - 제곱 전개

        - i ≤ (β · ‖w*‖²) / α²
     
            - i 약분 (i는 횟수이므로 0이 아님), α²으로 나눔
         
        - 여기서 최적 해 w*의 크기를 1로 정규화
     
            - 방향만 중요하고 크기는 임의로 조정 가능
         
        - 즉, **i ≤ β / α²** : 업데이트 횟수 **i는 β / α² 이하**이다
     
            - 따라서 최대 오류 횟수는 **최대 오류 횟수는 β / α²** 이다
         
                - 데이터가 잘 구분될수록 **α 큼** → **오류가 줄어든다**

                - 데이터가 크고 복잡할수록 **β 큼** → **오류가 늘어난다** 
     
<br/>

- **최종정리**

    - **lower bound** : w와 w*의 내적은 **실수가 발생**할 때마다 **margin 만큼 선형적**으로 증가한다
 
        - 이는 w가 점점 올바른 해 w*와 정렬되어 감  

    - **upper bound** : 상한: w의 제곱 노름은 βi 이하이다
    
        - β는 데이터 샘플들 중 가장 큰 입력 벡터의 길이의 제곱값
     
        - w가 **무한정 커지지 않고 일정 속도**로만 커짐
             
    -  **업데이트 횟수 i는 β / α² 이하**
 
        - 데이터가 잘 구분될수록 **α 큼** → **오류가 줄어든다**

        - 데이터가 크고 복잡할수록 **β 큼** → **오류가 늘어난다**

<br/>

- **퍼셉트론/머신러닝 구현 시 설전 팁**

    - 데이터를 정규화/스케일링해야 학습이 안정적이고 빠르다.

    - feature 크기를 맞춰줘야 특정 값이 과도하게 영향 주지 않는다.

    - epoch 제한을 두고, 끝까지 안 되면 데이터 자체가 분리 불가능하다고 판단한다

<br/>

- **퍼셉트론 수렴 정리**

    - 데이터가 **선형적으로 분리 가능**하다면, 퍼셉트론은 **반드시 수렴**한다
 
        - 두 클래스가 직선(또는 초평면)으로 나눌 수 있다면, 퍼셉트론은 무한히 틀리지 않고 언젠가 정답에 도달
     
        - 만약 선형적이지 않다면 **수렴 보장은 없다**
     
            - 실전 팁으로 약 1000 epoch 동안 수렴하지 않으면, 선형 분리가 불가능할 가능성이 높음
         
            - 이 경우 다른 알고리즘(ex: 다층 신경망)을 사용해야함 

<br/>

### 퍼셉트론의 한계점 

- **직선(선형) 경계**만 분리 가능

    - **XOR 문제를 풀 수 없다**
 
    - 곡선이나 복잡한 경계가 있는 데이터는 분류할 수 없다

<br/>

- **이진 분류**만 가능하다

    - 확장 방법이 있긴 하지만 성능이 제한적이다  

<br/>

- **확률**적인 출력을 **제공하지 않는다**

    - 퍼셉트론은 예/아니오 만 말하고 몇 % 확신하는지는 알려주지 못한다
 
    - 신뢰도 측정이 없다

<br/>

- **이상치(outlier)에 민감**하다

    - 하나의 잘못된 점이 경계를 크게 바꿀 수 있다 

<br/>

- **분리 불가능**한 데이터에 대해서는 **보장이 없다**

    - 퍼셉트론이 무한히 업데이트만 반복
 
    - 최선의 근사 해도 제공하지 않는다

<br/>

### 퍼셉트론 & XOR 문제 

- 아래 이미지는 퍼셉트론의 입력과 출력을 함수로 나타낸 것이다

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/XOR함수.png)
 
    - 이것을 선형적으로 분리하려고 노력해도 불가능
      
    - Minsky와 Papert가 1969년에 단일 퍼셉트론이 XOR을 풀 수 없음을 수학적으로 증명
 
        - 해결책 : **다층 신경망이 필요**하다
     
        - XOR 문제는 단일 퍼셉트론이 아니라, **여러 층(은닉층)을 가진 신경망으로 풀 수 있다**
     
        - 이로 인해 **다층 퍼셉트론(MLP)** 과 **현대 딥러닝**이 **발전**

<br/>

### 다층 신경망(Neural Networks)

![System Resources](../../images/Artificial%20Neural%20Network%20images/퍼셉트론구조표현.png)
![System Resources](../../images/Artificial%20Neural%20Network%20images/다층신경망.png)

- 위와 같이 한계점으로 인해 단순한 퍼셉트론에서 복잡한 **다층 신경망** 발전

    - 핵심 기능
 
    - **Hidden layers (은닉층)** – 중간 표현을 담당
 
        - 입력과 출력 사이에 층을 더 두어, 단일 퍼셉트론이 못하는 복잡한 패턴을 학습할 수 있게 한다
     
        - **특징들의 조합**을 학습할 수 있다
     
        - **충분한 뉴런**이 있다면 **어떤 연속 함수라도 근사**할 수 있다
     
        - XOR 문제를 포함해 **훨씬 더 많은 문제를 해결**한다
     
    - **Nonlinear activation (비선형 활성화)** : 단순 계단 함수만 쓰지 않는다
 
        - ReLU, sigmoid 같은 비선형 함수를 사용
     
    - **Backpropagation (역전파)** : 여러 층을 학습하는 방법
 
        - 오차를 뒤로 전파해 가중치를 조정하는 알고리즘 

<br/>

- **다층 신경망(Neural Networks) XOR 문제 풀이 과정**

![System Resources](../../images/Artificial%20Neural%20Network%20images/다층신경망xOR그림.png)

- 위 그림처럼 은닉층 2개를 사용한다 

    - h1 = NAND(x₁, x₂) : AND의 부정
 
    - h₂ = OR(x₁, x₂) : 하나라도 1 -> 1
 
    - 이 두개를 AND 엮어주면 다음과 같이 나온다
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/다층신경망XOR표.png)
     
        - NAND와 OR 둘 다 1 인 값을 묶어주면 XOR과 동일


































