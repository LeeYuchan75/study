### parameter & hyperparameter

- **매개변수(Parameter)**
  
    - 모델이 학습을 통해 직접 조정하는 값 → 예: 가중치 w, 편향 b

- **하이퍼파라미터(Hyperparameter)**

    - **사람이 사전에 정해주는** 값, 학습 과정에서 자동으로 바뀌지 않음

 <br/>
 
- 퍼셉트론의 매개변수 : w(가중치)**와 b(편향)

- 퍼셉트론의 하이퍼파라미터 : E (에폭 수)

- 아래는 실제 업데이트 과정을 보여준다

    - w := w + err × x 에서 퍼셉트론의 정답은 y = 0 또는 1 이므로 err은 −1, 0, +1 중 하나 

    - n개의 x 데이터를 가장 잘 설명할 수 있는 가중치를 계속 찾는 과정임 

<br/>

![System Resources](../../images/Artificial%20Neural%20Network%20images/파라미터코드예시.png)

<br/>

### Geometric Intuition (기하학적 직관)

- **내적 w ⋅ x**는 두 벡터가 **얼마나 같은 방향인지**(정렬 정도)를 알려준다

    - w ⋅ x > 0 이면 두 벡터가 비슷한 방향(각도 < 90°)
 
    - w ⋅ x = 0 이면 두 벡터는 직각(90°)
 
    - w ⋅ x < 0 이면 두 벡터는 반대 방향(각도 > 90°)
 
    - wTx=∥w∥∥x∥cos(θ)를 보면 이해가 쉽다 

<br/>

- w나 x를 몇 배 키우거나 줄여도 **분류 결과는 같음**

    - 분류는 **각도(방향)로 결정**되며, 스케일(크기)을 키워도 결과는 같지만 학습 속도에는 영향이 있음

<br/>

- 결정경계 거리 공식
    
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/결정경계거리공식.png)
 
    - ∣w⋅x+b∣ 값 자체가 '확신 정도'를 나타냄

    - 경계에서 멀수록 → 분류 확신 ↑
 
    - **퍼셉트론이 작동하는 이유**: **틀린 점을 반대편**으로 밀어서 올바른 쪽에 위치시키기 때문

<br/>

### 예시 문제 

- w = [2,1], b = 3 일 때 다음을 답하시오

    - x1 = [2,0]
 
        - w⋅x1 + b = 4 + (-3) = 1 → 양수 → 클래스 +1
   
    - x2​=[0,3]
 
        - w·x₂ + b = 3 + (-3) = 0 -> 경계 위 (보통은 -1로 )
    
    - x3​=[−1,0]
 
        - w·x₃ + b = -2 + (-3) = -5 < 0 -> 음수 -> 클래스 -1  

<br/>

### 편향과 결정 경계의 관계 

- 편향이 없을 때 결정 경계는 w ⋅ x = 0

    - 경계를 **자유롭게 움직일 수 없고 유연성이 떨어짐**
 
- 편향이 있으면 경계 방정식은 w ⋅ x + b = 0

    - 이제 경계는 원래 직선과 평행한 채 어디든 이동 가능
 
    - 편향 b가 원점으로부터의 거리(수직 이동)를 결정
 
        - b > 0 : 양수 클래스가 되기 쉬워짐
     
        - b < 0 : 음수 클래스가 되기 쉬워짐 

<br/>

### 특수한 경우 

- 여기서 x1과 x2를 그냥 x와 y로 생각하면 쉽다
  
- Case 1 : 결정 경계는 x1 = 0, 즉 y축

    -  y축 오른쪽(x1 > 0)은 양성(positive class)


- Case 2 : x1​ + x2 ​= 0, 즉 x2​=−x1

    - 이것도 x1과 x2를 x, y 로 생각하면 y = -x 직선이다
   
    - 즉, x1 + x2 > 0 은, -x 직선보다 위에 존재하면 +1, 아래면 -1과 동치이다 

 
- Case 3 : w를 2배한 경우 -> (2w)·x + 2b = 2(w·x + b) = 0

    - 가중치 크기를 키우면 **직선(경계) 위치는 똑같고**, 단지 점이 선에서 얼마나 멀리 있는지 **확신 정도**만 바뀐다

<br/>

### Convergence Bounds (수학적 상한선)

- **Convergence Bounds** : 학습 전에 퍼셉트론이 학습 과정에서 **최대 몇 번**까지 틀릴 수 있는지를 보장하는 수학적 상한선

- 퍼셉트론은 얼마나 빨리 학습하는가?

- 최대 오류 횟수는 (R/γ)^2 이하 ((R/γ)제곱 이하) (이후 증명)

    - R : 원점(0,0)으로부터 각 데이터 점까지의 거리 중 **최댓값**
 
    - γ (마진) : 결정 경계와 가장 가까운 점까지의 거리 (**최소값**)
 
        - 마진(margin)이 크면 (클래스가 멀리 떨어져 있으면) -> 빠르게 학습한다
     
        - 마진(margin)이 작으면 (클래스가 거의 겹쳐 있으면) -> 학습이 느리다
     
        - 데이터가 너무 크면 -> 학습 속도가 느려진다
      
<br/>

### Convergence Bounds 공식 및 증명 

- 아래는 가중치가 최소 어느정도 증가하고 최대 어느정도까지 증가하는지를 보여준다

    - 이것을 통해 **퍼셉트론**은 무한히 학습하지 않고 **수렴**한다는 것을 보여준다 

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/수학적상한선.png)

<br/>

- **lower bound (하한) 증명**

    - 처음에는 가중치 w0가 0이고, w*가 이상적인 분리기(정답 가중치)라고 하자
 
    - 만약 샘플을 잘못 분류하면, 가중치를 yixi만큼 업데이트 한다
 
        - 즉, w(t+1) = w(t) + yixi
     
    - 여기서 양 변에 w*를 내적해보자 
 
        -  w(t+1)​⋅w∗ = (wt​ + yi​xi​)​⋅w∗ = w(t)​⋅w + yi(xi​⋅w∗)
     
        - 즉, 새로운 가중치 w(t+1)과 w∗ 의 내적은 **이전 값 + yi(xi​⋅w∗)** 이다
     
        - 따라서 t번째에서 t+1번째로 가중치를 업데이트 할 때, 최적의 방향으로 yi(xi​⋅w∗) 만큼 밀어준다
 
    - w∗는 최적 분리기라 항상 샘플을 올바르게 분류하므로, 내적 값이 **최소 𝛼**( 𝛼는 마진 ) 이상이다
 
        - 즉 최적의 방향으로 yi(xi​⋅w∗) 밀어주는데, 이 값이 최소 마진보다는 크다는 것이다
     
        - 따라서 yi(xi​⋅w∗) ≥ α
 
    - 따라서 매번 실수를 할 때마다 w(t+1)​⋅w∗ ≥ wt​⋅w∗ + α 이다
 
        - 만약 1번 실수 한다면 : w1​⋅w∗ ≥ α
     
        - 2번 실수 한다면 : w2​⋅w∗ ≥ w1​⋅w∗ + α ≥ 2α 
     
        - 3번 실수 한다면 : w3​⋅w∗ ≥ w2​⋅w∗ + α ≥ 3α
     
        - 즉, i번 실수한다면 : **wi​⋅w∗ ≥ iα** 이고 **선형적으로 증가**한다는 결론이 나온다
     
            -  위 이미지에서 마진 공식 α = min yj​(xj​⋅w∗)를 보면 모든 샘플 xj에 대해 가장 작은 마진 값을 **고정**한 것이므로 상수 취급이 된다. 따라서 선형적으로 증가한다
         
            -  학습이 i번 업데이트되면, 현재 가중치 wᵢ는 최적 해 w*와 **최소한 iα만큼 가까워진다**
         

    - 이것을 길이로 일반화 해주면 (Cauchy–Schwarz 부등식 사용)
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/lower_bound최종공식.png)
     
    - 정리하면 w가 정답 방향 𝑤∗ 쪽으로 선형적으로 증가한다는 것이고
 
        - 길이로 표현해서 제곱을 나타낸 공식을 보면 **실제 가중치 벡터 크지는 이차 함수** 꼴로 증가하는 걸 볼 수 있다
     
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/bound그래프.png)
 

<br/>

- **upper bound (상한) 증명**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/upper_bound증명.png)
 
    - 5번 과정에서 β 는 맨 위 이미지 공식에서처럼, 최대 거리를 고정한 것
 
    - 6번 과정을 자세히 보면
 
        - 첫 번쨔 업데이트 : ||w1||^2 <= ||w0||^2 + 𝛽 = 0 + 𝛽 = 𝛽

        - 두 번째 업데이트 : ||w2||^2 <= ||w1||^2 + 𝛽 <= 𝛽 + 𝛽 = 2𝛽

        - 세 번째 업데이트: ||w3||^2 <= ||w2||^2 + 𝛽 <= 2𝛽 + 𝛽 = 3𝛽
 
 <br/>

- **최대 오류 횟수는 (R/γ)^2 이하 증명** 

    - The Squeeze (압축 논법)로 증명 : 지금까지 얻은 상·하한식을 모아 최종 결론(최대 오류 횟수)을 구하겠다
 
    - lower bound 증명 : **wᵢ·w∗ ≥ iα**
 
        - 학습이 i번 업데이트되면, 현재 가중치 wᵢ는 최적 해 w*와 최소한 iα만큼 가까워진다

    - upper bound 증명 : **‖wᵢ‖² ≤ iβ**
 
        - wᵢ의 크기 제곱은 iβ 이하이다
     
    - Cauchy-Schwarz 부등식 사용 : (wᵢ·w∗)² ≤ ‖wᵢ‖² · ‖w∗‖²
 
        - 이후 대입해주면 (iα)² ≤ iβ · ‖w*‖²
     
        - = i²α² ≤ iβ · ‖w*‖²
     
            - 제곱 전개

        - i ≤ (β · ‖w*‖²) / α²
     
            - i 약분 (i는 횟수이므로 0이 아님), α²으로 나눔
         
        - 여기서 최적 해 w*의 크기를 1로 정규화
     
            - 방향만 중요하고 크기는 임의로 조정 가능
         
        - 즉, **i ≤ β / α²** : 업데이트 횟수 **i는 β / α² 이하**이다
     
            - 따라서 최대 오류 횟수는 **최대 오류 횟수는 β / α²** 이다
         
                - 데이터가 잘 구분될수록 **α 큼** → **오류가 줄어든다**

                - 데이터가 크고 복잡할수록 **β 큼** → **오류가 늘어난다** 
     
<br/>

- **최종정리**

    - **lower bound** : w와 w*의 내적은 **실수가 발생**할 때마다 **margin 만큼 선형적**으로 증가한다
 
        - 이는 w가 점점 올바른 해 w*와 정렬되어 감  

    - **upper bound** : 상한: w의 제곱 노름은 βi 이하이다
    
        - β는 데이터 샘플들 중 가장 큰 입력 벡터의 길이의 제곱값
     
        - w가 **무한정 커지지 않고 일정 속도**로만 커짐
             
    -  **업데이트 횟수 i는 β / α² 이하**
 
        - 데이터가 잘 구분될수록 **α 큼** → **오류가 줄어든다**

        - 데이터가 크고 복잡할수록 **β 큼** → **오류가 늘어난다**

<br/>

- **퍼셉트론/머신러닝 구현 시 설전 팁**

    - 데이터를 정규화/스케일링해야 학습이 안정적이고 빠르다.

    - feature 크기를 맞춰줘야 특정 값이 과도하게 영향 주지 않는다.

    - epoch 제한을 두고, 끝까지 안 되면 데이터 자체가 분리 불가능하다고 판단한다



























































