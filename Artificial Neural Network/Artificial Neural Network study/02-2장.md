### parameter & hyperparameter

- **매개변수(Parameter)**
  
    - 모델이 학습을 통해 직접 조정하는 값 → 예: 가중치 w, 편향 b

- **하이퍼파라미터(Hyperparameter)**

    - **사람이 사전에 정해주는** 값, 학습 과정에서 자동으로 바뀌지 않음

 <br/>
 
- 퍼셉트론의 매개변수 : w(가중치)**와 b(편향)

- 퍼셉트론의 하이퍼파라미터 : E (에폭 수)

- 아래는 실제 업데이트 과정을 보여준다

    - w := w + err × x 에서 퍼셉트론의 정답은 y = 0 또는 1 이므로 err은 −1, 0, +1 중 하나 

    - n개의 x 데이터를 가장 잘 설명할 수 있는 가중치를 계속 찾는 과정임 

<br/>

![System Resources](../../images/Artificial%20Neural%20Network%20images/파라미터코드예시.png)

<br/>

### Geometric Intuition (기하학적 직관)

- **내적 w ⋅ x**는 두 벡터가 **얼마나 같은 방향인지**(정렬 정도)를 알려준다

    - w ⋅ x > 0 이면 두 벡터가 비슷한 방향(각도 < 90°)
 
    - w ⋅ x = 0 이면 두 벡터는 직각(90°)
 
    - w ⋅ x < 0 이면 두 벡터는 반대 방향(각도 > 90°)
 
    - wTx=∥w∥∥x∥cos(θ)를 보면 이해가 쉽다 

<br/>

- w나 x를 몇 배 키우거나 줄여도 **분류 결과는 같음**

    - 분류는 **각도(방향)로 결정**되며, 스케일(크기)을 키워도 결과는 같지만 학습 속도에는 영향이 있음

<br/>

- 결정경계 거리 공식
    
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/결정경계거리공식.png)
 
    - ∣w⋅x+b∣ 값 자체가 '확신 정도'를 나타냄

    - 경계에서 멀수록 → 분류 확신 ↑
 
    - **퍼셉트론이 작동하는 이유**: **틀린 점을 반대편**으로 밀어서 올바른 쪽에 위치시키기 때문

<br/>

### 예시 문제 

- w = [2,1], b = 3 일 때 다음을 답하시오

    - x1 = [2,0]
 
        - w⋅x1 + b = 4 + (-3) = 1 → 양수 → 클래스 +1
   
    - x2​=[0,3]
 
        - w·x₂ + b = 3 + (-3) = 0 -> 경계 위 (보통은 -1로 )
    
    - x3​=[−1,0]
 
        - w·x₃ + b = -2 + (-3) = -5 < 0 -> 음수 -> 클래스 -1  

<br/>

### 편향과 결정 경계의 관계 

- 편향이 없을 때 결정 경계는 w ⋅ x = 0

    - 경계를 **자유롭게 움직일 수 없고 유연성이 떨어짐**
 
- 편향이 있으면 경계 방정식은 w ⋅ x + b = 0

    - 이제 경계는 원래 직선과 평행한 채 어디든 이동 가능
 
    - 편향 b가 원점으로부터의 거리(수직 이동)를 결정
 
        - b > 0 : 양수 클래스가 되기 쉬워짐
     
        - b < 0 : 음수 클래스가 되기 쉬워짐 

<br/>

### 특수한 경우 

- 여기서 x1과 x2를 그냥 x와 y로 생각하면 쉽다
  
- Case 1 : 결정 경계는 x1 = 0, 즉 y축

    -  y축 오른쪽(x1 > 0)은 양성(positive class)


- Case 2 : x1​ + x2 ​= 0, 즉 x2​=−x1

    - 이것도 x1과 x2를 x, y 로 생각하면 y = -x 직선이다
   
    - 즉, x1 + x2 > 0 은, -x 직선보다 위에 존재하면 +1, 아래면 -1과 동치이다 

 
- Case 3 : w를 2배한 경우 -> (2w)·x + 2b = 2(w·x + b) = 0

    - 가중치 크기를 키우면 **직선(경계) 위치는 똑같고**, 단지 점이 선에서 얼마나 멀리 있는지 **확신 정도**만 바뀐다

<br/>

### Convergence Bounds (수학적 상한선)

- **Convergence Bounds** : 학습 전에 퍼셉트론이 학습 과정에서 **최대 몇 번**까지 틀릴 수 있는지를 보장하는 수학적 상한선

- 퍼셉트론은 얼마나 빨리 학습하는가?

- 최대 오류 횟수는 (R/γ)^2 이하 ((R/γ)제곱 이하)

    - R : 원점(0,0)으로부터 각 데이터 점까지의 거리 중 **최댓값**
 
    - γ (마진) : 결정 경계와 가장 가까운 점까지의 거리 (**최소값**)
 
        - 마진(margin)이 크면 (클래스가 멀리 떨어져 있으면) -> 빠르게 학습한다
     
        - 마진(margin)이 작으면 (클래스가 거의 겹쳐 있으면) -> 학습이 느리다
     
        - 데이터가 너무 크면 -> 학습 속도가 느려진다
      
<br/>

### p.66부터 










































































