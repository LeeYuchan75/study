### parameter & hyperparameter

- **매개변수(Parameter)**
  
    - 모델이 학습을 통해 직접 조정하는 값 → 예: 가중치 w, 편향 b

- **하이퍼파라미터(Hyperparameter)**

    - **사람이 사전에 정해주는** 값, 학습 과정에서 자동으로 바뀌지 않음

 <br/>
 
- 퍼셉트론의 매개변수 : w(가중치)**와 b(편향)

- 퍼셉트론의 하이퍼파라미터 : E (에폭 수)

- 아래는 실제 업데이트 과정을 보여준다

    - w := w + err × x 에서 퍼셉트론의 정답은 y = 0 또는 1 이므로 err은 −1, 0, +1 중 하나 

    - n개의 x 데이터를 가장 잘 설명할 수 있는 가중치를 계속 찾는 과정임 

<br/>

![System Resources](../../images/Artificial%20Neural%20Network%20images/파라미터코드예시.png)

<br/>

### Geometric Intuition (기하학적 직관)

- **내적 w ⋅ x**는 두 벡터가 **얼마나 같은 방향인지**(정렬 정도)를 알려준다

    - w ⋅ x > 0 이면 두 벡터가 비슷한 방향(각도 < 90°)
 
    - w ⋅ x = 0 이면 두 벡터는 직각(90°)
 
    - w ⋅ x < 0 이면 두 벡터는 반대 방향(각도 > 90°)
 
    - wTx=∥w∥∥x∥cos(θ)를 보면 이해가 쉽다 

<br/>

- w나 x를 몇 배 키우거나 줄여도 **분류 결과는 같음**

    - 분류는 **각도(방향)로 결정**되며, 스케일(크기)을 키워도 결과는 같지만 학습 속도에는 영향이 있음

<br/>

- 결정경계 거리 공식
    
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/결정경계거리공식.png)
 
    - ∣w⋅x+b∣ 값 자체가 '확신 정도'를 나타냄

    - 경계에서 멀수록 → 분류 확신 ↑
 
    - **퍼셉트론이 작동하는 이유**: **틀린 점을 반대편**으로 밀어서 올바른 쪽에 위치시키기 때문

<br/>

### 예시 문제 

- w = [2,1], b = 3 일 때 다음을 답하시오

    - x1 = [2,0]
 
        - w⋅x1 + b = 4 + (-3) = 1 → 양수 → 클래스 +1
   
    - x2​=[0,3]
 
        - w·x₂ + b = 3 + (-3) = 0 -> 경계 위 (보통은 -1로 )
    
    - x3​=[−1,0]
 
        - w·x₃ + b = -2 + (-3) = -5 < 0 -> 음수 -> 클래스 -1  

<br/>

### 편향과 결정 경계의 관계 

- 편향이 없을 때 결정 경계는 w ⋅ x = 0

    - 경계를 **자유롭게 움직일 수 없고 유연성이 떨어짐**
 
- 편향이 있으면 경계 방정식은 w ⋅ x + b = 0

    - 이제 경계는 원래 직선과 평행한 채 어디든 이동 가능
 
    - 편향 b가 원점으로부터의 거리(수직 이동)를 결정
 
        - b > 0 : 양수 클래스가 되기 쉬워짐
     
        - b < 0 : 음수 클래스가 되기 쉬워짐 

<br/>

### 특수한 경우 

- 여기서 x1과 x2를 그냥 x와 y로 생각하면 쉽다
  
- Case 1 : 결정 경계는 x1 = 0, 즉 y축

    -  y축 오른쪽(x1 > 0)은 양성(positive class)


- Case 2 : x1​ + x2 ​= 0, 즉 x2​=−x1

    - 이것도 x1과 x2를 x, y 로 생각하면 y = -x 직선이다
   
    - 즉, x1 + x2 > 0 은, -x 직선보다 위에 존재하면 +1, 아래면 -1과 동치이다 

 
- Case 3 : w를 2배한 경우 -> (2w)·x + 2b = 2(w·x + b) = 0

    - 가중치 크기를 키우면 **직선(경계) 위치는 똑같고**, 단지 점이 선에서 얼마나 멀리 있는지 **확신 정도**만 바뀐다

<br/>

### Convergence Bounds (수학적 상한선)

- **Convergence Bounds** : 학습 전에 퍼셉트론이 학습 과정에서 **최대 몇 번**까지 틀릴 수 있는지를 보장하는 수학적 상한선

- 퍼셉트론은 얼마나 빨리 학습하는가?

- 최대 오류 횟수는 (R/γ)^2 이하 ((R/γ)제곱 이하)

    - R : 원점(0,0)으로부터 각 데이터 점까지의 거리 중 **최댓값**
 
    - γ (마진) : 결정 경계와 가장 가까운 점까지의 거리 (**최소값**)
 
        - 마진(margin)이 크면 (클래스가 멀리 떨어져 있으면) -> 빠르게 학습한다
     
        - 마진(margin)이 작으면 (클래스가 거의 겹쳐 있으면) -> 학습이 느리다
     
        - 데이터가 너무 크면 -> 학습 속도가 느려진다
      
<br/>

### Convergence Bounds 공식 및 증명 

- 아래는 가중치가 최소 어느정도 증가하고 최대 어느정도까지 증가하는지를 보여준다

    - 이것을 통해 **퍼셉트론**은 무한히 학습하지 않고 **수렴**한다는 것을 보여준다 

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/수학적상한선.png)

<br/>

- **lower bound** : 우리가 반복해서 가중치 w를 업데이트할 때, w의 크기(길이)는 무조건 **일정 속도 이상**으로 커진다는 보장

- **lower bound 증명**

    - 처음에는 가중치 w0가 0이고, w*가 이상적인 분리기(정답 가중치)라고 하자
 
    - 만약 샘플을 잘못 분류하면, 가중치를 yixi만큼 업데이트 한다
 
        - 즉, w(t+1) = w(t) + yixi
     
    - 여기서 양 변에 w*를 내적해보자 
 
        -  w(t+1)​⋅w∗ = (wt​ + yi​xi​)​⋅w∗ = w(t)​⋅w + yi(xi​⋅w∗)
     
        - 즉, 새로운 가중치 w(t+1)과 w∗ 의 내적은 **이전 값 + yi(xi​⋅w∗)** 이다
     
        - 따라서 t번째에서 t+1번째로 가중치를 업데이트 할 때, 최적의 방향으로 yi(xi​⋅w∗) 만큼 밀어준다
 
    - w∗는 최적 분리기라 항상 샘플을 올바르게 분류하므로, 내적 값이 **최소 𝛼**( 𝛼는 마진 ) 이상이다
 
        - 즉 최적의 방향으로 yi(xi​⋅w∗) 밀어주는데, 이 값이 최소 마진보다는 크다는 것이다
     
        - 따라서 yi(xi​⋅w∗) ≥ α
 
    - 따라서 매번 실수를 할 때마다 w(t+1)​⋅w∗ ≥ wt​⋅w∗ + α 이다
 
        - 
































































