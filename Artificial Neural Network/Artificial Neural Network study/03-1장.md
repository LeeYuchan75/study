<img width="326" height="38" alt="image" src="https://github.com/user-attachments/assets/403f4da4-be6e-4de9-ba94-d21d44b64f18" />### Terminology (용어 정리)

- **output(출력)의 종류**

    - **Machine Learning (머신러닝)**

        - 입력과 출력 쌍을 가지고 학습하는 지도학습 형태
     
            - 비지도 학습과 강화 학습은 output이 존재 x 

    - **Statistical literature (통계적 문헌)**
 
        - {Predictors, Responses} 형태 :  (예측 변수, 반응)
     
        - {Independent variable, Dependent variables} 형태 : (독립변수(입력), 종속변수(출력))
     
    - **Quantitative Output (정량적 출력)**
 
        - 출력값이 **수치**로 나오는 문제
     
        - 어떤 측정값은 다른 것보다 크거나 작다
     
    - **Qualitative Output (정성적 출력)**
 
        - **범주(카테고리)** 로 분류하는 문제
     
        - 특수한 경우로 크기 순서가 존재하는 **순서형 범주형(ordered categorical)** 도 존재
     
            - ex: 티셔츠 사이즈 S, M, L

<br/>

- **문제별 역할 분류**

    - Quantitative Output (정량적 출력) : 예측할 때는 **회귀(regression)** 사용
 
    - Qualitative Output (정성적 출력) : 예측할 때는 **분류(classification)** 사용
 
    - 어떤 방법들은 정량적 입력에 가장 적합하게 정의되고, 어떤 방법들은 정성적 입력에 더 적합하며, 어떤 방법들은 두 경우 모두에 사용할 수 있다
 
        - 정량적 : 선형 회귀
     
        - 정성적 : 나이브 베이즈
     
        - 둘 다 : 결정 트리

<br/>

### Qualitative variables (정성적 출력)

- **Qualitative variables (정성적 출력)** : 값이 수치가 아니라 **범주(카테고리)** 를 나타내는 변수

    - 보통 숫자 코드로 표현함
 
        - 보통 하나의 이진수(bit)로 0/1 또는 -1/1로 표현
     
        - 이런 숫자 코드를 타겟(target)이라고 부르기도 한다
 
    - 범주가 두 개만 있을 때
 
        - ex: 남=0, 여=1
     
    - 만약 범주가 두 개 이상일 경우 : **더미 변수(dummy variable)** 사용, 즉 **원-핫 인코딩(one-hot encoding)** 사용
 
        - K개의 범주를 가진 질적 변수는 K개의 이진 변수(비트)로 표현되며, 한 번에 하나만 ‘켜져(on)’ 있다
     
        - ex: {Red, Blue, Green} → Red=(1,0,0), Blue=(0,1,0), Green=(0,0,1)


- **우리의 목표**

    - 입력 벡터 X가 주어졌을 때, **출력 Y를 잘 예측하는 것**이 목표이며, 예측값은 Ŷ(“y-hat”)으로 표시
 
        - Y는 실제값, Ŷ는 모델의 예측값 

<br/>

### Statistical Decision Theory (통계적 의사결정 이론) : 정량적 출력 용어 정리

- 정량적 출력을 수학적으로 다뤄보자

    - X ∈ ℝᵖ : X는 p차원 실수값 (random input vector)

    - Y ∈ ℝ : Y는 실수값(random output variable)

    - Pr(X, Y) : X와 Y의 결합 분포(joint distribution)

    - f(X) : 입력 X가 주어졌을 때 Y를 예측하는 함수
 
        - **머신러닝의 목표**는 Y를 잘 표현할 수 있는 f(x)를 찾는 것 
     
    - L(Y, f(X)) : 손실함수
 
        - 가장 흔한 손실함수 : L(Y, f(X)) = (Y – f(X))² (제곱 오차(squared error loss))

<br/>

### 기대 제곱 예측 오차(Expected squared prediction error, EPE)

- **기대 제곱 예측 오차(Expected squared prediction error)**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE.png)
   
    - 실제 값(Y)과 모델의 예측 값(f(X))의 차이를 제곱한 것의 기댓값(E)
 
        - **기대값(expectation)** : 어떤 과정을 무한히 반복했을 때 얻게 되는 평균 결과
     
        - 기대값은 E로 표기하고 **확률 분포**에 의해 정해지는 **가중 평균**이다 
      
    - 이것은 f를 선택하는 기준으로 사용
 
    - 여기서 우리는 입력(X)과 출력(Y) 모두에서 확률 변수와 불확실성을 다룬다
 
        - 입력과 출력이 고정된 값이 아니라, 확률적으로 변할 수 있으므로 전체 분포에 걸친 평균적 오차를 계산해야 한다

    - 단순히 훈련 데이터에서 오차를 최소화하는 게 아니라, 전체 분포에서 **평균적으로** 오차가 최소가 되는 함수 f를 찾는 게 목표

<br/>

### 기대 제곱 예측 오차 (EPE) 공식 분석 

- 우리가 위에서 봤던 EPE 공식은 아래와 같다

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개1.png)
 
    - 우리의 목표는 예측 모델 f(x)의 **전체 평균 제곱 오차(EPE)** 를 가장 작게 만드는 것이다

<br/>
 
- 이 목표를 달성하기 위한 전략은 전체 평균 오차를 한 번에 다루는 대신, 문제를 더 작은 단위로 쪼개서 평균을 계산하자는 것이다
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개2.png)
 
    - 위 공식에서 안쪽 기댓값 E(Y|X) 는 X = k 일 때 해당하는 확률을 계산하는 것이고 바깥쪽 기댓값 E(x)는 해당 그룹이 나타날 확률을 가중치로 사용하여 평균을 나타낸 것이다
 
    - 또한 f(x)는 우리가 만들고 싶은 예측 함수(Prediction Model)이다 
 
    - 아래 전체적인 예시가 있으니 일단, 작은 문제로 쪼개는 것에 초점을 두자 
 
<br/>

- 문제를 분리하는 전략 덕분에, 전체 EPE 최소화라는 거대한 문제를 아래와 같이 **각 그룹별 오차 최소화** 라는 단순한 미니 문제로 바꿀 수 있다 
  
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개3.png)
 
    - 오차를 최소화하는 그 최적의 값 c는 놀랍게도 아주 간단한 **조건부 기댓값(conditional mean)**, 즉 해당 x 그룹에 속하는 Y 값들의 평균으로 증명되었다.
 
    - 즉 아래 공식을 구하는 것이 **EPE를 최소화하는 모델을 구하는 것**이다 
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개4.png)
 
        - 이렇게 만들어진 모델 f(x)를 가지고 EPE를 계산하면, 그 어떤 다른 모델보다도 가장 낮은 EPE 점수가 나오게 된다 

<br/>

### 기대 제곱 예측 오차 (EPE) 공식 흐름 예시


- **위 공식 흐름을 이용한 예시** : 학생이 공부한 시간(X)을 알 때, 시험 점수(Y)를 가장 정확하게 예측하는 함수 f(x)를 찾아라 

    - X (입력): 공부 시간 (1시간 또는 2시간)

    - Y (결과): 시험 점수 (70점, 80점, 90점)

    - f(x) (모델): 우리가 만들 예측 함수. f(1)은 1시간 공부했을 때의 예측 점수
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식예시표.png)
      
    <br/>
    
    - 여기서 아래 공식을 사용해보자
 
        - 위에서 증명한 대로 아래 공식을 이용해서 EPE를 구하기 위한 f(x)를 구해보자
     
            - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개4.png)
              
    <br/>
    
    -  **1시간 공부한 그룹 (X=1)**

    -  (E[Y|X=1]) =  70×(0.1/0.6) + 80×(0.4/0.6) + 90×(0.1/0.6) = 80점
 
        - 여기서 조건부 확률 사용
     
        - ex: 70점을 의미하는 분수의 분모를 보면 X = 1 인 학생들 중 70점인 인 확률은 전체 0.6에서 0.1임
     
     - (E[Y|X=1]) = 80 이므로  f(1) = 80
 
     <br/>

     - **2시간 공부한 그룹 (X=2)**
 
     - E[Y|X=2] = 80×(0.1/0.4) + 90×(0.3/0.4) = 87.5점
 
     - f(2) = 87.5
 
     <br/>

     - **최종 f(x)**
 
         - f(1) = 80

         - f(2) = 87.5

    <br/>
    
    - **EPE 최종 계산**
 
       - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개1.png)
     
           - 위 공식에서 f(x)에 우리가 구한 함수 값을 넣어서 계산해주면 
 
       - [y - f(x)]² × Pr(x, y) 구하기
     
           - (70 - 80)² × 0.1 = 10.0

           - (80 - 80)² × 0.4 = 0.0

           - (90 - 80)² × 0.1 = 10.0

           - (80 - 87.5)² × 0.1 = 5.625

           - (90 - 87.5)² × 0.3 = 1.875
        
           - 최종 EPE = 10.0 + 0.0 + 10.0 + 5.625 + 1.875 = 27.5
        
           - 27.5가 **모델이 가질 수 있는 가장 낮은 오차 점수**

<br/>

### linear regression (선형회귀)

- 선형 회귀 모델은 회귀 함수 E(Y|X)가 입력 변수들에 대해 **선형(linear)** 이라고 가정

    - 위에서 증명한 EPE에 대한 내용은 이론적인 부분임, 실제로 E(Y|X)는 매우 복잡한 곡선일 수 있음
 
    - 하지만 현실에서는 데이터가 제한적이라 그 복잡한 곡선 형태를 완벽히 알아낼 방법이 없기 때문에 단순한 직선 모양으로 타협함 

<br/>

- 선형 회귀는 **단순**하며, 입력이 출력에 어떻게 영향을 미치는지에 대한 **해석 가능한(interpretable)** 설명을 제공

    - ex: (점수) = 2 × (공부시간) + 3 이라는 모델이 있다면, 공부시간이 1시간 늘면 점수가 2점 오른다라고 명확하게 설명하고 해석 가능
 
    - 예측 목적으로는, 때때로 비선형 모델보다 성능이 더 좋을 수도 있다
 
<br/>

- **선형회귀 공식**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1선형회귀공식.png)
 
    - Ŷ (Y햇): 모델이 예측한 값

    - β̂₀ (베타햇 제로): 절편(intercept)이라는 의미로 모든 입력(X)이 0일 때의 기본값

    - β̂ⱼ (베타햇 j): 계수(coefficient)라는 의미로 j번째 입력 변수 Xⱼ가 결과에 얼마나 영향을 미치는지 나타내는 가중치 또는 중요도

<br/>

- 모델이 예측한 값 Ŷ (Y햇)을 풀어서 쓰면 아래와 같다

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1y햇.png)
 
    - 하지만 현실 세계에서의 데이터는 완벽한 직선 위에 존재하지 않으므로 오차 ε (엡실론)를 반영해야 한다
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1선형회귀실제y값.png)

<br/>

- 정리하면
  
    - f(X) = E[Y|X] : 평균적인 오차를 가장 작게 만드는 함수 (이상적)
 
    - f(X) = β₀ + β₁X : 현실적인 예측 모델 (현실적)

<br/>

### 선형회귀 그래프 예시 

![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장선형회귀그래프예시.png)

- 왼쪽 그래프 :  X 지점(2, 5, 8)에서 Y의 평균(노란색 x)을 보여줌

    - 파란색 세로 선은 x = k 일 때, y 값의 분포임 

- 오른쪽 그래프 : 각 X 지점에서의 Y의 조건부 확률 분포를 보여줌

    - X=2일 때의 Y 분포, X=4일 때의 Y 분포처럼 X 값에 따라 조건적으로 달라지는 분포를 보여줌

    - 모든 확률 분포가 동일한데 이것은 등분산성(Homoscedasticity)을 가정한 것

<br/>
 
- 즉, 위 그래프를 보고 알 수 있는 사실은 특정 X 지점에서 Y 데이터가 분포하는 모습(p(Y|X=x))은, 회귀선(β₀+β₁X)을 정중앙으로 하는 **정규분포**이다

    - 예를 들어. 우리가 x 시간 만큼 공부했을 때 성적을 평균으로 구할 때 하나의 스칼라 값 (ex:  85점이 평균) 으로 나타내지만, 실제로는 정확히 80점이 아닐 수 있다
 
    - 하지만 이 결론으로 인해 Y의 분포는 85점을 중심으로 하는 정규분포라는 사실을 알기 때문에 구간을 제시할 수 있게 된다 

<br/>

### RSS (residual sum of squares) : 잔차 제곱합 

- ŷᵢ를 i번째 예측값이라고 하자. 그러면 eᵢ = yᵢ - ŷᵢ는 i번째 **잔차(residual)** 를 나타낸다

- ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장RSS공식.png)

    - 이것은 아래 공식과 동치이다
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장RSS공식2.png)

<br/>

- **RSS (residual sum of squares) 최소화 하는 방법**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장RSS최소화공식.png)
 
    - RSS는 반복적으로 업데이트를 하는게 아니라 우리가 가진 모든 데이터(모든 x와 y)를 이 공식에 단 한 번만 넣으면, RSS를 최소화하는 유일하고 완벽한 기울기와 편향 값이 바로 계산된다
 
        - 기울기 β̂₁ 분자 분석
     
            - (xᵢ - x̄): i번째 x값이 평균보다 큰가 작은가?

            - (yᵢ - ȳ): i번째 y값이 평균보다 큰가 작은가?
         
            - x도 평균보다 크고, y도 평균보다 크면 → (+) 값
         
                - 우상향 관계 파악  

            - x도 평균보다 작고, y도 평균보다 작으면 → (+) 값 (- × -)
         
                - 우하향 관계 파악 

            - 둘의 방향이 다르면 → (-) 값
       
        - β̂₁ 분모 분석
     
            - X 데이터 자체가 얼마나 넓게 퍼져있는가?

        <br/>

        - 절편 β̂₀ 분석
     
            - 우리가 구한 직선이 데이터의 **정중앙**을 지나가도록 보정해주는 값
         
            - 데이터를 가장 잘 나타내는 직선은 반드시 데이터의 무게중심, 즉 **평균 지점(x̄, ȳ)을 통과**해야 한다는 사실을 이용

<br/>

- **RSS 사용 예시**

- TV 광고비(X축)와 매출(Y축) 관계

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장RSS사용예시.png)

<br/>

### 선형 회귀 분석 

- 선형 모델은 p차원 입력 공간상의 함수로 볼 때 공식은 **f(X) = Xᵀβ** 이다

    - 이때 미분 값은 f'(X) = β 이므로 **계수 벡터 β**는 각 입력 변수(X)가 바뀔 때 예측(f(X))이 얼마나 가파르게 변하는지를 알려주는 **기울기 역할**을 한다 

<br/>

- 선형 모델을 벡터로 나타낼 떄 RSS는 다음과 같다

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장RSS벡터공식.png)

<br>

- 선형 모델을 행로 나타낼 떄 RSS는 다음과 같다

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장RSS행공식.png)

<br/>

-  RSS(β)는 파라미터(β)에 대한 **2차 함수(quadratic function)** 이므로, **최솟값은 항상 존재**하지만 **유일하지 않을 수 있음**

    - β에 대해 미분하면 정규 방정식 Xᵀ(y - Xβ) = 0 이므로, 이 식을 만족하는 β를 찾으면 된다
 
    - 만약 XᵀX가 비특이(nonsingular) 행렬이면, 유일한 해는 β̂ = (XᵀX)⁻¹Xᵀy 이다 

<br/>

### Classification (분류)

- 숫자가 아닌 카테고리를 예측 (ex: 강아지, 고양), 즉 **Qualitative(질적)** 변수를 예측

    - **확률**을 이용하여 계산 

<br/>

- Qualitative(질적) 변수는 **순서가 없는 집합** C의 값을 가진다

    - ex: 눈 색깔 ∈ {갈색, 파란색, 녹색}, 갈색, 파란색, 녹색의 순서는 중요하지 않음 

<br/>

- **Classification (분류) 정의** : 특성 벡터 X와 질적 반응 Y가 주어졌을 때, 분류 문제는 X를 입력으로 받아 Y의 값을 예측하는 함수 C(X)를 만드는 것

<br/>

- 예측 함수는 이산적인 값을 가지므로, 우리는 항상 입력 공간을 분류에 따라 레이블이 지정된 **영역(regions)들의 집합**으로 나눌 수 있다

    - ex: 스팸 영역과 정상 영역
 
    - 영역들의 경계(boundaries)는 예측 함수에 따라 거칠거나 부드러울 수 있다

    - 이러한 **결정 경계(decision boundaries)** 를 직선으로 만드는 방법을 **선형(linear) 분류**를 통해 이루어진다 

<br/>

- **Classification (분류) 예시**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장분류예시그림.png)
 
    - 주황색/파란색 점 : 실제 데이터로 각각 다른 종류의 데이터를 의미
 
        - 주황색 영역에 새로운 데이터가 들어오면 주황색 그룹이라고 예측
     
        - 파란색 영역에 새로운 데이터가 들어오면 파란색 그룹이라고 예측

    - 검은색 직선 : 두 그룹을 나누는 **결정 경계(Decision Boundary)**

<br/>

- 시험 점수로 예시를 들어서 만약  xᵀβ̂ = 0.5 인 결정 경계로 분류를 해보자 
 
    - RSS로 데이터를 학습시켜서 최적의 기울기가 0.02이고 편향이 -0.8 이 나왔다고 가정하면
     
    - 예측 점수 ŷ = 0.02 × (시험 점수 X) - 0.8 이다
     
        - 이 공식은 0.02 × X - 0.8 = 0.5 이고
         
        - 정리하면 0.02 × X = 1.3 이다. 따라서 X = 65점이 커트라인이다
     
     - 즉, 65점보다 높으면 합격으로 분류, 낮으면 불합격으로 분류됨

<br/>

### 분류 문제에 선형 회귀를 적용할 수 있을까 

- 채무 불이행(Default) 분류 문제에 대해 Y를 {아니오=0, 예=1}로 코딩한다고 가정해보자

    - Y를 X에 대해 단순 선형 회귀를 수행하고 Ŷ > 0.5이면 '예'로 분류할 수 있을까?
 
    - 이러한 **이진 결과(두 종류 중 하나)** 의 경우, 선형 회귀는 분류기로서 **꽤 잘 작동**하며, 나중에 논의할 선형 판별 분석(LDA)과 동일
 
    - 모집단에서 E(Y|X=x) = Pr(Y=1|X=x) 이므로, 우리는 회귀가 이 작업에 완벽하다고 생각할 수도 있다
 
        - E(Y|X=x) = Pr(Y=1|X=x)의 의미: Y의 값이 0(아니오)과 1(예) 뿐이므로, Y의 평균값(E(Y|X))은 결국 '1이 나올 비율'과 같다
     
<br/>

- **하지만 한계가 존재**한다

    - 두 개보다 많은 클래스(종류)를 가진 질적 (qualitative) 반응을 수용할 수 없다
 
    - 회귀 방법은 (클래스가 단 두 개일 때조차) Pr(Y|X)(확률)에 대한 의미 있는 추정치를 제공하지 않음
 
        - 선형 회귀의 예측값은 확률의 범위인 0과 1을 쉽게 벗어나기 때문 (Pr > 1, Pr < 0) 

    - 따라서 선형 모델 말고 로지스틱 모델을 쓰는게 더 나음 

















