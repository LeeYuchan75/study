### Terminology (용어 정리)

- **output(출력)의 종류**

    - **Machine Learning (머신러닝)**

        - 입력과 출력 쌍을 가지고 학습하는 지도학습 형태
     
            - 비지도 학습과 강화 학습은 output이 존재 x 

    - **Statistical literature (통계적 문헌)**
 
        - {Predictors, Responses} 형태 :  (예측 변수, 반응)
     
        - {Independent variable, Dependent variables} 형태 : (독립변수(입력), 종속변수(출력))
     
    - **Quantitative Output (정량적 출력)**
 
        - 출력값이 **수치**로 나오는 문제
     
        - 어떤 측정값은 다른 것보다 크거나 작다
     
    - **Qualitative Output (정성적 출력)**
 
        - **범주(카테고리)** 로 분류하는 문제
     
        - 특수한 경우로 크기 순서가 존재하는 **순서형 범주형(ordered categorical)** 도 존재
     
            - ex: 티셔츠 사이즈 S, M, L

<br/>

- **문제별 역할 분류**

    - Quantitative Output (정량적 출력) : 예측할 때는 **회귀(regression)** 사용
 
    - Qualitative Output (정성적 출력) : 예측할 때는 **분류(classification)** 사용
 
    - 어떤 방법들은 정량적 입력에 가장 적합하게 정의되고, 어떤 방법들은 정성적 입력에 더 적합하며, 어떤 방법들은 두 경우 모두에 사용할 수 있다
 
        - 정량적 : 선형 회귀
     
        - 정성적 : 나이브 베이즈
     
        - 둘 다 : 결정 트리

<br/>

### Qualitative variables (정성적 출력)

- **Qualitative variables (정성적 출력)** : 값이 수치가 아니라 **범주(카테고리)** 를 나타내는 변수

    - 보통 숫자 코드로 표현함
 
        - 보통 하나의 이진수(bit)로 0/1 또는 -1/1로 표현
     
        - 이런 숫자 코드를 타겟(target)이라고 부르기도 한다
 
    - 범주가 두 개만 있을 때
 
        - ex: 남=0, 여=1
     
    - 만약 범주가 두 개 이상일 경우 : **더미 변수(dummy variable)** 사용, 즉 **원-핫 인코딩(one-hot encoding)** 사용
 
        - K개의 범주를 가진 질적 변수는 K개의 이진 변수(비트)로 표현되며, 한 번에 하나만 ‘켜져(on)’ 있다
     
        - ex: {Red, Blue, Green} → Red=(1,0,0), Blue=(0,1,0), Green=(0,0,1)


- **우리의 목표**

    - 입력 벡터 X가 주어졌을 때, **출력 Y를 잘 예측하는 것**이 목표이며, 예측값은 Ŷ(“y-hat”)으로 표시
 
        - Y는 실제값, Ŷ는 모델의 예측값 

<br/>

### Statistical Decision Theory (통계적 의사결정 이론) : 정량적 출력 용어 정리

- 정량적 출력을 수학적으로 다뤄보자

    - X ∈ ℝᵖ : X는 p차원 실수값 (random input vector)

    - Y ∈ ℝ : Y는 실수값(random output variable)

    - Pr(X, Y) : X와 Y의 결합 분포(joint distribution)

    - f(X) : 입력 X가 주어졌을 때 Y를 예측하는 함수
 
        - **머신러닝의 목표**는 Y를 잘 표현할 수 있는 f(x)를 찾는 것 
     
    - L(Y, f(X)) : 손실함수
 
        - 가장 흔한 손실함수 : L(Y, f(X)) = (Y – f(X))² (제곱 오차(squared error loss))

<br/>

### 기대 제곱 예측 오차(Expected squared prediction error, EPE)

- **기대 제곱 예측 오차(Expected squared prediction error)**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE.png)
   
    - 실제 값(Y)과 모델의 예측 값(f(X))의 차이를 제곱한 것의 기댓값(E)
 
        - **기대값(expectation)** : 어떤 과정을 무한히 반복했을 때 얻게 되는 평균 결과
     
        - 기대값은 E로 표기하고 **확률 분포**에 의해 정해지는 **가중 평균**이다 
      
    - 이것은 f를 선택하는 기준으로 사용
 
    - 여기서 우리는 입력(X)과 출력(Y) 모두에서 확률 변수와 불확실성을 다룬다
 
        - 입력과 출력이 고정된 값이 아니라, 확률적으로 변할 수 있으므로 전체 분포에 걸친 평균적 오차를 계산해야 한다

    - 단순히 훈련 데이터에서 오차를 최소화하는 게 아니라, 전체 분포에서 **평균적으로** 오차가 최소가 되는 함수 f를 찾는 게 목표

<br/>

### 기대 제곱 예측 오차 (EPE) 공식 분석 

- 우리가 위에서 봤던 EPE 공식은 아래와 같다

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개1.png)
 
    - 우리의 목표는 예측 모델 f(x)의 **전체 평균 제곱 오차(EPE)** 를 가장 작게 만드는 것이다

<br/>
 
- 이 목표를 달성하기 위한 전략은 전체 평균 오차를 한 번에 다루는 대신, 문제를 더 작은 단위로 쪼개서 평균을 계산하자는 것이다
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개2.png)
 
    - 위 공식에서 안쪽 기댓값 E(Y|X) 는 X = k 일 때 해당하는 확률을 계산하는 것이고 바깥쪽 기댓값 E(x)는 해당 그룹이 나타날 확률을 가중치로 사용하여 평균을 나타낸 것이다
 
    - 또한 f(x)는 우리가 만들고 싶은 예측 함수(Prediction Model)이다 
 
    - 아래 전체적인 예시가 있으니 일단, 작은 문제로 쪼개는 것에 초점을 두자 
 
<br/>

- 문제를 분리하는 전략 덕분에, 전체 EPE 최소화라는 거대한 문제를 아래와 같이 **각 그룹별 오차 최소화** 라는 단순한 미니 문제로 바꿀 수 있다 
  
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개3.png)
 
    - 오차를 최소화하는 그 최적의 값 c는 놀랍게도 아주 간단한 **조건부 기댓값(conditional mean)**, 즉 해당 x 그룹에 속하는 Y 값들의 평균으로 증명되었다.
 
    - 즉 아래 공식을 구하는 것이 **EPE를 최소화하는 모델을 구하는 것**이다 
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개4.png)
 
        - 이렇게 만들어진 모델 f(x)를 가지고 EPE를 계산하면, 그 어떤 다른 모델보다도 가장 낮은 EPE 점수가 나오게 된다 

<br/>

### 기대 제곱 예측 오차 (EPE) 공식 흐름 예시


- **위 공식 흐름을 이용한 예시** : 학생이 공부한 시간(X)을 알 때, 시험 점수(Y)를 가장 정확하게 예측하는 함수 f(x)를 찾아라 

    - X (입력): 공부 시간 (1시간 또는 2시간)

    - Y (결과): 시험 점수 (70점, 80점, 90점)

    - f(x) (모델): 우리가 만들 예측 함수. f(1)은 1시간 공부했을 때의 예측 점수
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식예시표.png)
      
    <br/>
    
    - 여기서 아래 공식을 사용해보자
 
        - 위에서 증명한 대로 아래 공식을 이용해서 EPE를 구하기 위한 f(x)를 구해보자
     
            - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개4.png)
              
    <br/>
    
    -  1시간 공부한 그룹 (X=1)

    -  계산 (E[Y|X=1]) =  70×(0.1/0.6) + 80×(0.4/0.6) + 90×(0.1/0.6) = 80점
 
        - 여기서 조건부 확률 사용
     
        - ex: 70점을 의미하는 분수의 분모를 보면 X = 1 인 학생들 중 70점인 인 확률은 전체 0.6에서 0.1임
     
     - (E[Y|X=1]) = 80 이므로  f(1) = 80












































