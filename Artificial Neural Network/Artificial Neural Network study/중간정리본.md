### 중간 기말 : 공식 유도 위주 

### 1주차
- **Supervised learning (지도학습)** :정답 존재

    - MNIST 데이터셋 : 손글씨 예측
   
    - Object recognition (객체 인지) : 사진 속 물체 예측
   
    - Neural Machine Translation (신경망 기반 기계 번역) : 벡터를 이용해서 언어 변환 
   
    - Caption generation (캡션 생성) : 이미지에 맞는 문장 생성

<br/>

- **Unsupervised learning (비지도학습)** : 정답 없음

    - CycleGAN : 얼룩말 & 말 교체 가능 

<br/>

- **Reinforcement learning (강화학습)** : 보상 최대

    - DeepMind : 단일 신경망(single network)으로 Atari 2600 플레이

 <br/>
 
- **Statistical Learning (통계적 학습)**

    - 머신러닝 관점 : 맞고/틀리다(정답 예측)이 중요 ex: XGBOOST

    - 통계적 학습 관점 : **상관관계**를 찾고 설명하는 것 ex: linear regression, logistic regression
      
    - ex : 전립선암 위험 요인 찾기, 이메일 스팸 탐지, 유전자 기반 암 분류, 위성 이미지 픽셀 분류

<br/>

- **신경망(Neural Network, NN)**

    - 은닉층이 0개, 1개, 또는 여러 개여도 모두 신경망이라고 부름
 
    - 이미지 인식, 언어 번역, 음성 인식, 질병 진단, 로봇 제어

<br/>

- **심층신경망(Deep Neural Network, DNN)**

    - 은닉층이 2개 이상인 경우
 
<br/>

### 2 주차 

- **뉴런의 흐름**

    -  뉴런은 수상돌기로 입력 신호를 받고 → 세포체에서 처리하고 → 축삭을 통해 출력 신호를 전달하며 → 시냅스에서 다른 뉴런과 연결된다

<br/>

- **활성화 함수** : 뉴런의 발화 여부나 강도를 결정하는 함수
  
<br/>

- **퍼셉트론 측면**
    
    - **Net input (순입력)** : **가중치가 적용**된 입력값

    - **Activations (활성값)** : 활성화 함수(activation function)를 Net input (순입력)에 적용한 값

    - **Label output (라벨 출력)** : 마지막 층의 활성값에 **임계값(threshold)** 을 적용한 것
 
        - 퍼셉트론에서는 **활성화 함수**가 **임계값 함수**와 같음

<br/>

- **선형회귀 측면** : activation = net input = output (활성값 = 순입력 = 출력)

    - 선형 회귀는 **별도의 활성화 함수가 없이**, 단순히 입력합이 그대로 결과 

<br/>

- **퍼셉트론의 핵심**

    - 틀린 점을 반대편으로 밀어서 **올바른 쪽**에 위치시키기 

    - **임계점** : (가중합 + 편향) > 0 이면 출력=1, 그렇지 않으면 0

    - **선형적**으로 구분된 경우만 분류 가능 (비선형적 ex: XOR, NP-hard 은 불가능)
 
        - 올바른 출력을 위한 가중치 업데이트를 반복
     
            - w := w + err × x 
     
        - 위 업데이트 후의 예측값이, 업데이트 전의 예측값보다 **항상 크거나 같다**

<br/>

- **Linear Decision Boundaries (선형 결정 경계)**

    - 데이터 공간을 양쪽으로 나누는 선/면/초평면 : **w⋅x+b = 0**

<br/>

- **Convergence Bounds**

    - **lower bound** : w와 w*의 내적은 **실수가 발생**할 때마다 **margin 만큼 선형적**으로 증가한다
 
        - 이는 w가 점점 올바른 해 w*와 정렬되어 감  

    - **upper bound** : 상한: w의 제곱 노름은 βi 이하이다
    
        - β는 데이터 샘플들 중 가장 큰 입력 벡터의 길이의 제곱값
     
        - w가 **무한정 커지지 않고 일정 속도**로만 커짐
             
    -  **업데이트 횟수 i는 β / α² 이하**
 
        - 데이터가 잘 구분될수록 **α 큼** → **오류가 줄어든다**

        - 데이터가 크고 복잡할수록 **β 큼** → **오류가 늘어난다**

<br/>

- **퍼셉트론 장단점**

    - 장점
 
        - 단순하고 구현이 쉽다

        - 선형적으로 분리 가능한 데이터에서는 반드시 수렴한다

        - 온라인 학습(데이터가 순차적으로 들어와도 갱신 가능)에 적합
    


   - 퍼셉트론의 한계점 

       - **직선(선형) 경계**만 분리 가능

           - **XOR 문제를 풀 수 없다**
 
           - 곡선이나 복잡한 경계가 있는 데이터는 분류할 수 없다


       - **이진 분류**만 가능하다

           - 확장 방법이 있긴 하지만 성능이 제한적이다  



       - **확률**적인 출력을 **제공하지 않는다**

           - 퍼셉트론은 예/아니오 만 말하고 몇 % 확신하는지는 알려주지 못한다
 
           - 신뢰도 측정이 없다



       - **이상치(outlier)에 민감**하다

           - 하나의 잘못된 점이 경계를 크게 바꿀 수 있다 


       - **분리 불가능**한 데이터에 대해서는 **보장이 없다**

           - 퍼셉트론이 무한히 업데이트만 반복
 
           - 최선의 근사 해도 제공하지 않는다

<br>

- **다층신경망** 

    - 퍼셉트론의 한계 해결
 
    - **Hidden layers (은닉층)**
 
        - 충분한 뉴런이 있다면 어떤 연속 함수라도 근사
     
    - **Nonlinear activation (비선형 활성화)**
 
        - 단순 계단 함수만 쓰지 않는다
     
    - **Backpropagation (역전파)**
 
        - 오차를 뒤로 전파해 가중치를 조정 

<br/>

### 3 주차

- **요약정리**

    - EPE : 선형회귀 오차 분석

    - RSS : 선형회귀 최적의 베타

    - 최대 가능도(maximum likelihood) : 로지스틱의 최적의 베타 

<br/>

- **기대 제곱 예측 오차(Expected squared prediction error, EPE)**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE.png) 

    - 목표 : **평균적**으로 오차가 최소가 되는 함수 f를 찾는 것
 
    - **조건부 기댓값(conditional mean)** 사용
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/EPE공식전개4.png) 
 
    - 풀이 과정
    
        - (1) X = k인 조건부 기댓값(conditional mean) 구하기
     
        - (2) 구한 f(x)를 EPE 식에 대입
     
        - (3) 결과 값이 모델의 평균적인 오차가 되는 것 

<br/>

- **RSS (residual sum of squares) : 잔차 제곱합**

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장RSS공식.png) 

    - **선형회귀**에서 사용
 
    - 반복적 업데이트 x, 이 공식에 x와 y 데이터를 넣으면 최적의 기울기와 편향을 구할 수 있음
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-1장RSS최소화공식.png)

<br/>

### Logistic Model (로지스틱 모델)

- 정의 : 두 가지 종류 중 하나를 예측하는 대표적인 분류(Classification) 알고리즘

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장로지스틱공식.png)
 
    - 로지스틱 범위 : **[0,1]**
 
    - **오즈(odds)** : 사건이 일어나지 않을 확률에 대한 일어날 확률의 비율
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장odds공식.png)
 
    - **로그-오즈(log-odds)** : 0과 1 사이에 갇혀 있는 확률을 -∞ 에서 +∞ 까지의 직선 세계로 보내주는 것
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장로그오즈공식.png)
     
        - 오즈 (p/(1-p)): 범위가 [0, +∞]로 확장

        - 로그-오즈 (log(odds)): 범위가 [-∞, +∞]로 완벽하게 확장

    - **과정**

        - (1) : 로지스틱 회귀 모델은 입력 데이터 X를 받아 **선형 계산**한다
 
        - (2) : 1단계에서 나온 그 숫자를 **로지스틱 함수**에 넣어서 **확률**을 구한다 







































































