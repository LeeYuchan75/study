### 1주차
- **Supervised learning (지도학습)** :정답 존재

    - MNIST 데이터셋 : 손글씨 예측
   
    - Object recognition (객체 인지) : 사진 속 물체 예측
   
    - Neural Machine Translation (신경망 기반 기계 번역) : 벡터를 이용해서 언어 변환 
   
    - Caption generation (캡션 생성) : 이미지에 맞는 문장 생성

<br/>

- **Unsupervised learning (비지도학습)** : 정답 없음

    - CycleGAN : 얼룩말 & 말 교체 가능 

<br/>

- **Reinforcement learning (강화학습)** : 보상 최대

    - DeepMind : 단일 신경망(single network)으로 Atari 2600 플레이

 <br/>
 
- **Statistical Learning (통계적 학습)**

    - 머신러닝 관점 : 맞고/틀리다(정답 예측)이 중요 ex: XGBOOST

    - 통계적 학습 관점 : **상관관계**를 찾고 설명하는 것 ex: linear regression, logistic regression
      
    - ex : 전립선암 위험 요인 찾기, 이메일 스팸 탐지, 유전자 기반 암 분류, 위성 이미지 픽셀 분류

<br/>

- **신경망(Neural Network, NN)**

    - 은닉층이 0개, 1개, 또는 여러 개여도 모두 신경망이라고 부름
 
    - 이미지 인식, 언어 번역, 음성 인식, 질병 진단, 로봇 제어

<br/>

- **심층신경망(Deep Neural Network, DNN)**

    - 은닉층이 2개 이상인 경우
 
<br/>

### 2 주차 

- **뉴런의 흐름**

    -  뉴런은 수상돌기로 입력 신호를 받고 → 세포체에서 처리하고 → 축삭을 통해 출력 신호를 전달하며 → 시냅스에서 다른 뉴런과 연결된다

<br/>

- **퍼셉트론의 핵심**

    - 틀린 점을 반대편으로 밀어서 **올바른 쪽**에 위치시키기 

    - **임계점** : (가중합 + 편향) > 0 이면 출력=1, 그렇지 않으면 0

    - **선형적**으로 구분된 경우만 분류 가능 (비선형적 ex: XOR, NP-hard 은 불가능)
 
        - 올바른 출력을 위한 가중치 업데이트를 반복
     
            - w := w + err × x 
     
        - 위 업데이트 후의 예측값이, 업데이트 전의 예측값보다 **항상 크거나 같다**

<br/>

- **Linear Decision Boundaries (선형 결정 경계)**

    - 데이터 공간을 양쪽으로 나누는 선/면/초평면 : **w⋅x+b = 0**

<br/>

- **Convergence Bounds**

    - **lower bound** : w와 w*의 내적은 **실수가 발생**할 때마다 **margin 만큼 선형적**으로 증가한다
 
        - 이는 w가 점점 올바른 해 w*와 정렬되어 감  

    - **upper bound** : 상한: w의 제곱 노름은 βi 이하이다
    
        - β는 데이터 샘플들 중 가장 큰 입력 벡터의 길이의 제곱값
     
        - w가 **무한정 커지지 않고 일정 속도**로만 커짐
             
    -  **업데이트 횟수 i는 β / α² 이하**
 
        - 데이터가 잘 구분될수록 **α 큼** → **오류가 줄어든다**

        - 데이터가 크고 복잡할수록 **β 큼** → **오류가 늘어난다**

<br/>

- 퍼셉트론 장단점

    - 장점
 
        - 단순하고 구현이 쉽다

        - 선형적으로 분리 가능한 데이터에서는 반드시 수렴한다

        - 온라인 학습(데이터가 순차적으로 들어와도 갱신 가능)에 적합
