### From Biology to Artificial Intelligence

- 생물학적 뉴런의 구성 요소
  
    - 수상돌기(Dendrite): 입력을 받는 부분 (안테나처럼 신호를 받음)
 
    - 세포체(Cell body, Soma): 상돌기로 받은 신호들을 종합하고 계산하는 부분. CPU처럼 중앙 처리기 역할
 
    - 축삭(Axon) : 출력 신호를 전달하는 부분
 
    - 시냅스(Synapse): 뉴런과 뉴런 사이의 연결 지점. 신호가 실제로 전달되는 접점

<br/>

- **핵심 원리**: 입력이 **임계값을 넘으면** 뉴런은 발화(fire)한다 (반응한다)

- 뉴런 발화 과정

    - 많은 뉴런으로부터 신호를 받는다
 
    - 이 신호들을 합친다
 
    - 합이 임계값을 넘으면 발화한다
 
    - 전부 아니면 전무(All-or-nothing) 반응
     
        - 뉴런은 조금만 발화하거나 약하게 발화하는 게 없음. 기준치를 넘으면 100% 반응하고, 안 넘으면 0% 반응함

<br/>

- 뉴런은 수상돌기로 입력 신호를 받고 → 세포체에서 처리하고 → 축삭을 통해 출력 신호를 전달하며 → 시냅스에서 다른 뉴런과 연결된다

![System Resources](../../images/Artificial%20Neural%20Network%20images/뉴런구조.png)

<br/>

### Neurons (뉴런)

- **뉴런** : 뇌와 신경계 속 작은 처리 단위

    - 인간의 뇌에는 약 860억 개의 뉴런이 존재
 
        - 뉴런은 엄청나게 많아, 뇌가 고도의 정보처리를 할 수 있음
     
    - 뉴런은 가느다란 케이블을 통해 전기적, 화학적 신호로 소통함 

<br/>

- **감각 뉴런 (Sensory Perception)** : 신호를 뇌와 척수로 보낸다

    - 뉴런이 감각 정보를 처리하고 반응하도록 도와준다
 
        - ex: 뜨거운 것을 만졌을 때 바로 손을 떼는 현상  

<br/>

- 뉴런은 시냅스라 불리는 작은 틈을 통해 소통한다

    - 뉴런과 뉴런은 직접 붙어 있지 않고, 사이에 있는 시냅스을 통해 화학물질로 신호를 주고받음 

- 자주 소통할수록 시냅스가 강화되어, 학습과 기억에 도움을 준다

    - ex: 영어 단어를 자꾸 외우면 기억에 오래 남음 

<br/>

- **신경가소성 (Neuroplasticity)** : 경험을 통해 뇌가 변화할 수 있는 능력

    - 뇌는 고정된 것이 아니라, 새로운 경험을 하면 뉴런 연결이 바뀌고 뇌 구조도 달라질 수 있다.

        -  ex : 악기를 배우면 뇌의 관련 영역이 발달

<br/>

### Biological Neurons 과 Artificial Neurons 차이점 

- **Biological Neurons (생물학적 뉴런)**

    - 각각 약 1만 개의 연결을 가짐
 
    - 복잡한 시간적 역학(변화)
 
        - 신호가 시간에 따라 달라지고, 단순히 ‘켜짐/꺼짐’이 아니라 연속적인 변화가 있음 

    - 화학적, 전기적 신호를 사용해서 소통
 
    - 지속적으로 적응

<br/>

- **Artificial Neurons (Perceptrons, 인공 뉴런)**

    - 원하는 만큼 입력을 가질 수 있음
 
    - 단순한 수학 연산만 수행
 
        - 가중치 곱하기, 더하기, 임계값 비교 같은 간단한 연산
     
    - 디지털 신호 사용
 
        - 0 또는 1 같은 이산적인 신호로 계산

    - 학습 후에는 고정됨

<br/>

- Perceptrons (인공 뉴런) 예시 : 밖에 나가야 할까?

    - input data

        - x₁ = Is it sunny? (1=yes, 0=no)

        - x₂ = Is it warm? (1=yes, 0=no)

        - x₃ = Is it raining? (1=yes, 0=no)
     
    - 뉴런은 이런 가중치를 학습할 수 있다
 
        - w₁ = +2 (sunny is good) → w₁=+2 (햇볕은 좋음)

        - w₂ = +1 (warm is nice) → w₂=+1 (따뜻한 건 좋음)

        - w₃ = -3 (rain is bad) → w₃=-3 (비는 나쁨)
     
    - **의사결정: (2×햇볕 + 1×따뜻함 - 3×비 - 1) > 0 이면, 나간다**


- 즉, 입력 → 가중치 적용 → 합산 → 임계값 비교 → 출력 결정을 수행

<br/>

### 생물학적 뉴런 ->수학적 퍼셉트론 모델 변 과정 

- 생물학적 뉴런을 바탕으로한 변환 과정 

    - 수상돌기 (Dendrite)가 신호를 받는다 → 입력값으로 표현 (x₁, x₂, …, xₙ)

    - 시냅스 (Synaptic)의 강도 → 가중치 (w₁, w₂, …, wₙ)

    - 세포체 (soma)에서 통합 → 가중합 계산 Σ wᵢxᵢ

    - 발화 임계값 → 편향(b)

    - 발화/비발화 → 활성화 함수

- 퍼셉트론 식: (가중합 + 편향) > 0 이면 출력=1, 그렇지 않으면 0

<br/>

![System Resources](../../images/Artificial%20Neural%20Network%20images/퍼셉트론구조.png)

<br/>

### 활성화 함수(Activation Function) 

- 활성화 함수(Activation Function) : 뉴런이 계산한 가중합(입력 × 가중치 + 편향)을 받아, 이를 출력값으로 변환하여 뉴런의 발화 여부나 강도를 결정하는 함수

![System Resources](../../images/Artificial%20Neural%20Network%20images/활성화함수.png)

- 활성화함수 공식 
  
![System Resources](../../images/Artificial%20Neural%20Network%20images/활성화함수공식.png)

 - 활성화함수 공식 예시

    - 입력 특징

        - x1 = 햇볕이 있음 (1/0)

        - x2 = 따뜻함 (1/0)

        - x3 = 비가 옴 (1/0)

    - 가중치

        - w1 = +2 (햇볕은 좋음)

        - w2 = +1 (따뜻한 건 좋음)

        - w3 = -3 (비는 나쁨)

    - 편향

        - b = -1 (조금 귀찮음)
     
    - 만약 날씨가 맑고 따뜻하고 비 안 온다면 (x=[1,1,0])

        - w⋅x+b=(2)(1)+(1)(1)+(−3)(0)−1=2+1+0−1=2>0 처럼 내적을 사용 
     
        - 출력 : 밖에 나간다(1)
    

 ### Linear Decision Boundaries (선형 결정 경계)

 - **Linear Decision Boundaries**

 - 머신러닝에서 분류를 할 때, 데이터를 구분하는 경계

     - 1차원 : 점(point)
  
     - 2차원 : 선(line)
  
     - 3차원 : 면(plane)
  
     - 4차원 이상 : 초평면(hyperplane) -> 직접 볼 수 없음 

<br/>

- 선형 결정 경계 공식
  
![System Resources](../../images/Artificial%20Neural%20Network%20images/선형결정경계공식.png)

- 위 공식을 보면 wTx 는 **내적**을 의미한다

    - wTx + b = w[0]x[0] + w[1]x[1] + ... + w[m-1]x[m-1] + b
 
    - 즉, 내적은 각 차원의 가중치 x 특징값을 모두 더한 값
 
        - b를 오른쪽으로 이항시킴으로서 기준을 0으로 일반화 가능 
     
        - 이 식은 n차원에서도 동일하게 적용 가능함

<br/>

- **특징 계수(feature coefficient)**

    - 벡터 w는 각 특징(입력값 x)에 곱해지는 가중치들을 모아둔 것
 
        - 이 가중치 하나하나를 **계수(coefficient)** 라고 함  
  
    - ex: 스팸 이메일을 분류한다고 하고, 네 번째 특징 x[4]이 이메일에 포함된 철자 오류 개수를 나타낸다고 하자
 
        - 만약 이 특징의 계수 w[4]가 양수라면, 철자 오류 개수가 많을수록 스팸으로 분류 가능성이 커짐
     
        - 즉, 가중치 벡터 w의 각 원소는 해당 특징이 결과에 미치는 **영향력(계수)** 을 의미

<br/>

- **bias term b (편향 b)**

    - 편향 b는 **입력 특징(feature)에 의존하지 않는다**
 
        - 즉, wTx는 입력 특징값 x에 따라 변하지만, **b는 항상 고정된 상수**
     
            - 데이터와 무관하게 **결정 경계를 평행 이동**시키는 역할
         
            - 따라서 전체 예측 결과를 더 긍정적(+1 쪽) 또는 부정적(-1 쪽)으로 이동시킬 수 있음
            - 만약 b가 큰 음수라면, 모델은 전체적으로 음수(-1) 예측을 하도록 치우치게 됨

    <br/>


    - 편향 항을 포함하는 것은 **필수적**이다
 
    - 선형 결정 경계로 완벽하게 분리 가능한 데이터셋이 있더라도, 바이어스 항이 없으면 그렇게 하는 것이 **불가능**할 수 있음
 
        - ex: b=0이면, 결정 경계는 무조건 원점(0,0)을 지나야 하지만 실제 데이터는 원점을 중심으로 대칭적으로 분리되지 않을 수 있음
     
        - 따라서 b가 있어야 경계를 옮겨서 데이터를 **제대로 분리가 가능**함

![System Resources](../../images/Artificial%20Neural%20Network%20images/편향필요예시.png)

<br/>

### 선형 결정 경계(Linear Decision Boundary)의 직관과 해석

![System Resources](../../images/Artificial%20Neural%20Network%20images/선형결정경계해석.png)

- 직관과 해석
 
    - w⋅x+b=0

        - 데이터 공간을 양쪽으로 나누는 선/면/초평면 

    - activation = w⋅x+b

        - 점이 결정 경계로부터 얼마나 떨어져 있는지를 계산
 
            - SVM 같은 알고리즘에서 유용함


    - 직관적 해석
 
        - 수학적 : 단순 w⋅x+b=0라는 방정식
     
        - 기하학적 : 결정 경계는 항상 가중치 벡터 w와 수직 관계를 가짐
     
            - 증명 : 어떤 두 점 x1과 x2가 이 경계 위에 있다면
         
                - w ⋅ x1 ​+ b = 0 / w ⋅ x2​ + b = 0
             
                - 두 식을 정리하면 w⋅(x1​−x2​)=0 -> w와 내적이 0 → 직교
             
                - 즉, 어떤 점 이든 w와 수직방향임

        - 물리적 : 경계는 말 그대로 균형선같은 역할을 함

<br/>

### Weights

- 가중치는 **각 특징(feature)의 중요성**을 나타낸다

- 만약 특정 특징 값이 조금 변했을 때, 최종 분류 결과가 얼마나 민감하게 변할까?

    - 이 질문은 **미분**으로 해결 가능
 
    - 예시 : f(x) = sign(Σi wi xi + b)에서 x4가 변했을 때의 민감도는 ?
 
        - 네 번째 특징에 대해 미분하면, 변화율은 w4이다 (시그마의 나머지 값은 상수 취)
     
        - 즉, 𝑤4값이 곧 그 특징이 결과에 미치는 영향의 크기와 방향
      
 <br/>

 - 가중치 예시 문제 : 스팸 여부를 판단하는 데 쓰이는 입력 데이터

 - feature를 다음과 같이 정의하자

     - x1: 단어 개수

     - x2 : 철자 오류 개수

     - x3 : 링크 개수

     - x4 : 발신자 평판
 

- 학습된 가중치는 [0.1 , 2.5 , 1.8 , −3.2]

- 해석
  
    - w₂ = 2.5 (spelling errors)

         - 철자 오류 개수는 스팸일 가능성을 크게 높이는 특징

    - w₄ = -3.2 (sender reputation)

        - 발신자 평판이 좋으면 스팸이 아닐 가능성이 큼
     
            - 여기서 평판은 apple, 삼성은 신뢰도가 높지만, 처음보는 곳은 낮다는 의미 

    - w₁ = 0.1 (word count)
      
        - 단어 개수는 영향력이 거의 없음.


    - 철자 오류가 1개 늘면 스팸 점수가 2.5만큼 증가하고, 발신자 평판이 1만큼 좋아지면 스팸 점수가 3.2만큼 감소
 
        - 즉, **가중치의 크기**는 **민감도**이다 

<br/>

### 신경망 그래프 모델링 

- 노드 = 뉴런 (신경세포)

- 간선 = 가중치 wij를 가진 시냅스 연결

- 표기법: wij는 뉴런 i에서 뉴런 j로 가는 가중치

    - **순 입력(net input)** : 입력값 × 가중치들을 모두 더한 값 (누적 )
 
        - **netⱼ = Σᵢ (wᵢⱼ · oᵢ)** : 뉴런 j가 받은 신호 =  여러 입력 뉴런 i에서 나온 출력 oi에, 가중치 wij를 곱해서 모두 더한 값
 
    - **Output** : 임계값 이상이면 출력은 1, 작으면 0
 
        - Tj는 뉴런 j의 임계값 

![System Resources](../../images/Artificial%20Neural%20Network%20images/뉴런출력예시.png)

- 예시 문제

    - 입력 뉴런 값들:

         - o1 = 1, o2 = 0, o3 = 1

    - 가중치:
 
        - w1j = 0.5, w2j = 1.0, w3j = 0.8

    - 임계값:

         - Tj = 1.0

    - 계산 : netj=(0.5×1)+(1.0×0)+(0.8×1)=0.5 + 0 + 0.8 = 1.3
 
        - netj​ ≥ Tj 이므로 oj​=1

<br/>

### McCulloch & Pitts (1943) 신경망 모델

- McCulloch와 Pitts(1943)는 뉴런이 적절한 가중치와 임계값을 선택하면 **논리 게이트**처럼 동작할 수 있다고 함 (실제로 만듦)

    - 뉴런들의 네트워크는 컴퓨터가 할 수 있는 **모든 계산을 할 수 있다**
 
    - 즉, 뉴런은 AND, OR, NOT 같은 **논리 게이트를 모방**할 수 있다
        
    - 하지만 진짜 질문은, 이 뉴런들이 무엇을 계산해야 하는지를 **스스로 학습할 수 있는가**? 이다
 
        - 단순히 사람이 가중치와 임계값을 **정해주는 게 아니라**, 컴퓨터가 **스스로 학습**하여 최적의 값을 찾아야 한다
     
        - 그래서 등장한 것이 **퍼셉트론 학습 알고리즘**이다

<br/>

- 퍼셉트론은 아래와 같이 간단한 논리 연산은 구현 가능하다
  
![System Resources](../../images/Artificial%20Neural%20Network%20images/논리게이트1.png)
![System Resources](../../images/Artificial%20Neural%20Network%20images/논리게이트2.png)

<br/>

### 선형회귀 vs 퍼셉트론 차이점 

- **퍼셉트론**

    - 입력값을 받아서 -> (가중치 곱) → 합계(Net input) → 규칙(activation function) 적용 → 최종 결과(output) 과정이 이루어짐
 
    - 넘으면 1, 아니면 0
 
- **선형 회귀**

    - 입력값을 받아서 -> (가중치 곱) → 합계(Net input) 에서 끝
 
    - 합계가 곧 결과
 
- 예시

    - 입력값: x1 = 2, x2 = 3
   
    - 가중치: w1 = 1, w2 = 2

    - 편향(bias): b = -4
 
        - 선형회귀 : z=(2)(1)+(3)(2)−4=2+6−4=4
 
        - 퍼셉트론 : z=4, -> 활성화 함수 -> 0 or 1로 분류 

![System Resources](../../images/Artificial%20Neural%20Network%20images/퍼셉트론구조예시.png)

<br/>

### 우리가 하고싶은 것 

- 결국 분류 오류를 최소화하는 분류기를 만들고 싶음

    - **퍼셉트론** 알고리즘으로 만들 수 있음
 
        -  그러나 데이터가 **선형적으로 구분(linearly separable)되지 않으면** 일반적으로 이 문제는 풀 수 없다는 것이다 (**NP 하드 문제**)
     
        - 즉, 퍼셉트론은 **직선(혹은 초평면)으로만** 데이터를 나누기 때문에, XOR 같은 데이터는 해결 불가
     
        - 일반적인 해결법 : **학습 오류를 작게** 만드는 **매개변수(parameters)**를 찾는 것

<br/>

- **데이터가 선형적으로 구분된 경우**

    - 입력과 정답(출력)이 함께 주어지는 지도 학습 데이터가 있다고 가정
 
    - 각 입력 예제에 대해 올바른 출력을 만들도록 **가중치(weight)** 를 학습한다
 
        - 퍼셉트론은 **반복적인** 업데이트 알고리즘으로 올바른 가중치를 학습한다
     
            - 점차적으로 맞추어 나감  

    - 이진 분류 문제에서, 두 클래스가 선형적으로 구분 가능하다면 퍼셉트론은 올바른 결정 경계를 찾는다

<br/>

### Online learning (온라인 학습)

- **Online learning (온라인 학습)**: 데이터를 하나 보자마자 **곧바로** 가중치 w를 업데이트

- **Batch learning (배치 학습)** : 모든 데이터를 모아놓고 **한꺼번에** 계산 → 그 다음에 업데이트

<br/>

- Online learning 의 경우 만약 오류가 없으면 w를 바꾸지 않고 오류가 있으면 w를 수정한다

    - 수정 방식
 
      ![System Resources](../../images/Artificial%20Neural%20Network%20images/퍼셉트론업데이트.png)
 
        -  위 방식을 사용하면, 예측이 틀리면 **정답 방향으로** 가중치를 조금 움직

<br/>

- 또한, 위 업데이트 후의 예측값이, 업데이트 전의 예측값보다 **항상 크거나 같다**

    - 이것은 아래 공식으로 증명이 가능하다
      
    ![System Resources](../../images/Artificial%20Neural%20Network%20images/온라인알고리즘_퍼셉트론_수식전개.png)

    - 위 공식을 보면 xTxy^2 + y^2에 의해 업데이트 값이 항상 크다는 것을 알 수 있음
 
        -  값이 커진다 = 점점 +1과 -1쪽으로 붙는다 
 

![System Resources](../../images/Artificial%20Neural%20Network%20images/퍼셉트론알고리즘.png)












