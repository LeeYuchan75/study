### Validation Set Approach (검증 세트 접근법)

- 데이터가 충분히 많을 경우, 가장 좋은 방법은 데이터를 무작위로 세 부분(훈련 세트, 검증 세트, 테스트 세트)으로 나누는 것이다.

    - **훈련 세트 (training set)** : 모델을 학습시키는 데 사용
 
    - **검증 세트 (validation set)** : 모델 선택을 위해 예측 오차를 추정하는 데 사용
 
    - **테스트 세트 (test set)** : 최종적으로 선택된 모델의 일반화 오차를 평가하는 데 사용

<br/>

- 세 부분에 얼마나 데이터를 배분할지는 일반적인 규칙을 주기 어렵다. 이는 데이터의 **신호-대-잡음 비율**과 훈련 샘플 크기에 달려 있기 때문이다

    - 데이터가 복잡하거나 잡음이 많으면 검증/테스트에 더 많이 할당할 수도 있고, 단순하면 적게 할 수도 있다. 즉 상황 따라 다르다
 
    - 일반적인 분할 예시는 훈련 50%, 검증 25%, 테스트 25%이다

<br/>

### k-Fold Cross-Validation

- 데이터를 k개의 동일한 크기 부분(폴드)로 나눈다. 보통 k=5 또는 k=10을 사용한다

    - k 중에서 하나씩 돌아가며 i번째를 테스트 세트로 사용하고 나머지를 훈련 세트로 사용한다
 
        - 여기서 훈련 세트와 검증 세트의 경우의 수를 다양하게 한 것이고, 테스트 세트는 이후 별도로 사용한다.
     
        - 따라서 각 분리 과정은 훈련 세트와 검증 세트만 분리하는 것이다  
 
    - 이렇게 얻은 k개의 테스트 오차를 평균낸다
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/04-2장k-flod테스트오차평균.png)
     
        - k개의 테스트 오차(MSE)를 평균내어 최종 점수로 삼는다

<br/>

- **k-fold 장점**

    - 모든 데이터를 학습과 검증에 **모두 활용**
 
        - 데이터 낭비가 없고, 작은 데이터셋에서도 효율적

    - 한 번만 나눠서 검증하는 방법보다 **더 안정적인 추정치**를 준다
 
    - 데이터 분할에 대한 의존성을 줄인다
 
        - 특정 Train/Test 분할에 따라 결과가 크게 달라지는 문제를 완화한다 

<br/>

### Leave-One-Out Cross-Validation (LOOCV)

- **데이터를 하나씩 빼서(test)**, **나머지로 학습(train)** 하는 교차검증 방식

    - 검증 세트(validation set)를 두지 않고, 각 데이터가 번갈아 가며 테스트 역할을 함

    - **작은 데이터셋에 효과적**이고 적합하다
 
        - 데이터가 너무 적으니까 Train/Validation/Test로 잘라버리면 학습할 데이터가 **부족해짐**
     
        - 그래서 Validation 세트를 따로 두지 않고 → 각 데이터가 한 번씩 Test 역할을 하도록 함
     
        - 즉, n개의 데이터 중 n-1개는 Train, 1개는 Test로 쓰고, 이걸 n번 반복해서 평균낸다
 
    - 특수한 경우: k = n
 
        - n-1개의 데이터로 학습하고, 남은 1개로 테스트한다
     
        - 이 과정을 모든 n개의 데이터에 대해 반복

<br/>

### 정규화 (Why Regularization)

- **정규화가 필요한 이유**

    - **The High-Dimensional Problem (고차원 문제)**
 
        - 변수가 개수(p)가 관측치 개수(n)와 비슷하거나 더 많을 때 (ex: 100개의 샘플에 변수가 100개 이상 존재할)
     
        - **최소제곱법(Least squares) 분산**이 매우 커진다
     
            - 데이터 샘플이 조금만 바뀌어도 예측 결과가 크게 달라짐 → 불안정 

        - 모델이 **과적합**되기 쉽다
     
        - 변수가 관측치보다 많으면 **해가 무한히 많아질 수 있다**
     
            - 방정식보다 미지수가 많으니, 회귀 계수 조합이 무한하게 존재 → 답이 하나로 정해지지 않음

   <br/>
   
    - **따라서 정규화가 필요하다**
 
        - 모델 복잡성에 **패널티**를 추가함
     
            -  즉, 변수가 많아져도 아무렇게나 커지지 못하도록 제약을 건다

        - **회귀 계수를 0에 가깝게** 줄인다
     
            - 불필요한 변수를 억제하고 단순화 

        - **분산**을 크게 줄이는 대신, **편향**을 약간 늘린다
     
            - 약간의 단순화(오차 허용)를 통해 모델 안정성을 높이고 과적합을 막는다

<br/>

- 해당 파트에서 정규화는 고차원 문제 해결으로 제시되며 대표적으로 아래 방법이 존재한다

    - Ridge: 모든 계수를 줄임(0은 아님)

    - Lasso: 일부 계수를 0으로 만들어 변수 선택 가능

    - Dimension Reduction: 차원을 줄여 단순화
    
    - Subset Selection: 일부 변수만 선택 

<br/>

### Shrinkage method

- **Shrinkage method** : **계수를 줄여서** 과적합을 막는 정규화 기법

    - 모든 변수 p를 포함하는 모델을 적합할 수 있는데, 이때 계수 추정치를 제약(constrain)하거나 정규화(regularize)하는 기법을 사용한다.

        - 즉, 계수들을 0에 가깝게 줄이는(shrink) 방법을 의미한다
 
    - 계수 추정치를 줄이면 그 분산을 크게 감소시킬 수 있음이 밝혀졌다
 
    - 회귀 계수를 **0에 가깝게** 줄이는 가장 잘 알려진 두 가지 기법은 **릿지 회귀(Ridge Regression)** 와 **라쏘 회귀(Lasso)** 이다
 
        - Ridge: 모든 계수를 줄임 (0은 아님)

        - Lasso: 일부 계수를 아예 0으로 만들어 변수 선택까지 가능 
 


<br/>

- **Subset Selection**

    - 변수들 중 일부만 선택해서 나머지는 ***완전히 버리는** 방법 

        - 모델이 단순해지고 해석이 쉬움

        - 하지만 선택 과정이 불안정할 수 있음 (데이터 조금만 바뀌어도 선택된 변수가 달라짐) 









































































