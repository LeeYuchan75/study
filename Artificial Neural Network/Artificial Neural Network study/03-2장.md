### Logistic Model (로지스틱 모델)

- 정의 : 두 가지 종류 중 하나를 예측하는 대표적인 분류(Classification) 알고리즘

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장로지스틱공식.png)
 
    - 위 로지스틱 공식의 범위는 **[0,1]** 이다

<br/>

- **로그-오즈(log-odds)**

    - **odds** : 사건이 일어나지 않을 확률에 대한 일어날 확률의 비율
 
        - 로지스틱 함수 공식을 수학적으로 정리하고 변형하면 아래 공식 나오는데 이것을 odds라고 한다 
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장odds공식.png)
     
        - P(X) : 일어날 확률
     
        - 1 - P(X) : 일어나지 않을 확률
     
    - 로그-오즈(log-odds) : 0과 1 사이에 갇혀 있는 확률을 -∞ 에서 +∞ 까지의 직선 세계로 보내주는 것
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장로그오즈공식.png)
     
            - 오즈 (p/(1-p)): 범위가 [0, +∞]로 확장

            - 로그-오즈 (log(odds)): 범위가 [-∞, +∞]로 완벽하게 확장 
     
        - 위 odds 공식을 로그 안에 넣어주면 로그 규칙에 의해 e의 지수만 나오게 됨 
     

 
<br/>

- 위 개념을 정리하면

    - (1) : 로지스틱 회귀 모델은 입력 데이터 X를 받아 **선형 계산**한다
 
    - (2) : 1단계에서 나온 그 숫자를 **로지스틱 함수**에 넣어서 **확률**을 구한다 

<br/>

### 최대 가능도(maximum likelihood) 

- 로지스틱 회귀의 최적 계수를 찾을 때 선형 회귀에서 썼던 최소 제곱법 대신 **최대 가능도 추정법**을 사용함

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장최대가능도공식.png)
 
    - Π 기호는 **곱하기**를 의미함

    - Π p(xᵢ): 실제 정답이 1인 모든 데이터에 대해, 모델이 예측한 확률들을 전부 곱함

    - Π (1 - p(xᵢ')): 실제 정답이 0인 모든 데이터에 대해, 모델이 예측한 '0일 확률'들을 전부 곱함
 
    - 쉽게 설명하자면, 동전을 3번 던졌는데 '앞-뒤-앞'이 나왔을 때, 이 일이 일어날 확률은 P(앞) × P(뒤) × P(앞)로 계산한다. 즉 각 확률을 **독립적**으로 보고 이 확률을 모두 곱한 것이다
 
        - 최대 가능도(maximum likelihood) 는 이 원리를 사용하여 가장 높은 확률을 보여주는 베타를 구하는 것이다
     
        - 여기서 곱의 범위는 0~1 이기 때문에 계속 곱하면 작아져서 계산할 때 **로그**를 사용한다

<br/>

### 로지스틱 문제 

- 문제 : 잔액(balance)이 1000달러 일 때, 연체할 확률을 추정하라

    - 즉 Pr(default = Yes | balance)의 값을 구하여라
 
    - **여기서 핵심** : **확률**를 구하라고 했으므로 -> **로직스틱을 이용**해야함
 
        - 만약 **특정 수치에서의 값**을 물어본다면 -> **선형회귀**로 x값을 넣어서 y값 도출 

<br/>

- 문제에서 제공된 표

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장문제예시표.png)
 
    - Intercept (절편): -10.6513
 
        - 절편은 -10.6513 → 잔액이 0일 때 기본 log-odds 값 

    - balance (계수): 0.0055
 
        - 잔액이 1 단위(=1달러) 늘어날 때, **로그 오즈(log-odds)** 가 0.0055만큼 증가
     
        - 즉, 절편과 계수는 아래 공식에서 β0와 β1을 의미한다
     
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장문제예시공식대입.png)
          
    <br/>

    - 따라서 로그 오즈 공식에다가 β0와 β1을 대입하고 X = 1000을 대입하면 내가 잔액(balance)이 1000달러일 때, 연체할 확률이 도출된다
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장문제예시연체확률공식대입.png)
     
             - 즉, 잔액이 1000달러일 때 연체 확률은 0.006 -> 매우 낮음
         
         - 만약 잔액이 2000달러라면 ?
     
             - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장문제예시연체확률공식대입2.png)

<br>

- 예시를 바꿔서 학생일 때 연체할 확률을 구해보자

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장학생연체확률.png)

<br/>

### 로지스틱 회귀(Logistic Regression)와 선형 분류 경계

- 위에서 로지스틱과 로즈 오즈 공간을 통해 비선형 공간과 선형 공간을 자유롭게 이동할 수 있다고 하였다.

    - 또한 아래 공식으로 변환할 수 있다고 배웠다 
 
    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장로그오즈공간2.png)

<br/>

- 이제 이 변환을 어떻게 하는지 상세하게 확인해보자

    - 일반적인 로지스틱 모델에서는 클래스가 2개이다 (다항 로지스틱도 존재하지만 여기선 2개라고 가정)
 
        - 클래스가 두 개일때 대표적인 사후확률(posterior probabilities)은 아래와 같다
     
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장사후모델종류.png)

            - Pr(G=2∣X=x) 의 분자가 1인 이유는 아래와 같다
         
                - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장사후확률증명1.png)
             
                - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장사후확률증명2.png)
             
        - 즉, 위 공식에 대입해서 선형 공간으로 매핑할 수 있다 

<br/>

### 선형 분류 전반의 원리

- K개의 클래스가 있다고 가정하고, 각 클래스를 1, 2, ..., K로 표시한다고 하자 

    - 이때 k번째 클래스에 대해 적합된 선형 모델은 f_k(x) = β^k0 + β^kT x 이다
 
<br/>

- 클래스 k와 클래스 ℓ 사이의 **결정 경계**는 **fk(x) = fℓ(x)** 인 점들의 집합이다

    - 쉽게 말해서 어떤 점 x가 두 클래스에 **똑같이 속할** 가능성이 있는 위치가 두 클래스 사이의 경계선(또는 면)이다
 
        - ex: 55 ~ 65점 = c등급이라하고, 65 ~ 75 = B등급 75 ~ 100 = A등급이라고 하자
 
            - 이때 C등급과 B등급의 경계선은 65점이다

    <br/>
    
    - 똑같이 속할 가능성이 존재한 fk(x) = fℓ(x)의 공식을 전개해보자
 
        - f_k(x) = β^k0 + β^kT x 
        
        - f_ℓ(x) = β^ℓ0 + β^ℓT x 이므로 두 함수가 같다고 한 뒤 이항해주면 
     
            - (β^​k0​ − β^​ℓ0​) + (β^​k ​− β^​ℓ​)Tx = 0 이다
         
            - 즉 이것이 k 클래스와 ℓ 클래스의 결정경계이다
         
                - 차원에 따라 직선, 평면, 초평면이라고도 불린다  

    <br/>

    - 이러한 회귀 접근법은 각 클래스에 대해 판별 함수 δk(x)를 모델링하고, 입력 x를 판별 함수 값이 가장 큰 클래스에 할당하는 방법의 일종이다
 
        - δk(x) : 클래스 k에 속할 정도(점수)를 계산하는 함수

        - 만약 판별 함수 δk(x) 또는 사후 확률 Pr(G=k∣X=x)가 x에 대해 선형이라면, 결정 경계 역시 선형이다
     
            - 즉, 점수나 확률이 입력 x의 직선식으로 표현되면, 클래스 사이의 경계도 직선(또는 평면)이 된다 

<br/>

### 다항 로지스틱 회귀

- 로지스틱 회귀 모델은 K개의 클래스에 대한 사후 확률(posterior probabilities)을 x의 선형 함수로 모델링하려는 데서 출발한다

    - 동시에 이 확률들이 모두 더하면 1이 되고, 각 확률은 **[0, 1]** 구간에 있도록 보장한다
 
    - 모든 클래스 확률의 합이 1이 되도록 **softmax** 형태를 사용
 
        - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장다항로지스틱회귀소프트맥스공식.png)
     
        - 클래스 k (마지막 클래스 K를 제외한 경우)에 속할 확률은 해당 선형조합을 지수화(exp) 한 값을, 모든 클래스들의 지수 합으로 나눈 형태
     
            - 마지막 클래스 K를 제외한 이유는 아래와 같이 모든 클래스의 확률 합은 1이기 때문에 자동으로 K값이 결정되기 때문
     
            - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장다항로지스틱회귀K번째소프트맥스공식.png)
         
            - 추가적으로 위 공식에 대한 증명은 아래와 같다
         
            - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장다항로지스틱회귀K번째소프트맥스공식증명.png)

<br/>

- 다항 로지스틱 회귀에서 소프트맥스와 동등한 공식으로 기준 클래스를 잡아서 표현할 수도 있다

    - ![System Resources](../../images/Artificial%20Neural%20Network%20images/03-2장다항로지스틱회귀기준클래스공식.png)
 
    - 이전에 나온 공식을 분모 분자에 두면 각 분모의 지수화(exp) 한 값이 약분이 되고, 해당 식에 로그를 취하면 k번째 클래스와 K 클래스의 (즉, 임의의 클래스와 기준 K) 결정 경계를 나타내는 공식이 된다 








































































































































